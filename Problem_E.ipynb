{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(10, requires_grad=True)\n",
    "b = torch.rand(10, requires_grad=True)\n",
    "\n",
    "output = (2 * a).sum()\n",
    "torch.autograd.grad(output, (a,))\n",
    "# torch.autograd.grad(output, (a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_function(batch, linear, labels):\n",
    "    assert batch.requires_grad, \"Batch lacks requires_grad\"\n",
    "    x = linear(batch).float()\n",
    "    assert x.requires_grad, \"x lacks requires_grad after linear\"\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "    down_projection_function = CrossEntropyLoss(reduction = \"mean\")\n",
    "    # Down projection to small space\n",
    "    loss = down_projection_function(x.view(-1, x.shape[-1]), labels.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth_zoo.rl_replacements import UnslothEfficientGRPO\n",
    "from functools import partial\n",
    "\n",
    "# given a batch, computes a reward for each of the items in the batch\n",
    "# def example_reward_function(batch) -> torch.Tensor:\n",
    "#     pass\n",
    "\n",
    "# given a batch, computes a loss by passing it through linear and calculating rewards\n",
    "# def is_this_grpo(batch, linear, reward_functions):\n",
    "#     assert batch.requires_grad, \"Batch lacks requires_grad\"\n",
    "#     x = linear(batch).float()\n",
    "#     assert x.requires_grad, \"x lacks requires_grad after linear\"\n",
    "#     from torch.nn import CrossEntropyLoss\n",
    "#     down_projection_function = CrossEntropyLoss(reduction = \"mean\")\n",
    "#     # Down projection to small space\n",
    "#     loss = down_projection_function(x.view(-1, x.shape[-1]), labels.view(-1))\n",
    "#     return loss\n",
    "\n",
    "# we are not supposed to re-implement GRPO, I think we should try to call UnslothEfficientGRPO, and pass in a curried version. the currying should provide most of the args\n",
    "def curried_grpo_function(old_hidden_states, completion_input_ids, completion_mask, advantages, beta):\n",
    "    def inner_fn(\n",
    "        batch,  # new_hidden_states\n",
    "        linear, # lm_head\n",
    "        labels, # not used\n",
    "    ):\n",
    "        return UnslothEfficientGRPO.apply(\n",
    "            _old_hidden_states=old_hidden_states,\n",
    "            _new_hidden_states=batch,\n",
    "            lm_head=linear,\n",
    "            _input_ids=completion_input_ids, # the concatenated input ids of the prompt & completion\n",
    "            _mask=completion_mask,           # the concatenated mask of the prompt & completion\n",
    "            _advantages=advantages,          # the advantage scores of the new completions compared to the old ones\n",
    "            beta=beta,\n",
    "            scaler=None, # seems to be optional\n",
    "            n_chunks=1,  # the chunking is done by the caller of this function\n",
    "        )\n",
    "    return inner_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['UNSLOTH_IS_PRESENT'] = '1'\n",
    "from unsloth_zoo.rl_replacements import UnslothEfficientGRPO\n",
    "import torch\n",
    "\n",
    "old_hidden_states = torch.randn(6, 241, 2048, dtype=torch.bfloat16, device=\"cuda\", requires_grad=True)\n",
    "new_hidden_states = torch.randn(6, 241, 2048, dtype=torch.bfloat16, device=\"cuda\", requires_grad=True)\n",
    "lm_head = torch.randn(128256, 2048, dtype=torch.bfloat16, device=\"cuda\", requires_grad=True)\n",
    "completion_input_ids = torch.randint(0, 128256, (6, 240), dtype=torch.int64, device=\"cuda\")\n",
    "\n",
    "# filter out 128004\n",
    "# completion_mask = torch.randint(0, 2, (6, 240), dtype=torch.int64, device=\"cuda\")\n",
    "completion_mask = torch.ones_like(completion_input_ids)\n",
    "advantages = torch.randn(6, dtype=torch.float32, device=\"cuda\")\n",
    "advantages = torch.zeros(6, dtype=torch.float32, device=\"cuda\")\n",
    "beta = 0.04\n",
    "scaler = None\n",
    "n_chunks = 6\n",
    "\n",
    "UnslothEfficientGRPO.apply(\n",
    "    new_hidden_states,\n",
    "    old_hidden_states,\n",
    "    lm_head,\n",
    "    completion_input_ids,\n",
    "    completion_mask,\n",
    "    advantages,\n",
    "    beta,\n",
    "    scaler,\n",
    "    n_chunks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example in the statement was given as follows:\n",
    "# bsz = 4, qlen = 4096, hd = 4096, vocab = 128K\n",
    "# lets do something similar\n",
    "\n",
    "# example input with 4 samples, ctx = 8192 tokens, and hidden dimension of 2048\n",
    "input = torch.randn(4, 8192, 2048, device=\"cuda\", requires_grad=True)\n",
    "\n",
    "# linear (dense) layer which accepts an input of 2048 hidden dimensions, and outputs to a vocabulary of 4096\n",
    "linear = nn.Linear(2048, 4096).to(\"cuda\")\n",
    "\n",
    "# correct labels for the 16 samples in the batch, each sample needs to specify for ctx = 8192 tokens\n",
    "labels = torch.randint(0, 4096, (4, 8192), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRANSFORMATION_FUNCTION = True\n",
    "if TEST_TRANSFORMATION_FUNCTION:\n",
    "    # calculate the loss, which should result in a single scalar value\n",
    "    loss = transformation_function(input, linear, labels)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward.weight.data\n",
    "# forward.bias.data\n",
    "# labels.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input\n",
    "X0, X1 = torch.chunk(X, chunks=2, dim=0)\n",
    "labels_0, labels_1 = torch.chunk(labels, chunks=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z0 = transformation_function(X0, linear, labels_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be very clear about the terminology here\n",
    "# X is the input to the memory efficient linear function\n",
    "# Y is W @ X + b, where W is the weight matrix and b is the bias of the linear layer\n",
    "# Z is f(Y), where f is the transformation function\n",
    "# the output is expected to be a single scalar value\n",
    "\n",
    "class MemoryEfficientLinear(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, linear, labels, forward_function):\n",
    "        # TODO early exit if there is only one sample or if the number\n",
    "        # TODO of samples isn't a multiple of two\n",
    "        n_batch = X.shape[0]\n",
    "        # X0, X1 = torch.chunk(X, chunks=2, dim=0)\n",
    "        labels_0, labels_1 = torch.chunk(labels, chunks=2, dim=0)\n",
    "        with torch.enable_grad():\n",
    "            X0 = X[:n_batch // 2]\n",
    "            X1 = X[n_batch // 2:]\n",
    "            assert X0.requires_grad\n",
    "            assert X1.requires_grad\n",
    "            Z0 = forward_function(X0, linear, labels_0)\n",
    "            Z1 = forward_function(X1, linear, labels_1)\n",
    "            # at some point, realized need to move the `X0 =` and `X1 =` into this block\n",
    "            # use this to check if grad is working\n",
    "            output = ((Z0.to(torch.float64) + Z1.to(torch.float64)) * 0.5).to(torch.float32)\n",
    "        ctx.save_for_backward(X0, X1, Z0, Z1, linear.weight)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        X0, X1, Z0, Z1, linear_weight = ctx.saved_tensors\n",
    "        \n",
    "        # Gradient scaling factor from the mean\n",
    "        grad_scale = grad_output * 0.5\n",
    "\n",
    "        # print('Z0', Z0)\n",
    "        # print('X0', X0)\n",
    "        \n",
    "        # Compute gradients w.r.t. X1 from Z0\n",
    "        grad_X0 = torch.autograd.grad(Z0, X0, grad_outputs=grad_scale, retain_graph=True)[0]\n",
    "\n",
    "        # Compute gradients w.r.t. X0 from Z1\n",
    "        grad_X1 = torch.autograd.grad(Z1, X1, grad_outputs=grad_scale, retain_graph=True)[0]\n",
    "        \n",
    "        # grad_linear_weight = (\n",
    "        #     torch.autograd.grad(Z0, linear_weight, grad_outputs=grad_scale, retain_graph=True)[0] +\n",
    "        #     torch.autograd.grad(Z1, linear_weight, grad_outputs=grad_scale, retain_graph=True)[0]\n",
    "        # )[0]\n",
    "\n",
    "        # print('grad_X0', grad_X0)\n",
    "        # print('grad_X1', grad_X1)\n",
    "        # print('grad_linear_weight', grad_linear_weight)\n",
    "\n",
    "        # Assemble full gradient w.r.t. X\n",
    "        grad_X = torch.cat([grad_X0, grad_X1], dim=0)\n",
    "\n",
    "        # Return gradients for all inputs\n",
    "        return grad_X, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MemoryEfficientLinear.apply(input, linear, labels, transformation_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tests to see if the outputs match\n",
    "for x in range(100):\n",
    "    input = torch.randn(4, 8, 2, device=\"cuda\", requires_grad=True)\n",
    "    linear = nn.Linear(2, 4).to(\"cuda\")\n",
    "    labels = torch.randint(0, 4, (4, 8), device=\"cuda\")\n",
    "    expected = transformation_function(input, linear, labels)\n",
    "    actual = MemoryEfficientLinear.apply(input, linear, labels, transformation_function)\n",
    "    assert(torch.allclose(expected, actual))\n",
    "    \n",
    "    # now check if the backpropagation calculates the same\n",
    "    expected.backward()\n",
    "    gradI_expected = input.grad\n",
    "    gradW_expected = linear.weight.grad\n",
    "    gradB_expected = linear.bias.grad\n",
    "\n",
    "    MemoryEfficientLinear.apply(input, linear, labels, transformation_function).backward()\n",
    "    gradI_actual = input.grad\n",
    "    gradW_actual = linear.weight.grad\n",
    "    gradB_actual = linear.bias.grad\n",
    "\n",
    "    assert(torch.allclose(gradI_expected, gradI_actual))\n",
    "    assert(torch.allclose(gradW_expected, gradW_actual))\n",
    "    assert(torch.allclose(gradB_expected, gradB_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "silly_vec = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "loss = (silly_vec * 2).sum()\n",
    "loss.backward()\n",
    "silly_vec.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self score: 2 + 1 + 1 = 4\n",
    "# if attemped_E:\n",
    "#     E_score = 0\n",
    "#     if VRAM_50_percent_reduction: E_score += 2\n",
    "#     if remove_float32_upcast: E_score = 0\n",
    "#     if show_ce_loss_works: E_score += 1\n",
    "#     if show_other_functions_work: E_score += 1\n",
    "#     if hardcoded_gradients: E_score = 0\n",
    "#     if allows_dynamic_chunk_sizes: E_score += 1\n",
    "#     if llama_1B_training_loss_matches: E_score += 1\n",
    "#     else: E_score = 0\n",
    "#     if GRPO_memory_efficient_linear_works: E_score += 4\n",
    "#     final_score += E_score\n",
    "# else:\n",
    "#     final_score += 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
