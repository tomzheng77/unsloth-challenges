{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(10, requires_grad=True)\n",
    "b = torch.rand(10, requires_grad=True)\n",
    "\n",
    "output = (2 * a).sum()\n",
    "torch.autograd.grad(output, (a,))\n",
    "# torch.autograd.grad(output, (a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_function(batch, linear, labels):\n",
    "    assert batch.requires_grad, \"Batch lacks requires_grad\"\n",
    "    x = linear(batch).float()\n",
    "    assert x.requires_grad, \"x lacks requires_grad after linear\"\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "    down_projection_function = CrossEntropyLoss(reduction = \"mean\")\n",
    "    # Down projection to small space\n",
    "    loss = down_projection_function(x.view(-1, x.shape[-1]), labels.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example in the statement was given as follows:\n",
    "# bsz = 4, qlen = 4096, hd = 4096, vocab = 128K\n",
    "# lets do something similar\n",
    "\n",
    "# example input with 4 samples, ctx = 8192 tokens, and hidden dimension of 2048\n",
    "input = torch.randn(4, 8192, 2048, device=\"cuda\", requires_grad=True)\n",
    "\n",
    "# linear (dense) layer which accepts an input of 2048 hidden dimensions, and outputs to a vocabulary of 4096\n",
    "linear = nn.Linear(2048, 4096).to(\"cuda\")\n",
    "\n",
    "# correct labels for the 16 samples in the batch, each sample needs to specify for ctx = 8192 tokens\n",
    "labels = torch.randint(0, 4096, (4, 8192), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRANSFORMATION_FUNCTION = True\n",
    "if TEST_TRANSFORMATION_FUNCTION:\n",
    "    # calculate the loss, which should result in a single scalar value\n",
    "    loss = transformation_function(input, linear, labels)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward.weight.data\n",
    "# forward.bias.data\n",
    "# labels.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input\n",
    "X0, X1 = torch.chunk(X, chunks=2, dim=0)\n",
    "labels_0, labels_1 = torch.chunk(labels, chunks=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z0 = transformation_function(X0, linear, labels_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be very clear about the terminology here\n",
    "# X is the input to the memory efficient linear function\n",
    "# Y is W @ X + b, where W is the weight matrix and b is the bias of the linear layer\n",
    "# Z is f(Y), where f is the transformation function\n",
    "# the output is expected to be a single scalar value\n",
    "\n",
    "class MemoryEfficientLinear(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, linear, labels, forward_function):\n",
    "        # TODO early exit if there is only one sample or if the number\n",
    "        # TODO of samples isn't a multiple of two\n",
    "        n_batch = X.shape[0]\n",
    "        # X0, X1 = torch.chunk(X, chunks=2, dim=0)\n",
    "        labels_0, labels_1 = torch.chunk(labels, chunks=2, dim=0)\n",
    "        with torch.enable_grad():\n",
    "            X0 = X[:n_batch // 2]\n",
    "            X1 = X[n_batch // 2:]\n",
    "            assert X0.requires_grad\n",
    "            assert X1.requires_grad\n",
    "            Z0 = forward_function(X0, linear, labels_0)\n",
    "            Z1 = forward_function(X1, linear, labels_1)\n",
    "            # at some point, realized need to move the `X0 =` and `X1 =` into this block\n",
    "            # use this to check if grad is working\n",
    "            output = (Z0 + Z1) / 2\n",
    "        ctx.save_for_backward(X0, X1, Z0, Z1, linear.weight)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        X0, X1, Z0, Z1, linear_weight = ctx.saved_tensors\n",
    "        \n",
    "        # Gradient scaling factor from the mean\n",
    "        grad_scale = grad_output * 0.5\n",
    "\n",
    "        # print('Z0', Z0)\n",
    "        # print('X0', X0)\n",
    "        \n",
    "        # Compute gradients w.r.t. X1 from Z0\n",
    "        grad_X0 = torch.autograd.grad(Z0, X0, grad_outputs=grad_scale, retain_graph=True)[0]\n",
    "\n",
    "        # Compute gradients w.r.t. X0 from Z1\n",
    "        grad_X1 = torch.autograd.grad(Z1, X1, grad_outputs=grad_scale, retain_graph=True)[0]\n",
    "        \n",
    "        grad_linear_weight = (\n",
    "            torch.autograd.grad(Z0, linear_weight, grad_outputs=grad_scale, retain_graph=True)[0] +\n",
    "            torch.autograd.grad(Z1, linear_weight, grad_outputs=grad_scale, retain_graph=True)[0]\n",
    "        )[0]\n",
    "\n",
    "        print('grad_X0', grad_X0)\n",
    "        print('grad_X1', grad_X1)\n",
    "        print('grad_linear_weight', grad_linear_weight)\n",
    "\n",
    "        # Assemble full gradient w.r.t. X\n",
    "        grad_X = torch.cat([grad_X0, grad_X1], dim=0)\n",
    "\n",
    "        # Return gradients for all inputs\n",
    "        return grad_X, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MemoryEfficientLinear.apply(input, linear, labels, transformation_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tests to see if the outputs match"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
