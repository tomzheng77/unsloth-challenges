{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MemoryEfficientLinearFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    A custom autograd function that computes Z = X @ W in chunks, applies a function f,\n",
    "    and performs memory-efficient backpropagation.\n",
    "    \n",
    "    Args:\n",
    "        X (torch.Tensor): Input tensor of shape [batch_size, sequence_length, hidden_dim]\n",
    "        W (torch.Tensor): Weight matrix of shape [hidden_dim, vocab_size]\n",
    "        f (callable): Transformation function applied to Z (e.g., lambda x: x for logits)\n",
    "        num_chunks (int): Number of chunks to split X into along the batch dimension\n",
    "    \n",
    "    Returns:\n",
    "        Y (torch.Tensor): Output tensor after applying f to Z\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, W, f, num_chunks):\n",
    "        # Ensure inputs are in the correct format\n",
    "        assert X.dim() == 3, \"X must be 3D: [batch_size, sequence_length, hidden_dim]\"\n",
    "        assert W.dim() == 2, \"W must be 2D: [hidden_dim, vocab_size]\"\n",
    "        assert callable(f), \"f must be a callable function\"\n",
    "        \n",
    "        # Split X into chunks along the batch dimension\n",
    "        X_chunks = torch.chunk(X, num_chunks, dim=0)\n",
    "        Y_chunks = []\n",
    "        \n",
    "        # Process each chunk\n",
    "        for X_chunk in X_chunks:\n",
    "            Z_chunk = X_chunk @ W  # Shape: [chunk_size, seq_len, vocab_size]\n",
    "            Y_chunk = f(Z_chunk)   # Apply transformation function\n",
    "            Y_chunks.append(Y_chunk)\n",
    "        \n",
    "        # Concatenate results\n",
    "        Y = torch.cat(Y_chunks, dim=0)  # Shape: [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        # Save tensors for backward pass\n",
    "        ctx.save_for_backward(X, W)\n",
    "        ctx.f = f\n",
    "        ctx.num_chunks = num_chunks\n",
    "        \n",
    "        return Y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass computes gradients w.r.t. X and W using chunking.\n",
    "        \n",
    "        Args:\n",
    "            grad_output (torch.Tensor): Gradient of loss w.r.t. Y (dL/dY)\n",
    "        \n",
    "        Returns:\n",
    "            grad_X (torch.Tensor): Gradient w.r.t. X (dL/dX)\n",
    "            grad_W (torch.Tensor): Gradient w.r.t. W (dL/dW)\n",
    "            None, None: No gradients for f or num_chunks\n",
    "        \"\"\"\n",
    "        X, W = ctx.saved_tensors\n",
    "        f = ctx.f\n",
    "        num_chunks = ctx.num_chunks\n",
    "        \n",
    "        # Split inputs and gradients into chunks\n",
    "        X_chunks = torch.chunk(X, num_chunks, dim=0)\n",
    "        grad_output_chunks = torch.chunk(grad_output, num_chunks, dim=0)\n",
    "        \n",
    "        grad_X_chunks = []\n",
    "        grad_W = torch.zeros_like(W)  # Initialize gradient for W\n",
    "        \n",
    "        # Process each chunk\n",
    "        for X_chunk, grad_Y_chunk in zip(X_chunks, grad_output_chunks):\n",
    "            # Recompute Z_chunk for this chunk\n",
    "            Z_chunk = X_chunk @ W\n",
    "            Z_chunk_ = Z_chunk.detach().requires_grad_(True)\n",
    "            \n",
    "            # Compute gradients through f using autograd\n",
    "            with torch.enable_grad():\n",
    "                Y_chunk_ = f(Z_chunk_)\n",
    "                grad_Z_chunk = torch.autograd.grad(\n",
    "                    outputs=Y_chunk_,\n",
    "                    inputs=Z_chunk_,\n",
    "                    grad_outputs=grad_Y_chunk,\n",
    "                    retain_graph=True\n",
    "                )[0]\n",
    "            \n",
    "            # Compute gradients w.r.t. X and accumulate w.r.t. W\n",
    "            grad_X_chunk = grad_Z_chunk @ W.transpose(-1, -2)  # dL/dX = dL/dZ @ W^T\n",
    "            grad_X_chunks.append(grad_X_chunk)\n",
    "            \n",
    "            # Accumulate dL/dW = X^T @ dL/dZ\n",
    "            # Use einsum to handle 3D X_chunk and 3D grad_Z_chunk\n",
    "            grad_W += torch.einsum('bsh,bsv->hv', X_chunk, grad_Z_chunk)\n",
    "        \n",
    "        # Concatenate gradients for X\n",
    "        grad_X = torch.cat(grad_X_chunks, dim=0)\n",
    "        \n",
    "        return grad_X, grad_W, None, None  # No gradients for f or num_chunks\n",
    "\n",
    "# Example usage function\n",
    "def test_custom_function():\n",
    "    # Define problem dimensions\n",
    "    batch_size, seq_len, hidden_dim, vocab_size = 4, 2, 3, 5\n",
    "    num_chunks = 2\n",
    "    \n",
    "    # Create input tensors\n",
    "    X = torch.randn(batch_size, seq_len, hidden_dim, dtype=torch.float32, requires_grad=True)\n",
    "    W = torch.randn(hidden_dim, vocab_size, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    # Define a transformation function (e.g., identity for logits or ReLU)\n",
    "    f = lambda x: torch.relu(x)  # Example: ReLU as a nonlinearity\n",
    "    \n",
    "    # Apply the custom function\n",
    "    Y = MemoryEfficientLinearFunction.apply(X, W, f, num_chunks)\n",
    "    \n",
    "    # Simulate a loss (e.g., sum of squares)\n",
    "    loss = Y.pow(2).sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Input X shape:\", X.shape)\n",
    "    print(\"Weight W shape:\", W.shape)\n",
    "    print(\"Output Y shape:\", Y.shape)\n",
    "    print(\"Gradient dL/dX shape:\", X.grad.shape)\n",
    "    print(\"Gradient dL/dW shape:\", W.grad.shape)\n",
    "    \n",
    "    # Validate with standard computation\n",
    "    Z_standard = X @ W\n",
    "    Y_standard = f(Z_standard)\n",
    "    loss_standard = Y_standard.pow(2).sum()\n",
    "    loss_standard.backward()\n",
    "    \n",
    "    print(\"\\nGradient match check:\")\n",
    "    print(\"dL/dX matches:\", torch.allclose(X.grad, X.grad.clone(), atol=1e-5))\n",
    "    print(\"dL/dW matches:\", torch.allclose(W.grad, W.grad.clone(), atol=1e-5))\n",
    "\n",
    "# Test cross-entropy loss\n",
    "def test_cross_entropy():\n",
    "    batch_size, seq_len, hidden_dim, vocab_size = 4, 2, 3, 5\n",
    "    num_chunks = 2\n",
    "    \n",
    "    X = torch.randn(batch_size, seq_len, hidden_dim, dtype=torch.float32, requires_grad=True)\n",
    "    W = torch.randn(hidden_dim, vocab_size, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    # Use identity function for logits\n",
    "    f = lambda x: x\n",
    "    \n",
    "    # Compute output\n",
    "    Y = MemoryEfficientLinearFunction.apply(X, W, f, num_chunks)\n",
    "    \n",
    "    # Simulate targets for cross-entropy\n",
    "    targets = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    loss = F.cross_entropy(Y.view(-1, vocab_size), targets.view(-1))\n",
    "    loss.backward()\n",
    "    \n",
    "    print(\"\\nCross-entropy test:\")\n",
    "    print(\"Gradient dL/dX shape:\", X.grad.shape)\n",
    "    print(\"Gradient dL/dW shape:\", W.grad.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing with ReLU:\")\n",
    "    test_custom_function()\n",
    "    print(\"\\nTesting with Cross-Entropy:\")\n",
    "    test_cross_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
