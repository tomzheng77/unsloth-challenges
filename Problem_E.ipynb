{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_function(batch, linear, labels):\n",
    "    x = linear(batch).float() # Up projection to large space\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "    down_projection_function = CrossEntropyLoss(reduction = \"mean\")\n",
    "    # Down projection to small space\n",
    "    loss = down_projection_function(x.view(-1, x.shape[-1]), labels.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example in the statement was given as follows:\n",
    "# bsz = 4, qlen = 4096, hd = 4096, vocab = 128K\n",
    "# lets do something similar\n",
    "\n",
    "# example input with 4 samples, ctx = 8192 tokens, and hidden dimension of 2048\n",
    "input = torch.randn(4, 8192, 2048, device=\"cuda\", requires_grad=True)\n",
    "\n",
    "# linear (dense) layer which accepts an input of 2048 hidden dimensions, and outputs to a vocabulary of 4096\n",
    "forward = nn.Linear(2048, 4096).to(\"cuda\")\n",
    "\n",
    "# correct labels for the 16 samples in the batch, each sample needs to specify for ctx = 8192 tokens\n",
    "labels = torch.randint(0, 4096, (4, 8192), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRANSFORMATION_FUNCTION = False\n",
    "if TEST_TRANSFORMATION_FUNCTION:\n",
    "    # calculate the loss, which should result in a single scalar value\n",
    "    loss = transformation_function(input, forward, labels)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientLinear(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, linear, labels, forward_function):\n",
    "        # TODO early exit if there is only one sample or if the number\n",
    "        # TODO of samples isn't a multiple of two\n",
    "        outputs = [] # NOTE this is likely a hint\n",
    "        X0, X1 = torch.chunk(X, chunks=2, dim=0)\n",
    "        L0, L1 = torch.chunk(labels, chunks=2, dim=0)\n",
    "        Y0 = forward_function(X1, linear, L0)\n",
    "        Y1 = forward_function(X0, linear, L1)\n",
    "        outputs.append(Y0)\n",
    "        outputs.append(Y1)\n",
    "        ctx.save_for_backward(X, Y0, Y1)\n",
    "        return torch.mean(torch.tensor(outputs))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dY):\n",
    "        print(dY)\n",
    "        X, Y0, Y1 = ctx.saved_tensors\n",
    "        # EDIT THIS FUNCTION\n",
    "        return X, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MemoryEfficientLinear.apply(input, forward, labels, transformation_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
