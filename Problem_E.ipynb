{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_function(batch, linear, labels):\n",
    "    x = linear(batch).float() # Up projection to large space\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "    down_projection_function = CrossEntropyLoss(reduction = \"mean\")\n",
    "    # Down projection to small space\n",
    "    loss = down_projection_function(x.view(-1, x.shape[-1]), labels.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example in the statement was given as follows:\n",
    "# bsz = 4, qlen = 4096, hd = 4096, vocab = 128K\n",
    "# lets do something similar\n",
    "\n",
    "# example input with 4 samples, ctx = 8192 tokens, and hidden dimension of 2048\n",
    "input = torch.randn(4, 8192, 2048, device=\"cuda\", requires_grad=True)\n",
    "\n",
    "# linear (dense) layer which accepts an input of 2048 hidden dimensions, and outputs to a vocabulary of 4096\n",
    "forward = nn.Linear(2048, 4096).to(\"cuda\")\n",
    "\n",
    "# correct labels for the 16 samples in the batch, each sample needs to specify for ctx = 8192 tokens\n",
    "labels = torch.randint(0, 4096, (4, 8192), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRANSFORMATION_FUNCTION = False\n",
    "if TEST_TRANSFORMATION_FUNCTION:\n",
    "    # calculate the loss, which should result in a single scalar value\n",
    "    loss = transformation_function(input, forward, labels)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientLinear(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, linear, labels, forward_function):\n",
    "        # TODO early exit if there is only one sample or if the number\n",
    "        # TODO of samples isn't a multiple of two\n",
    "        outputs = [] # NOTE this is likely a hint\n",
    "        X0, X1 = torch.chunk(X, chunks=2, dim=0)\n",
    "        L0, L1 = torch.chunk(labels, chunks=2, dim=0)\n",
    "        Y0 = forward_function(X1, linear, L0)\n",
    "        Y1 = forward_function(X0, linear, L1)\n",
    "        outputs.append(Y0)\n",
    "        outputs.append(Y1)\n",
    "        ctx.save_for_backward(X, Y0, Y1)\n",
    "        return torch.mean(torch.tensor(outputs))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dY):\n",
    "        print(dY)\n",
    "        X, Y0, Y1 = ctx.saved_tensors\n",
    "        # EDIT THIS FUNCTION\n",
    "        return X, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MemoryEfficientLinear.apply(input, forward, labels, transformation_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MemoryEfficientLinearFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    A custom autograd function that computes Z = X @ W in chunks, applies a function f,\n",
    "    and performs memory-efficient backpropagation.\n",
    "    \n",
    "    Args:\n",
    "        X (torch.Tensor): Input tensor of shape [batch_size, sequence_length, hidden_dim]\n",
    "        W (torch.Tensor): Weight matrix of shape [hidden_dim, vocab_size]\n",
    "        f (callable): Transformation function applied to Z (e.g., lambda x: x for logits)\n",
    "        num_chunks (int): Number of chunks to split X into along the batch dimension\n",
    "    \n",
    "    Returns:\n",
    "        Y (torch.Tensor): Output tensor after applying f to Z\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, W, f, num_chunks):\n",
    "        # Ensure inputs are in the correct format\n",
    "        assert X.dim() == 3, \"X must be 3D: [batch_size, sequence_length, hidden_dim]\"\n",
    "        assert W.dim() == 2, \"W must be 2D: [hidden_dim, vocab_size]\"\n",
    "        assert callable(f), \"f must be a callable function\"\n",
    "        \n",
    "        # Split X into chunks along the batch dimension\n",
    "        X_chunks = torch.chunk(X, num_chunks, dim=0)\n",
    "        Y_chunks = []\n",
    "        \n",
    "        # Process each chunk\n",
    "        for X_chunk in X_chunks:\n",
    "            Z_chunk = X_chunk @ W  # Shape: [chunk_size, seq_len, vocab_size]\n",
    "            Y_chunk = f(Z_chunk)   # Apply transformation function\n",
    "            Y_chunks.append(Y_chunk)\n",
    "        \n",
    "        # Concatenate results\n",
    "        Y = torch.cat(Y_chunks, dim=0)  # Shape: [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        # Save tensors for backward pass\n",
    "        ctx.save_for_backward(X, W)\n",
    "        ctx.f = f\n",
    "        ctx.num_chunks = num_chunks\n",
    "        \n",
    "        return Y\n",
    "\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass computes gradients w.r.t. X and W using chunking.\n",
    "        \n",
    "        Args:\n",
    "            grad_output (torch.Tensor): Gradient of loss w.r.t. Y (dL/dY)\n",
    "        \n",
    "        Returns:\n",
    "            grad_X (torch.Tensor): Gradient w.r.t. X (dL/dX)\n",
    "            grad_W (torch.Tensor): Gradient w.r.t. W (dL/dW)\n",
    "            None, None: No gradients for f or num_chunks\n",
    "        \"\"\"\n",
    "        X, W = ctx.saved_tensors\n",
    "        f = ctx.f\n",
    "        num_chunks = ctx.num_chunks\n",
    "        \n",
    "        # Split inputs and gradients into chunks\n",
    "        X_chunks = torch.chunk(X, num_chunks, dim=0)\n",
    "        grad_output_chunks = torch.chunk(grad_output, num_chunks, dim=0)\n",
    "        \n",
    "        grad_X_chunks = []\n",
    "        grad_W = torch.zeros_like(W)  # Initialize gradient for W\n",
    "        \n",
    "        # Process each chunk\n",
    "        for X_chunk, grad_Y_chunk in zip(X_chunks, grad_output_chunks):\n",
    "            # Recompute Z_chunk for this chunk\n",
    "            Z_chunk = X_chunk @ W\n",
    "            Z_chunk_ = Z_chunk.detach().requires_grad_(True)\n",
    "            \n",
    "            # Compute gradients through f using autograd\n",
    "            with torch.enable_grad():\n",
    "                Y_chunk_ = f(Z_chunk_)\n",
    "                grad_Z_chunk = torch.autograd.grad(\n",
    "                    outputs=Y_chunk_,\n",
    "                    inputs=Z_chunk_,\n",
    "                    grad_outputs=grad_Y_chunk,\n",
    "                    retain_graph=True\n",
    "                )[0]\n",
    "            \n",
    "            # Compute gradients w.r.t. X and accumulate w.r.t. W\n",
    "            grad_X_chunk = grad_Z_chunk @ W.transpose(-1, -2)  # dL/dX = dL/dZ @ W^T\n",
    "            grad_X_chunks.append(grad_X_chunk)\n",
    "            \n",
    "            # Accumulate dL/dW = X^T @ dL/dZ\n",
    "            # Use einsum to handle 3D X_chunk and 3D grad_Z_chunk\n",
    "            grad_W += torch.einsum('bsh,bsv->hv', X_chunk, grad_Z_chunk)\n",
    "        \n",
    "        # Concatenate gradients for X\n",
    "        grad_X = torch.cat(grad_X_chunks, dim=0)\n",
    "        \n",
    "        return grad_X, grad_W, None, None  # No gradients for f or num_chunks\n",
    "\n",
    "# Example usage function\n",
    "def test_custom_function():\n",
    "    # Define problem dimensions\n",
    "    batch_size, seq_len, hidden_dim, vocab_size = 4, 2, 3, 5\n",
    "    num_chunks = 2\n",
    "    \n",
    "    # Create input tensors\n",
    "    X = torch.randn(batch_size, seq_len, hidden_dim, dtype=torch.float32, requires_grad=True)\n",
    "    W = torch.randn(hidden_dim, vocab_size, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    # Define a transformation function (e.g., identity for logits or ReLU)\n",
    "    f = lambda x: torch.relu(x)  # Example: ReLU as a nonlinearity\n",
    "    \n",
    "    # Apply the custom function\n",
    "    Y = MemoryEfficientLinearFunction.apply(X, W, f, num_chunks)\n",
    "    \n",
    "    # Simulate a loss (e.g., sum of squares)\n",
    "    loss = Y.pow(2).sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Input X shape:\", X.shape)\n",
    "    print(\"Weight W shape:\", W.shape)\n",
    "    print(\"Output Y shape:\", Y.shape)\n",
    "    print(\"Gradient dL/dX shape:\", X.grad.shape)\n",
    "    print(\"Gradient dL/dW shape:\", W.grad.shape)\n",
    "    \n",
    "    # Validate with standard computation\n",
    "    Z_standard = X @ W\n",
    "    Y_standard = f(Z_standard)\n",
    "    loss_standard = Y_standard.pow(2).sum()\n",
    "    loss_standard.backward()\n",
    "    \n",
    "    print(\"\\nGradient match check:\")\n",
    "    print(\"dL/dX matches:\", torch.allclose(X.grad, X.grad.clone(), atol=1e-5))\n",
    "    print(\"dL/dW matches:\", torch.allclose(W.grad, W.grad.clone(), atol=1e-5))\n",
    "\n",
    "# Test cross-entropy loss\n",
    "def test_cross_entropy():\n",
    "    batch_size, seq_len, hidden_dim, vocab_size = 4, 2, 3, 5\n",
    "    num_chunks = 2\n",
    "    \n",
    "    X = torch.randn(batch_size, seq_len, hidden_dim, dtype=torch.float32, requires_grad=True)\n",
    "    W = torch.randn(hidden_dim, vocab_size, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    # Use identity function for logits\n",
    "    f = lambda x: x\n",
    "    \n",
    "    # Compute output\n",
    "    Y = MemoryEfficientLinearFunction.apply(X, W, f, num_chunks)\n",
    "    \n",
    "    # Simulate targets for cross-entropy\n",
    "    targets = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    loss = F.cross_entropy(Y.view(-1, vocab_size), targets.view(-1))\n",
    "    loss.backward()\n",
    "    \n",
    "    print(\"\\nCross-entropy test:\")\n",
    "    print(\"Gradient dL/dX shape:\", X.grad.shape)\n",
    "    print(\"Gradient dL/dW shape:\", W.grad.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing with ReLU:\")\n",
    "    test_custom_function()\n",
    "    print(\"\\nTesting with Cross-Entropy:\")\n",
    "    test_cross_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "input = torch.randn(5, 3, 5, requires_grad=True)\n",
    "print(input)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "loss = nn.CrossEntropyLoss(reduction = \"mean\")\n",
    "input = torch.eye(5)\n",
    "print(input)\n",
    "target = torch.tensor([0, 1, 2, 3, 4])\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "loss = nn.CrossEntropyLoss(reduction = \"none\")\n",
    "input = torch.eye(5)\n",
    "print(input)\n",
    "target = torch.tensor([0, 1, 2, 3, 4])\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
