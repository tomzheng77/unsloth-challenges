FullyShardedDataParallel(
  (_fsdp_wrapped_module): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): FullyShardedDataParallel(
            (_fsdp_wrapped_module): Embedding(128256, 4096, padding_idx=128004)
          )
          (layers): ModuleList(
            (0-31): 32 x LlamaDecoderLayer(
              (self_attn): FullyShardedDataParallel(
                (_fsdp_wrapped_module): LlamaSdpaAttention(
                  (q_proj): lora.Linear4bit(
                    (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                    (lora_dropout): ModuleDict(
                      (default): Identity()
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=4096, out_features=64, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=64, out_features=4096, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (k_proj): lora.Linear4bit(
                    (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                    (lora_dropout): ModuleDict(
                      (default): Identity()
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=4096, out_features=64, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=64, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (v_proj): lora.Linear4bit(
                    (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                    (lora_dropout): ModuleDict(
                      (default): Identity()
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=4096, out_features=64, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=64, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (o_proj): lora.Linear4bit(
                    (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                    (lora_dropout): ModuleDict(
                      (default): Identity()
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=4096, out_features=64, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=64, out_features=4096, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (rotary_emb): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): LlamaRotaryEmbedding()
                  )
                )
              )
              (mlp): FullyShardedDataParallel(
                (_fsdp_wrapped_module): LlamaMLP(
                  (gate_proj): lora.Linear4bit(
                    (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                    (lora_dropout): ModuleDict(
                      (default): Identity()
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=4096, out_features=64, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=64, out_features=14336, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (up_proj): lora.Linear4bit(
                    (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                    (lora_dropout): ModuleDict(
                      (default): Identity()
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=4096, out_features=64, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=64, out_features=14336, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (down_proj): lora.Linear4bit(
                    (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                    (lora_dropout): ModuleDict(
                      (default): Identity()
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=14336, out_features=64, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=64, out_features=4096, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (act_fn): SiLU()
                )
              )
              (input_layernorm): FullyShardedDataParallel(
                (_fsdp_wrapped_module): LlamaRMSNorm((2048,), eps=1e-05)
              )
              (post_attention_layernorm): FullyShardedDataParallel(
                (_fsdp_wrapped_module): LlamaRMSNorm((2048,), eps=1e-05)
              )
            )
          )
          (norm): FullyShardedDataParallel(
            (_fsdp_wrapped_module): LlamaRMSNorm((2048,), eps=1e-05)
          )
          (rotary_emb): FullyShardedDataParallel(
            (_fsdp_wrapped_module): LlamaRotaryEmbedding()
          )
        )
        (lm_head): FullyShardedDataParallel(
          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=128256, bias=False)
        )
      )
    )
  )
)
fsdp_count_after=164

 [10/10 05:14, Epoch 0/1]
Step	Training Loss
1	9.422600
2	10.694800
3	8.159900
4	7.568300
5	7.623100
6	7.465700
7	8.128100
8	7.918200
9	7.233900
10	5.895600
 [10/10 05:14, Epoch 0/1]
Step	Training Loss
1	9.422600
2	10.694800
3	8.159900
4	7.568300
5	7.623100
6	7.465700
7	8.128100
8	7.918200
9	7.233900
10	5.895600

 [10/10 01:06, Epoch 0/1]
Step	Training Loss
1	9.394700
2	9.443500
3	11.914400
4	9.053400
5	8.022000
6	7.508000
7	6.865400
8	7.310600
9	6.559400
10	7.795200
