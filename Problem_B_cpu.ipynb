{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful functions used through the entire notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import set_seed\n",
    "import time\n",
    "import inspect\n",
    "import os\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "HAS_BFLOAT16 = (major_version >= 8)\n",
    "from inspect import currentframe as _C, getframeinfo\n",
    "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
    "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
    "\n",
    "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
    "def NAME(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "    return names[0] if len(names) != 0 else \"\"\n",
    "\n",
    "def assert_same(x, y, line, dtype):\n",
    "    assert(x.dtype == dtype)\n",
    "    try: torch.testing.assert_close(x, y, check_stride = True)\n",
    "    except Exception as error:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
    "        )\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPFUL functions to undo Unsloth patches:\n",
    "import sys\n",
    "\n",
    "def remove_patched_module(package_name):\n",
    "    modules_to_delete = [\n",
    "        name for name in sys.modules\n",
    "        if name == package_name or name.startswith(package_name + \".\")\n",
    "    ]\n",
    "    for name in modules_to_delete: del sys.modules[name]\n",
    "\n",
    "remove_patched_module(\"trl\")\n",
    "remove_patched_module(\"transformers\")\n",
    "remove_patched_module(\"peft\")\n",
    "remove_patched_module(\"bitsandbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,\"\\\n",
    "    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "max_seq_length = 2048\n",
    "torch.set_default_dtype(torch.float16)\n",
    "model_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "dtype = torch.float16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit              = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type       = \"nf4\",\n",
    "    bnb_4bit_compute_dtype    = dtype,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = \"auto\",\n",
    "    attn_implementation = \"sdpa\",\n",
    "    quantization_config = bnb_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 64,\n",
    "    lora_alpha = 128,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Get LoRA and setup model\n",
    "model = get_peft_model(model, lora_config)\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        if \".lora_A.\" in name or \".lora_B.\" in name: param.requires_grad_(True)\n",
    "        else: param.requires_grad_(False)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Get dataset\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train[:10%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    processing_class = tokenizer,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 1,\n",
    "        max_steps = 10,\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"outputs\",\n",
    "        seed = 3407,\n",
    "        max_seq_length = max_seq_length,\n",
    "        fp16 = model.get_input_embeddings().weight.dtype == torch.float16,\n",
    "        bf16 = model.get_input_embeddings().weight.dtype == torch.bfloat16,\n",
    "        report_to = \"none\", # For W&B\n",
    "        dataset_num_proc = 4,\n",
    "    ),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
