import torch
import os
import triton
import triton.language as tl

os.environ['TORCH_COMPILE_DEBUG'] = '1'
os.environ['TRITON_DEBUG'] = '1'
os.environ["TORCHDYNAMO_VERBOSE"] = "1"
os.environ["TORCHINDUCTOR_FORCE_DISABLE_CACHES"] = "1"
os.environ["TORCHINDUCTOR_COMPILE_THREADS"] = "1"


# this was the code generated by torch.compile, it produces ptx with
# cvt.f32.bf16 %r3, %rs4;
# which isn't supported by Tesla T4
@triton.jit
def triton_poi_fused__to_copy_mul_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    tmp0 = tl.load(in_ptr0 + (0)).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK])
    tmp3 = tl.load(in_ptr1 + (0)).to(tl.float32)
    tmp4 = tl.broadcast_to(tmp3, [XBLOCK])
    tmp2 = tmp1.to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tmp6 = tmp2 * tmp5
    tl.store(out_ptr0 + (tl.full([XBLOCK], 0, tl.int32)), tmp6, None)


# Define BF16 tensors on a T4 GPU
a = torch.tensor([1.2345], dtype=torch.bfloat16, device='cuda')
b = torch.tensor([2.3456], dtype=torch.bfloat16, device='cuda')
out = torch.tensor([0], dtype=torch.float32, device='cuda')

grid = (1,)
triton_poi_fused__to_copy_mul_0[grid](a, b, out, 1, 1)

print(out)
print(list(triton_poi_fused__to_copy_mul_0.cache[0].values())[0].asm['ptx'])
