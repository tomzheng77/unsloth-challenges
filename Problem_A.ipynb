{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful functions used through the entire notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import set_seed\n",
    "import time\n",
    "import inspect\n",
    "import os\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "HAS_BFLOAT16 = (major_version >= 8)\n",
    "from inspect import currentframe as _C, getframeinfo\n",
    "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
    "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
    "\n",
    "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
    "def NAME(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "    return names[0] if len(names) != 0 else \"\"\n",
    "\n",
    "def assert_same(x, y, line, dtype):\n",
    "    assert(x.dtype == dtype)\n",
    "    try: torch.testing.assert_close(x, y, check_stride = True)\n",
    "    except Exception as error:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
    "        )\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitsandbytes.nn import Linear4bit\n",
    "from transformers.activations import ACT2FN\n",
    "from unsloth.kernels.utils import fast_dequantize\n",
    "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
    "def unsloth_dequantize(weight):\n",
    "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
    "\n",
    "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
    "    return Linear4bit(\n",
    "        hd, m, bias = None,\n",
    "        compute_dtype       = dtype,\n",
    "        compress_statistics = True,\n",
    "        quant_type          = \"nf4\",\n",
    "    )\n",
    "\n",
    "# [NEW] as at 18th Feb 2025\n",
    "def assert_correct_bnb(weight, dtype):\n",
    "    assert(weight.weight.dtype == torch.uint8)\n",
    "    assert(weight.weight.quant_state.dtype == dtype)\n",
    "    assert(weight.weight.quant_state.absmax.dtype == torch.uint8)\n",
    "    assert(weight.weight.quant_state.code.dtype == torch.float32)\n",
    "    assert(weight.weight.quant_state.offset.dtype == torch.float32)\n",
    "    assert(weight.weight.quant_state.blocksize == 64)\n",
    "    assert(weight.weight.quant_state.state2.absmax.dtype == torch.float32)\n",
    "    assert(weight.weight.quant_state.state2.code.dtype == torch.float32)\n",
    "    assert(weight.weight.quant_state.state2.blocksize == 256)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hd = 4096, m = 14336, dtype = torch.float16):\n",
    "        super().__init__()\n",
    "        self.gate_proj = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
    "        self.up_proj   = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
    "        self.down_proj = bnb_Linear4bit(m, hd, dtype = dtype).to(\"cuda\")\n",
    "        # [NEW] as at 18th Feb 2025\n",
    "        self.gate_proj.weight.quant_state.dtype = dtype\n",
    "        self.up_proj  .weight.quant_state.dtype = dtype\n",
    "        self.down_proj.weight.quant_state.dtype = dtype\n",
    "        self.act_fn = ACT2FN[\"silu\"]\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "def mlp_forward(X, mlp, fx):\n",
    "    up   = X @ fx(mlp.  up_proj).t()\n",
    "    gate = X @ fx(mlp.gate_proj).t()\n",
    "    h = mlp.act_fn(gate) * up\n",
    "    down = h @ fx(mlp.down_proj).t()\n",
    "    return down\n",
    "\n",
    "def mlp_dequantize(X, mlp, fx):\n",
    "    a = fx(mlp.  up_proj).t(); torch.cuda.synchronize()\n",
    "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
    "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
    "    return a, b, c\n",
    "\n",
    "def test_dequantize(dequantize_fx):\n",
    "    elapsed = 0\n",
    "    options = [\n",
    "        (2, 3333, 2048,  8192, 3407, torch.float16),\n",
    "        (5,  777, 1024,  4096, 3409, torch.bfloat16),\n",
    "        (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
    "    ]\n",
    "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
    "        set_seed(seed)\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        mlp = MLP(hd = hd, m = m, dtype = dt)\n",
    "        X = torch.randn((bsz, qlen, hd), device = \"cuda\", dtype = dt)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Warmup\n",
    "        for _ in range(2):\n",
    "            assert_same( mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
    "            # [NEW] as at 18th Feb 2025\n",
    "            assert_correct_bnb(mlp.  up_proj, dt)\n",
    "            assert_correct_bnb(mlp.gate_proj, dt)\n",
    "            assert_correct_bnb(mlp.down_proj, dt)\n",
    "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
    "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
    "            assert_same(a, A, _F(_C()), dt)\n",
    "            assert_same(b, B, _F(_C()), dt)\n",
    "            assert_same(c, C, _F(_C()), dt)\n",
    "\n",
    "        # Benchmarking\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        for _ in range(1000): mlp_dequantize(X, mlp, dequantize_fx)\n",
    "        elapsed += time.time() - start\n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.kernels.utils import fast_dequantize\n",
    "def unsloth_dequantize(weight):\n",
    "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
    "test_dequantize(unsloth_dequantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
    "test_dequantize(peft_dequantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from triton import jit\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def _your_dequantize_nf4_kernel():\n",
    "    ### TRITON CODE GOES HERE\n",
    "    return\n",
    "\n",
    "def _your_dequantize_nf4(weight, quant_state):\n",
    "    ### SETUP TRITON LAUNCH HERE\n",
    "    return None\n",
    "\n",
    "def your_dequantize_nf4(weight):\n",
    "    return _your_dequantize_nf4(weight.weight.data, weight.weight.quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch._inductor.config as inductor_config\n",
    "\n",
    "# Enable Inductor debug output (which may include Triton if used)\n",
    "inductor_config.debug = True\n",
    "inductor_config.verbose = True\n",
    "inductor_config.trace.enabled = True  # Enable tracing for Inductor\n",
    "inductor_config.triton.cudagraphs = False  # Disable cudagraphs for clearer output\n",
    "\n",
    "@torch.compile(backend=\"inductor\")\n",
    "def multiply_2(x):\n",
    "    return x * 2\n",
    "\n",
    "# Test with a sample input\n",
    "x = torch.randn(10, device=\"cuda\")\n",
    "y = multiply_2(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "\n",
    "@triton.jit\n",
    "def fused_dequantize_kernel(\n",
    "    a_ptr,              # Input: packed 4-bit tensor (uint8)\n",
    "    quant_absmax_ptr,   # Input: quant_state.absmax (uint8)\n",
    "    state2_code_ptr,    # Input: quant_state.state2.code (float32)\n",
    "    state2_absmax_ptr,  # Input: quant_state.state2.absmax (float32)\n",
    "    code_ptr,           # Input: quant_state.code (float32)\n",
    "    output_ptr,         # Output: dequantized result (bfloat16)\n",
    "    offset,             # Input: quant_state.offset (float32)\n",
    "    n_packed_elements,  # Number of uint8 elements in A\n",
    "    blocksize,          # Elements per block (e.g., 256)\n",
    "    BLOCK_SIZE: tl.constexpr  # Number of packed elements processed per thread block\n",
    "):\n",
    "    # Program ID: each thread block processes one output block\n",
    "    pid = tl.program_id(axis=0)\n",
    "    out_block_idx = pid\n",
    "    num_out_blocks = n_packed_elements * 2 // blocksize\n",
    "\n",
    "    # Early return if block index is out of bounds\n",
    "    if out_block_idx >= num_out_blocks:\n",
    "        return\n",
    "\n",
    "    # Compute the scaling factor for this block\n",
    "    absmax_idx = out_block_idx * blocksize\n",
    "    quant_absmax_val = tl.load(quant_absmax_ptr + absmax_idx).to(tl.int32)\n",
    "    code_val = tl.load(state2_code_ptr + quant_absmax_val)\n",
    "    state2_absmax_val = tl.load(state2_absmax_ptr + out_block_idx)\n",
    "    scaling = code_val * state2_absmax_val + offset\n",
    "\n",
    "    # Process packed elements for this block\n",
    "    packed_per_block = blocksize // 2  # Number of uint8 elements per block\n",
    "    packed_start = out_block_idx * packed_per_block\n",
    "    packed_offsets = packed_start + tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "    # Mask to prevent out-of-bounds access\n",
    "    packed_mask = packed_offsets < n_packed_elements\n",
    "\n",
    "    # Load packed uint8 values\n",
    "    packed_vals = tl.load(a_ptr + packed_offsets, mask=packed_mask, other=0).to(tl.uint8)\n",
    "\n",
    "    # Unpack 4-bit values\n",
    "    val0 = (packed_vals >> 4).to(tl.int32)    # High 4 bits\n",
    "    val1 = (packed_vals & 0b1111).to(tl.int32) # Low 4 bits\n",
    "\n",
    "    # Lookup dequantized values\n",
    "    result0 = tl.load(code_ptr + val0, mask=packed_mask, other=0.0)\n",
    "    result1 = tl.load(code_ptr + val1, mask=packed_mask, other=0.0)\n",
    "\n",
    "    # Apply scaling\n",
    "    result0 = result0 * scaling\n",
    "    result1 = result1 * scaling\n",
    "\n",
    "    # Compute output offsets (interleaved: val0, val1, val0, val1, ...)\n",
    "    out_start = out_block_idx * blocksize\n",
    "    out_offsets0 = out_start + 2 * tl.arange(0, BLOCK_SIZE)\n",
    "    out_offsets1 = out_offsets0 + 1\n",
    "    out_mask = out_offsets1 < (n_packed_elements * 2)\n",
    "\n",
    "    # Store results as bfloat16\n",
    "    tl.store(output_ptr + out_offsets0, result0.to(tl.bfloat16), mask=out_mask)\n",
    "    tl.store(output_ptr + out_offsets1, result1.to(tl.bfloat16), mask=out_mask)\n",
    "\n",
    "# Host function to launch the kernel\n",
    "def triton_fused_dequantize(A, quant_state):\n",
    "    \"\"\"\n",
    "    Fused dequantization of a 4-bit packed tensor using Triton.\n",
    "    \n",
    "    Args:\n",
    "        A: torch.Tensor of shape [1, n_packed_elements], dtype uint8, containing packed 4-bit values\n",
    "        quant_state: Object containing:\n",
    "            - absmax: torch.Tensor of shape [total_elements], dtype uint8\n",
    "            - state2.code: torch.Tensor, lookup table for absmax dequantization\n",
    "            - state2.absmax: torch.Tensor of shape [num_blocks], per-block scaling\n",
    "            - code: torch.Tensor, lookup table for 4-bit values\n",
    "            - offset: float, scaling offset\n",
    "            - blocksize: int, elements per block (e.g., 256)\n",
    "            - shape: tuple, desired output shape (e.g., [M, N])\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor of shape quant_state.shape, dtype bfloat16\n",
    "    \"\"\"\n",
    "    assert A.is_cuda, \"Input tensor must be on CUDA\"\n",
    "    n_packed_elements = A.shape[1]\n",
    "    output = torch.empty(quant_state.shape, dtype=torch.bfloat16, device=\"cuda\").t().contiguous()\n",
    "\n",
    "    blocksize = quant_state.blocksize\n",
    "    num_out_blocks = (n_packed_elements * 2) // blocksize\n",
    "    BLOCK_SIZE = blocksize // 2  # Number of packed elements per thread block\n",
    "\n",
    "    grid = (num_out_blocks,)\n",
    "    fused_dequantize_kernel[grid](\n",
    "        A,\n",
    "        quant_state.absmax,\n",
    "        quant_state.state2.code,\n",
    "        quant_state.state2.absmax,\n",
    "        quant_state.code,\n",
    "        output,\n",
    "        quant_state.offset,\n",
    "        n_packed_elements,\n",
    "        blocksize,\n",
    "        BLOCK_SIZE=BLOCK_SIZE\n",
    "    )\n",
    "    return output.t()  # Reshape to original dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
