{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan: as part of patching the forward function, it would need to call the patched version of dequantize_4bit,\n",
    "# and possibly a version of it that also does transposing\n",
    "from bitsandbytes import functional\n",
    "if not hasattr(functional, 'original_dequantize_4bit'):\n",
    "    functional.original_dequantize_4bit = functional.dequantize_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "# NOTE: the special patching happens here as I'm not sure if this being imported\n",
    "# later will cause it to be un-patchable\n",
    "next_case_index = 0\n",
    "DIR_NAME = 'dequantize_4bit_cases'\n",
    "PREFIX = 'case_'\n",
    "\n",
    "# clear the directory first\n",
    "for file in os.listdir(DIR_NAME):\n",
    "    os.remove(f'{DIR_NAME}/{file}')\n",
    "\n",
    "def printing_dequantize_4bit(\n",
    "    A, # torch.Tensor\n",
    "    quant_state = None, # Optional[QuantState]\n",
    "    absmax = None, # Optional[torch.Tensor]\n",
    "    out = None, # Optional[torch.Tensor]\n",
    "    blocksize = 64, # int\n",
    "    quant_type = \"fp4\",\n",
    "):\n",
    "    assert(quant_state is not None)\n",
    "    assert(absmax is None)\n",
    "    assert(out is None)\n",
    "    assert(blocksize == 64)\n",
    "    assert(quant_type == \"fp4\")\n",
    "\n",
    "    global next_case_index\n",
    "    os.makedirs(DIR_NAME, exist_ok=True)\n",
    "    \n",
    "    torch.save(A, f'{DIR_NAME}/{PREFIX}{next_case_index}_A.pt')\n",
    "    if quant_state is not None:\n",
    "        qs_dict = quant_state.as_dict(packed = True)\n",
    "        # recurse through qs_dict, replace each tensor with a filename\n",
    "        for key, value in qs_dict.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                torch.save(value, f'{DIR_NAME}/{PREFIX}{next_case_index}_{key}.pt')\n",
    "                qs_dict[key] = f'{DIR_NAME}/{PREFIX}{next_case_index}_{key}.pt'\n",
    "        \n",
    "        with open(f'{DIR_NAME}/{PREFIX}{next_case_index}_qs.json', 'w') as f:\n",
    "            json.dump(qs_dict, f)\n",
    "\n",
    "    next_case_index += 1\n",
    "    return functional.original_dequantize_4bit(A, quant_state, absmax, out, blocksize, quant_type)\n",
    "\n",
    "functional.dequantize_4bit = printing_dequantize_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of cases that have been dumped\n",
    "# and the number that have quant_state\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "num_cases = 0\n",
    "num_qstate = 0\n",
    "for file in os.listdir(DIR_NAME):\n",
    "    if file.endswith('_A.pt'):\n",
    "        num_cases += 1\n",
    "        if os.path.exists(f'{DIR_NAME}/{file[:-5]}_qs.json'):\n",
    "            num_qstate += 1\n",
    "\n",
    "print(f'Number of cases: {num_cases}')\n",
    "print(f'Number of cases with quant_state: {num_qstate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of reading a specific case that was dumped\n",
    "import torch\n",
    "CASE_INDEX = 3333\n",
    "if os.path.exists(f'{DIR_NAME}/{PREFIX}{CASE_INDEX}_A.pt'):\n",
    "    A = torch.load(f'{DIR_NAME}/{PREFIX}{CASE_INDEX}_A.pt')\n",
    "    with open(f'{DIR_NAME}/{PREFIX}{CASE_INDEX}_qs.json', 'r') as f:\n",
    "        qs_dict = json.load(f)\n",
    "        for key, value in qs_dict.items():\n",
    "            if isinstance(value, str) and value.endswith('.pt'):\n",
    "                qs_dict[key] = torch.load(value)\n",
    "        quant_state = functional.QuantState.from_dict(qs_dict, 'cuda')\n",
    "        print(quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def augmented_dequantize_4bit(\n",
    "    A, # torch.Tensor\n",
    "    quant_state = None, # Optional[QuantState]\n",
    "    absmax = None, # Optional[torch.Tensor]\n",
    "    out = None, # Optional[torch.Tensor]\n",
    "    blocksize = 64, # int\n",
    "    quant_type = \"fp4\",\n",
    "):\n",
    "    assert(quant_state is not None)\n",
    "    assert(absmax is None)\n",
    "    assert(out is None)\n",
    "    assert(blocksize == 64)\n",
    "    assert(quant_type == \"fp4\")\n",
    "\n",
    "    return functional.original_dequantize_4bit(A, quant_state, absmax, out, blocksize, quant_type)\n",
    "\n",
    "# functional.dequantize_4bit = augmented_dequantize_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan: patch the forward function of nn.Linear4bit, such that it can be torch.compile(d)\n",
    "# this should mean that the backward pass can also be automatically derived\n",
    "# first lets store the original functions before patching, this cell should not be modified\n",
    "import bitsandbytes as bnb\n",
    "if not hasattr(bnb.nn.Linear4bit, 'original_forward'):\n",
    "    bnb.nn.Linear4bit.original_forward = bnb.nn.Linear4bit.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printing_4bit_forward(self, *args, **kwargs):\n",
    "    print(self.weight)\n",
    "    return self.original_forward(*args, **kwargs)\n",
    "\n",
    "bnb.nn.Linear4bit.forward = printing_4bit_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful functions used through the entire notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import set_seed\n",
    "import time\n",
    "import inspect\n",
    "import os\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "HAS_BFLOAT16 = (major_version >= 8)\n",
    "from inspect import currentframe as _C, getframeinfo\n",
    "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
    "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
    "\n",
    "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
    "def NAME(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "    return names[0] if len(names) != 0 else \"\"\n",
    "\n",
    "def assert_same(x, y, line, dtype):\n",
    "    assert(x.dtype == dtype)\n",
    "    try: torch.testing.assert_close(x, y, check_stride = True)\n",
    "    except Exception as error:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
    "        )\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch_compile_options = torch_compile_options = {\n",
    "    \"epilogue_fusion\"   : True,\n",
    "    \"max_autotune\"      : True,\n",
    "    \"shape_padding\"     : True,\n",
    "    \"trace.enabled\"     : True,\n",
    "    \"triton.cudagraphs\" : False,\n",
    "}\n",
    "\n",
    "@torch.compile(fullgraph = False, dynamic = True, options = torch_compile_options)\n",
    "def compiled_llama_mlp(self, x):\n",
    "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "    return down_proj\n",
    "\n",
    "import transformers.models.llama.modeling_llama\n",
    "transformers.models.llama.modeling_llama.LlamaMLP.forward = compiled_llama_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \\\n",
    "    \"expandable_segments:True,\"\\\n",
    "    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n",
    "\n",
    "max_seq_length = 1024\n",
    "torch.set_default_dtype(torch.float16)\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "dtype = torch.float16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit              = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type       = \"nf4\",\n",
    "    bnb_4bit_compute_dtype    = dtype,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = \"auto\",\n",
    "    attn_implementation = \"sdpa\",\n",
    "    quantization_config = bnb_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 32,\n",
    "    lora_alpha = 64,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Get LoRA and setup model\n",
    "model = get_peft_model(model, lora_config)\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        if \".lora_A.\" in name or \".lora_B.\" in name: param.requires_grad_(True)\n",
    "        else: param.requires_grad_(False)\n",
    "\n",
    "# Currently GC will cause torch.compile to be disabled, so disable it\n",
    "# model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Get dataset\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train[:10%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must show all graph breaks are not seen with torch.compile\n",
    "import os\n",
    "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_FORCE_DISABLE_CACHES\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\n",
    "\n",
    "import logging\n",
    "torch._inductor.config.debug = True\n",
    "torch._logging.set_logs(\n",
    "    dynamo = logging.WARN,\n",
    "    inductor = logging.WARN,\n",
    "    graph_breaks = True,\n",
    "    recompiles = True,\n",
    "    recompiles_verbose = True,\n",
    "    compiled_autograd_verbose = True,\n",
    "    # aot_joint_graph = True, # Enable for more logs\n",
    "    # aot_graphs = True,\n",
    ")\n",
    "torch._dynamo.config.verbose = True\n",
    "torch._dynamo.config.suppress_errors = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fast Iteration Cell, run this and the one below to quickly iterate on the forward function\n",
    "def printing_4bit_forward(self, *args, **kwargs):\n",
    "    # original forward function does matmul between weight transposed and X\n",
    "    # if we can assume weight always follows a certain pattern\n",
    "    print('==================================================')\n",
    "    qs = self.weight.quant_state\n",
    "    items = [\n",
    "        qs.absmax,\n",
    "        qs.shape,\n",
    "        qs.code,\n",
    "        qs.dtype,\n",
    "        qs.blocksize,\n",
    "        qs.quant_type,\n",
    "        qs.offset,\n",
    "        qs.state2,\n",
    "        qs.nested,\n",
    "    ]\n",
    "    # assert(qs.quant_type == 'nf4')\n",
    "    # assert(qs.nested == False)\n",
    "    # assert(qs.dtype == torch.bfloat16)\n",
    "    print('quant_state', items)\n",
    "    print(self.weight)\n",
    "    print('--------------------------------------------------')\n",
    "    print(self.weight.t())\n",
    "    print('==================================================')\n",
    "    return self.original_forward(*args, **kwargs)\n",
    "\n",
    "# bnb.nn.Linear4bit.forward = printing_4bit_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    processing_class = tokenizer,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 1,\n",
    "        max_steps = 10,\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"outputs\",\n",
    "        seed = 3407,\n",
    "        max_seq_length = max_seq_length,\n",
    "        fp16 = model.get_input_embeddings().weight.dtype == torch.float16,\n",
    "        bf16 = model.get_input_embeddings().weight.dtype == torch.bfloat16,\n",
    "        report_to = \"none\", # For W&B\n",
    "        dataset_num_proc = 4,\n",
    "    ),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
