{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bitsandbytes\n",
    "from typing import Any\n",
    "\n",
    "# Original Params4bit class (for reference, you donâ€™t need to define this)\n",
    "# class Params4bit:\n",
    "#     def __init__(self, data: torch.Tensor, **kwargs):\n",
    "#         self.data = data\n",
    "#         self.__dict__.update(kwargs)\n",
    "#     def some_method(self):\n",
    "#         return self.data * 2\n",
    "\n",
    "# Your wrapper class\n",
    "OriginalParams4bit = bitsandbytes.nn.Params4bit\n",
    "class Params4bitWrap:\n",
    "    def __init__(self, data: torch.Tensor, *args, **kwargs):\n",
    "        # Explicitly store the tensor\n",
    "        assert(isinstance(data, torch.Tensor) == True)\n",
    "        self._params4bit = OriginalParams4bit(data, *args, **kwargs)\n",
    "        self.data = data\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        # Delegate undefined attributes/methods to _params4bit\n",
    "        if name == \"data\":\n",
    "            return object.__getattribute__(self, name)\n",
    "        if name == \"_params4bit\":\n",
    "            return object.__getattribute__(self, name)\n",
    "        return getattr(self._params4bit, name)\n",
    "    \n",
    "    def __setattr__(self, name: str, value: Any) -> None:\n",
    "        # Special handling for 'data' to keep it in sync\n",
    "        if name == \"data\":\n",
    "            # Set self.data directly as a tensor\n",
    "            assert(isinstance(value, torch.Tensor) == True)\n",
    "            object.__setattr__(self, name, value)\n",
    "            self._params4bit.data = value\n",
    "        elif name == \"_params4bit\":\n",
    "            object.__setattr__(self, name, value)\n",
    "        else:\n",
    "            setattr(self._params4bit, name, value)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return self._params4bit.__getstate__()\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self._params4bit.__setstate__(state)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_prequantized(cls, data, *args, **kwargs):\n",
    "        raise NotImplementedError(\"This method is not implemented\")\n",
    "        # Delegate to the original Params4bit.from_prequantized\n",
    "        # original_instance = OriginalParams4bit.from_prequantized(data, *args, **kwargs)\n",
    "        # wrapped_instance = Params4bitWrap.__new__(cls)\n",
    "        # wrapped_instance.data = data\n",
    "        # wrapped_instance._params4bit = original_instance\n",
    "        # return wrapped_instance\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        raise NotImplementedError(\"This method is not implemented\")\n",
    "        # new_instance = type(self).__new__(type(self))\n",
    "        # state = self.__getstate__()\n",
    "        # new_instance.__setstate__(state)\n",
    "        # return new_instance\n",
    "\n",
    "    def __copy__(self):\n",
    "        # original_instance = self._params4bit.__copy__()\n",
    "        raise NotImplementedError(\"This method is not implemented\")\n",
    "        # return original_instance\n",
    "        \n",
    "\n",
    "    def to_parameter(self):\n",
    "        return torch.nn.Parameter(self.data)\n",
    "\n",
    "\n",
    "# Monkey patch: Replace Params4bit with Params4bitWrap\n",
    "bitsandbytes.nn.Params4bit = Params4bitWrap\n",
    "\n",
    "\n",
    "# Now any code that imports and uses Params4bit will get Params4bitWrap instead\n",
    "@torch.compile(fullgraph=True)\n",
    "def my_function(param):\n",
    "    return param.data + 999  # Should work with torch.compile\n",
    "\n",
    "\n",
    "# Example: Simulate third-party code creating Params4bit\n",
    "from bitsandbytes.nn import Params4bit  # This is now Params4bitWrap\n",
    "\n",
    "param = Params4bit(torch.randn(5))\n",
    "result = my_function(param)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan: as part of patching the forward function, it would need to call the patched version of dequantize_4bit,\n",
    "# and possibly a version of it that also does transposing\n",
    "from bitsandbytes import functional\n",
    "\n",
    "from functions import my_dequantize_4bit\n",
    "\n",
    "if not hasattr(functional, 'original_dequantize_4bit'):\n",
    "    functional.original_dequantize_4bit = functional.dequantize_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "# NOTE: the special patching happens here as I'm not sure if this being imported\n",
    "# later will cause it to be un-patchable\n",
    "next_case_index = 0\n",
    "DIR_NAME = 'dequantize_4bit_cases'\n",
    "PREFIX = 'case_'\n",
    "\n",
    "# clear the directory first\n",
    "for file in os.listdir(DIR_NAME):\n",
    "    os.remove(f'{DIR_NAME}/{file}')\n",
    "\n",
    "def printing_dequantize_4bit(\n",
    "    A, # torch.Tensor\n",
    "    quant_state = None, # Optional[QuantState]\n",
    "    absmax = None, # Optional[torch.Tensor]\n",
    "    out = None, # Optional[torch.Tensor]\n",
    "    blocksize = 64, # int\n",
    "    quant_type = \"fp4\",\n",
    "):\n",
    "    assert(quant_state is not None)\n",
    "    assert(absmax is None)\n",
    "    assert(out is None)\n",
    "    assert(blocksize == 64)\n",
    "    assert(quant_type == \"fp4\")\n",
    "\n",
    "    global next_case_index\n",
    "    os.makedirs(DIR_NAME, exist_ok=True)\n",
    "    \n",
    "    torch.save(A, f'{DIR_NAME}/{PREFIX}{next_case_index}_A.pt')\n",
    "    if quant_state is not None:\n",
    "        qs_dict = quant_state.as_dict(packed = True)\n",
    "        # recurse through qs_dict, replace each tensor with a filename\n",
    "        for key, value in qs_dict.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                torch.save(value, f'{DIR_NAME}/{PREFIX}{next_case_index}_{key}.pt')\n",
    "                qs_dict[key] = f'{DIR_NAME}/{PREFIX}{next_case_index}_{key}.pt'\n",
    "        \n",
    "        with open(f'{DIR_NAME}/{PREFIX}{next_case_index}_qs.json', 'w') as f:\n",
    "            json.dump(qs_dict, f)\n",
    "\n",
    "    next_case_index += 1\n",
    "    return functional.original_dequantize_4bit(A, quant_state, absmax, out, blocksize, quant_type)\n",
    "\n",
    "functional.dequantize_4bit = printing_dequantize_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of cases that have been dumped\n",
    "# and the number that have quant_state\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "num_cases = 0\n",
    "num_qstate = 0\n",
    "for file in os.listdir(DIR_NAME):\n",
    "    if file.endswith('_A.pt'):\n",
    "        num_cases += 1\n",
    "        if os.path.exists(f'{DIR_NAME}/{file[:-5]}_qs.json'):\n",
    "            num_qstate += 1\n",
    "\n",
    "print(f'Number of cases: {num_cases}')\n",
    "print(f'Number of cases with quant_state: {num_qstate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of reading a specific case that was dumped\n",
    "import torch\n",
    "CASE_INDEX = 3333\n",
    "if os.path.exists(f'{DIR_NAME}/{PREFIX}{CASE_INDEX}_A.pt'):\n",
    "    A = torch.load(f'{DIR_NAME}/{PREFIX}{CASE_INDEX}_A.pt')\n",
    "    with open(f'{DIR_NAME}/{PREFIX}{CASE_INDEX}_qs.json', 'r') as f:\n",
    "        qs_dict = json.load(f)\n",
    "        for key, value in qs_dict.items():\n",
    "            if isinstance(value, str) and value.endswith('.pt'):\n",
    "                qs_dict[key] = torch.load(value)\n",
    "        quant_state = functional.QuantState.from_dict(qs_dict, 'cuda')\n",
    "        print(quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def augmented_dequantize_4bit(\n",
    "    A, # torch.Tensor\n",
    "    quant_state = None, # Optional[QuantState]\n",
    "    absmax = None, # Optional[torch.Tensor]\n",
    "    out = None, # Optional[torch.Tensor]\n",
    "    blocksize = 64, # int\n",
    "    quant_type = \"fp4\",\n",
    "):\n",
    "    assert(quant_state is not None)\n",
    "    assert(absmax is None)\n",
    "    assert(out is None)\n",
    "    assert(blocksize == 64)\n",
    "    assert(quant_type == \"fp4\")\n",
    "\n",
    "    return my_dequantize_4bit(A, quant_state)\n",
    "    # return functional.original_dequantize_4bit(A, quant_state, absmax, out, blocksize, quant_type)\n",
    "\n",
    "functional.dequantize_4bit = augmented_dequantize_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan: patch the forward function of nn.Linear4bit, such that it can be torch.compile(d)\n",
    "# this should mean that the backward pass can also be automatically derived\n",
    "# first lets store the original functions before patching, this cell should not be modified\n",
    "import bitsandbytes as bnb\n",
    "if not hasattr(bnb.nn.Linear4bit, 'original_forward'):\n",
    "    bnb.nn.Linear4bit.original_forward = bnb.nn.Linear4bit.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printing_4bit_forward(self, *args, **kwargs):\n",
    "    print(self.weight)\n",
    "    return self.original_forward(*args, **kwargs)\n",
    "\n",
    "bnb.nn.Linear4bit.forward = printing_4bit_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this cell is disabled, then it should show the custom dequantize function working\n",
    "# Transposition trick, to avoid having the transposition be done by calling Params4bit.t()\n",
    "# NOT transpose in the Linear4bit.forward, BUT secretly transpose in the MatMul4Bit\n",
    "# maybe instead of it calling MatMul4bit, it could call a custom implementation of torch Function\n",
    "\n",
    "import bitsandbytes.functional as F\n",
    "from typing import Optional\n",
    "from math import prod\n",
    "from bitsandbytes.nn.modules import fix_4bit_weight_quant_state_from_module\n",
    "from functions import ENABLE_ASSERTIONS\n",
    "\n",
    "class TransposeBMatMul4Bit(torch.autograd.Function):\n",
    "    # forward is the same, but we added the fallback for pre-turing GPUs\n",
    "    # backward is mostly the same, but adds one extra clause (see \"elif state.CxB is not None\")\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, B, out=None, bias=None, quant_state: Optional[F.QuantState] = None):\n",
    "        # default of pytorch behavior if inputs are empty\n",
    "        ctx.is_empty = False\n",
    "        if prod(A.shape) == 0:\n",
    "            ctx.is_empty = True\n",
    "            ctx.A = A\n",
    "            ctx.B = B\n",
    "            ctx.bias = bias\n",
    "            B_shape = quant_state.shape\n",
    "            if A.shape[-1] == B_shape[0]:\n",
    "                return torch.empty(A.shape[:-1] + B_shape[1:], dtype=A.dtype, device=A.device)\n",
    "            else:\n",
    "                return torch.empty(A.shape[:-1] + B_shape[:1], dtype=A.dtype, device=A.device)\n",
    "\n",
    "        # 1. Dequantize\n",
    "        # 2. MatmulnN\n",
    "        output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias) # NOTE the transposition\n",
    "\n",
    "        # 3. Save state\n",
    "        ctx.state = quant_state\n",
    "        ctx.dtype_A, ctx.dtype_B, ctx.dtype_bias = A.dtype, B.dtype, None if bias is None else bias.dtype\n",
    "\n",
    "        if any(ctx.needs_input_grad[:2]):\n",
    "            ctx.tensors = (None, B)\n",
    "        else:\n",
    "            ctx.tensors = (None, None)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if ctx.is_empty:\n",
    "            bias_grad = None if ctx.bias is None else torch.zeros_like(ctx.bias)\n",
    "            return torch.zeros_like(ctx.A), torch.zeros_like(ctx.B), None, bias_grad, None\n",
    "\n",
    "        req_gradA, _, _, req_gradBias, _ = ctx.needs_input_grad\n",
    "        _, B = ctx.tensors\n",
    "\n",
    "        grad_A, grad_B, grad_bias = None, None, None\n",
    "\n",
    "        if req_gradBias:\n",
    "            # compute grad_bias first before changing grad_output dtype\n",
    "            grad_bias = grad_output.sum(0, dtype=ctx.dtype_bias)\n",
    "\n",
    "        # not supported by PyTorch. TODO: create work-around\n",
    "        # if req_gradB: grad_B = torch.matmul(grad_output.t(), A)\n",
    "        if req_gradA:\n",
    "            grad_A = torch.matmul(grad_output, F.dequantize_4bit(B, ctx.state).to(grad_output.dtype).t()) # NOTE the transposition\n",
    "\n",
    "        return grad_A, grad_B, None, grad_bias, None\n",
    "\n",
    "def inner_transpose_forward(self, x: torch.Tensor):\n",
    "    fix_4bit_weight_quant_state_from_module(self)\n",
    "\n",
    "    # weights are cast automatically as Int8Params, but the bias has to be cast manually\n",
    "    if self.bias is not None and self.bias.dtype != x.dtype:\n",
    "        self.bias.data = self.bias.data.to(x.dtype)\n",
    "\n",
    "    if not self.compute_type_is_set:\n",
    "        self.set_compute_type(x)\n",
    "        self.compute_type_is_set = True\n",
    "\n",
    "    inp_dtype = x.dtype\n",
    "    if self.compute_dtype is not None:\n",
    "        x = x.to(self.compute_dtype)\n",
    "\n",
    "    bias = None if self.bias is None else self.bias.to(self.compute_dtype)\n",
    "\n",
    "    # return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
    "    # NOTE: here we pass in the data (nf4-packed weights) directly as Matrix B\n",
    "    # we assume the GEMV path of bnb.matmul_4bit is never taken\n",
    "    A = x\n",
    "    B = self.weight.data\n",
    "    out = None\n",
    "    quant_state = self.weight.quant_state\n",
    "\n",
    "    # shape is  torch.Size([1, 100, 2048]) torch.Size([2097152, 1]) <reversed object at 0x7e1f00bb5cc0>\n",
    "    # print('shape is ', A.shape, B.shape, reversed(B.shape))\n",
    "    if ENABLE_ASSERTIONS:\n",
    "        assert(B.shape[1] == 1)\n",
    "    return TransposeBMatMul4Bit.apply(A, B.t(), out, bias, quant_state).to(inp_dtype)\n",
    "\n",
    "bnb.nn.Linear4bit.forward = inner_transpose_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful functions used through the entire notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import set_seed\n",
    "import time\n",
    "import inspect\n",
    "import os\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "HAS_BFLOAT16 = (major_version >= 8)\n",
    "from inspect import currentframe as _C, getframeinfo\n",
    "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
    "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
    "\n",
    "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
    "def NAME(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "    return names[0] if len(names) != 0 else \"\"\n",
    "\n",
    "def assert_same(x, y, line, dtype):\n",
    "    assert(x.dtype == dtype)\n",
    "    try: torch.testing.assert_close(x, y, check_stride = True)\n",
    "    except Exception as error:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
    "        )\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch_compile_options = torch_compile_options = {\n",
    "    \"epilogue_fusion\"   : True,\n",
    "    \"max_autotune\"      : True,\n",
    "    \"shape_padding\"     : True,\n",
    "    \"trace.enabled\"     : True,\n",
    "    \"triton.cudagraphs\" : False,\n",
    "}\n",
    "\n",
    "@torch.compile(fullgraph = False, dynamic = True, options = torch_compile_options)\n",
    "def compiled_llama_mlp(self, x):\n",
    "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "    return down_proj\n",
    "\n",
    "import transformers.models.llama.modeling_llama\n",
    "transformers.models.llama.modeling_llama.LlamaMLP.forward = compiled_llama_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \\\n",
    "    \"expandable_segments:True,\"\\\n",
    "    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n",
    "\n",
    "max_seq_length = 1024\n",
    "torch.set_default_dtype(torch.float16)\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "dtype = torch.float16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit              = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type       = \"nf4\",\n",
    "    bnb_4bit_compute_dtype    = dtype,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = \"auto\",\n",
    "    attn_implementation = \"sdpa\",\n",
    "    quantization_config = bnb_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 32,\n",
    "    lora_alpha = 64,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Get LoRA and setup model\n",
    "model = get_peft_model(model, lora_config)\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        if \".lora_A.\" in name or \".lora_B.\" in name: param.requires_grad_(True)\n",
    "        else: param.requires_grad_(False)\n",
    "\n",
    "# Currently GC will cause torch.compile to be disabled, so disable it\n",
    "# model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Get dataset\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train[:10%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must show all graph breaks are not seen with torch.compile\n",
    "import os\n",
    "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_FORCE_DISABLE_CACHES\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\n",
    "\n",
    "import logging\n",
    "torch._inductor.config.debug = True\n",
    "torch._logging.set_logs(\n",
    "    dynamo = logging.WARN,\n",
    "    inductor = logging.WARN,\n",
    "    graph_breaks = True,\n",
    "    recompiles = True,\n",
    "    recompiles_verbose = True,\n",
    "    compiled_autograd_verbose = True,\n",
    "    # aot_joint_graph = True, # Enable for more logs\n",
    "    # aot_graphs = True,\n",
    ")\n",
    "torch._dynamo.config.verbose = True\n",
    "torch._dynamo.config.suppress_errors = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fast Iteration Cell, run this and the one below to quickly iterate on the forward function\n",
    "def printing_4bit_forward(self, *args, **kwargs):\n",
    "    # original forward function does matmul between weight transposed and X\n",
    "    # if we can assume weight always follows a certain pattern\n",
    "    print('==================================================')\n",
    "    qs = self.weight.quant_state\n",
    "    items = [\n",
    "        qs.absmax,\n",
    "        qs.shape,\n",
    "        qs.code,\n",
    "        qs.dtype,\n",
    "        qs.blocksize,\n",
    "        qs.quant_type,\n",
    "        qs.offset,\n",
    "        qs.state2,\n",
    "        qs.nested,\n",
    "    ]\n",
    "    # assert(qs.quant_type == 'nf4')\n",
    "    # assert(qs.nested == False)\n",
    "    # assert(qs.dtype == torch.bfloat16)\n",
    "    print('quant_state', items)\n",
    "    print(self.weight)\n",
    "    print('--------------------------------------------------')\n",
    "    print(self.weight.t())\n",
    "    print('==================================================')\n",
    "    return self.original_forward(*args, **kwargs)\n",
    "\n",
    "# bnb.nn.Linear4bit.forward = printing_4bit_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    processing_class = tokenizer,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 1,\n",
    "        max_steps = 10,\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"outputs\",\n",
    "        seed = 3407,\n",
    "        max_seq_length = max_seq_length,\n",
    "        fp16 = model.get_input_embeddings().weight.dtype == torch.float16,\n",
    "        bf16 = model.get_input_embeddings().weight.dtype == torch.bfloat16,\n",
    "        report_to = \"none\", # For W&B\n",
    "        dataset_num_proc = 4,\n",
    "    ),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
