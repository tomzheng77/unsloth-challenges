V0326 23:29:15.685000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", 0]}
V0326 23:29:15.685000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["<frozen runpy>", 1]}
V0326 23:29:15.685000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/ipykernel_launcher.py", 2]}
V0326 23:29:15.686000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/traitlets/config/application.py", 3]}
V0326 23:29:15.686000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py", 4]}
V0326 23:29:15.686000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py", 5]}
V0326 23:29:15.686000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/usr/lib/python3.12/asyncio/base_events.py", 6]}
V0326 23:29:15.686000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/usr/lib/python3.12/asyncio/events.py", 7]}
V0326 23:29:15.686000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py", 8]}
V0326 23:29:15.687000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py", 9]}
V0326 23:29:15.687000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py", 10]}
V0326 23:29:15.687000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py", 11]}
V0326 23:29:15.687000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py", 12]}
V0326 23:29:15.687000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/tmp/ipykernel_324469/999499299.py", 13]}
V0326 23:29:15.688000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/transformers/trainer.py", 14]}
V0326 23:29:15.688000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py", 15]}
V0326 23:29:15.688000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", 16]}
V0326 23:29:15.688000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py", 17]}
V0326 23:29:15.688000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py", 18]}
V0326 23:29:15.688000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/peft_model.py", 19]}
V0326 23:29:15.689000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py", 20]}
V0326 23:29:15.689000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py", 21]}
V0326 23:29:15.689000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", 22]}
V0326 23:29:15.689000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/tmp/ipykernel_324469/3685874839.py", 23]}
V0326 23:29:15.689000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 333, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 1, "name": "compiled_rms_layernorm", "filename": 23}]}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:15.690000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "7175fb3410a4c9d531a102ea639f79ef"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992155689986.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:15.690000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "000c1d40f429e9929bef85d9bfd7b7b5"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992155689986.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:15.703000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 0, "size": 409600}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:15.703000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 3, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [1, 100, 2048], "is_leaf": true, "requires_grad": true, "stride": [204800, 2048, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b2d4a8c80>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:15.704000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 0, "source": "L['hidden_states']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:15.739000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 0, "size": 4096}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:15.739000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 1, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [2048], "is_leaf": true, "is_parameter": true, "stride": [1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x744b6c242f30>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:15.739000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 2, "source": "L['self']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:15.745000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", 24]}
V0326 23:29:15.745000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_utils_internal.py", 25]}
V0326 23:29:15.745000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", 26]}
V0326 23:29:15.746000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", 27]}
V0326 23:29:15.746000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/builtin.py", 28]}
V0326 23:29:15.746000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/builder.py", 29]}
V0326 23:29:15.746000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/utils.py", 30]}
V0326 23:29:15.746000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/utils/_stats.py", 31]}
V0326 23:29:15.746000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", 32]}
V0326 23:29:15.747000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_subclasses/fake_impls.py", 33]}
V0326 23:29:15.747000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/__init__.py", 34]}
V0326 23:29:15.747000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py", 35]}
V0326 23:29:15.747000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/fx/experimental/sym_node.py", 36]}
V0326 23:29:15.747000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/fx/experimental/recording.py", 37]}
V0326 23:29:15.748000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_logging/_internal.py", 38]}
V0326 23:29:15.748000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s1", "sources": ["L['hidden_states'].size()[2]"], "value": "2048", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 333, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2220, "name": "BINARY_OP", "filename": 27}, {"line": 301, "name": "impl", "filename": 27}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 711, "name": "<lambda>", "filename": 28}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 943, "name": "_handle_insert_op_in_graph", "filename": 28}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 1924, "name": "_dispatch_impl", "filename": 32}, {"line": 831, "name": "fast_binary_impl", "filename": 33}, {"line": 785, "name": "infer_size", "filename": 33}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 7, "name": "compiled_rms_layernorm", "filename": 23}]}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:15.753000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_hidden_states_": [1, "s0", "2048"], "l_self_parameters_weight_": [2048], "hidden_states": [1, "s0", "2048"], "pow_1": [1, "s0", "2048"], "variance": [1, "s0", 1], "add": [1, "s0", 1], "rsqrt": [1, "s0", 1], "hidden_states_1": [1, "s0", "2048"], "to_1": [1, "s0", "2048"], "mul_1": [1, "s0", 2048]}}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "e86a029eb98778a3a81be681cfbccd2a"}
	class GraphModule(torch.nn.Module):
	    def forward(self, s0: "Sym(s0)", L_hidden_states_: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", L_self_parameters_weight_: "f16[2048][1]cuda:0"):
	        l_hidden_states_ = L_hidden_states_
	        l_self_parameters_weight_ = L_self_parameters_weight_
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:4 in compiled_rms_layernorm, code: hidden_states = hidden_states.to(torch.float32)
	        hidden_states: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = l_hidden_states_.to(torch.float32);  l_hidden_states_ = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:5 in compiled_rms_layernorm, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
	        pow_1: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = hidden_states.pow(2)
	        variance: "f32[1, s0, 1][s0, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:6 in compiled_rms_layernorm, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
	        add: "f32[1, s0, 1][s0, 1, 1]cuda:0" = variance + 1e-05;  variance = None
	        rsqrt: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
	        hidden_states_1: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:7 in compiled_rms_layernorm, code: return self.weight * hidden_states.to(input_dtype)
	        to_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = hidden_states_1.to(torch.float16);  hidden_states_1 = None
	        mul_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = l_self_parameters_weight_ * to_1;  l_self_parameters_weight_ = to_1 = None
	        return (mul_1,)
	        
V0326 23:29:15.754000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "0dc44cd3fad91caea19ed2c3d11db303"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992155754518.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:15.754000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "bbd220fae632797e2371be4cc864c322"}
	{
	"name": "backend_compile",
	"ts": 1742992155754518.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.158000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "07d9b7e314fc43a5868977645b3ac3d4"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992156158649.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.208000 324469 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "de7c048f9c8c6a757ac5c7387615d124"}
	class joint_helper(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "Sym(s0)"; primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"; primals_3: "f16[2048][1]cuda:0"; tangents_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /tmp/ipykernel_324469/3685874839.py:4 in compiled_rms_layernorm, code: hidden_states = hidden_states.to(torch.float32)
	        convert_element_type: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.float32);  primals_2 = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:5 in compiled_rms_layernorm, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
	        pow_1: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 2)
	        mean: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:6 in compiled_rms_layernorm, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
	        add_9: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
	        rsqrt: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_9);  add_9 = None
	        alias: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt)
	        alias_1: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.alias.default(alias);  alias = None
	        mul_15: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type, rsqrt)
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:7 in compiled_rms_layernorm, code: return self.weight * hidden_states.to(input_dtype)
	        convert_element_type_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_15, torch.float16);  mul_15 = None
	        mul_22: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_3, convert_element_type_1);  convert_element_type_1 = None
	        mul_26: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, primals_3);  tangents_1 = primals_3 = None
	        convert_element_type_2: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_26, torch.float32);  mul_26 = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:6 in compiled_rms_layernorm, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
	        mul_27: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2, convert_element_type)
	        mul_28: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2, rsqrt);  convert_element_type_2 = rsqrt = None
	        sum_1: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_27, [2], True, dtype = torch.float32);  mul_27 = None
	        alias_2: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
	        alias_3: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
	        pow_2: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_3, 3);  alias_3 = None
	        mul_29: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_1, -0.5);  sum_1 = None
	        mul_30: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, pow_2);  mul_29 = pow_2 = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:5 in compiled_rms_layernorm, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
	        expand: "f32[1, s0, 2048][s0, 1, 0]cuda:0" = torch.ops.aten.expand.default(mul_30, [1, primals_1, 2048]);  mul_30 = primals_1 = None
	        div: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.div.Scalar(expand, 2048);  expand = None
	        pow_3: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 1.0);  convert_element_type = None
	        mul_31: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_3, 2.0);  pow_3 = None
	        mul_32: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div, mul_31);  div = mul_31 = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:5 in compiled_rms_layernorm, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
	        add_28: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_28, mul_32);  mul_28 = mul_32 = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:4 in compiled_rms_layernorm, code: hidden_states = hidden_states.to(torch.float32)
	        convert_element_type_3: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_28, torch.float16);  add_28 = None
	        return pytree.tree_unflatten([mul_22, None, convert_element_type_3, None], self._out_spec)
	        
V0326 23:29:16.330000 324469 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "1e1c16b0708cbed9278037163c3bf642"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "Sym(s0)", primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", primals_3: "f16[2048][1]cuda:0"):
	         # File: /tmp/ipykernel_324469/3685874839.py:4 in compiled_rms_layernorm, code: hidden_states = hidden_states.to(torch.float32)
	        convert_element_type: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.float32)
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:5 in compiled_rms_layernorm, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
	        pow_1: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 2)
	        mean: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:6 in compiled_rms_layernorm, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
	        add_9: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
	        rsqrt: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_9);  add_9 = None
	        mul_15: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type, rsqrt);  convert_element_type = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:7 in compiled_rms_layernorm, code: return self.weight * hidden_states.to(input_dtype)
	        convert_element_type_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_15, torch.float16);  mul_15 = None
	        mul_22: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_3, convert_element_type_1);  convert_element_type_1 = None
	        return (mul_22, primals_2, primals_3, rsqrt, primals_1)
	        
V0326 23:29:16.331000 324469 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "fa2316e03786dd3fc9511ea8bf517f91"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "Sym(s0)", primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", primals_3: "f16[2048][1]cuda:0", rsqrt: "f32[1, s0, 1][s0, 1, 1]cuda:0", tangents_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"):
	         # File: /tmp/ipykernel_324469/3685874839.py:7 in compiled_rms_layernorm, code: return self.weight * hidden_states.to(input_dtype)
	        mul_26: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, primals_3);  tangents_1 = primals_3 = None
	        convert_element_type_2: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_26, torch.float32);  mul_26 = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:4 in compiled_rms_layernorm, code: hidden_states = hidden_states.to(torch.float32)
	        convert_element_type: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.float32);  primals_2 = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:6 in compiled_rms_layernorm, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
	        mul_27: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2, convert_element_type)
	        mul_28: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2, rsqrt);  convert_element_type_2 = None
	        sum_1: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_27, [2], True, dtype = torch.float32);  mul_27 = None
	        pow_2: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt, 3);  rsqrt = None
	        mul_29: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_1, -0.5);  sum_1 = None
	        mul_30: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, pow_2);  mul_29 = pow_2 = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:5 in compiled_rms_layernorm, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
	        expand: "f32[1, s0, 2048][s0, 1, 0]cuda:0" = torch.ops.aten.expand.default(mul_30, [1, primals_1, 2048]);  mul_30 = primals_1 = None
	        div: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.div.Scalar(expand, 2048);  expand = None
	        pow_3: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 1.0);  convert_element_type = None
	        mul_31: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_3, 2.0);  pow_3 = None
	        mul_32: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div, mul_31);  div = mul_31 = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:5 in compiled_rms_layernorm, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
	        add_28: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_28, mul_32);  mul_28 = mul_32 = None
	        
	         # File: /tmp/ipykernel_324469/3685874839.py:4 in compiled_rms_layernorm, code: hidden_states = hidden_states.to(torch.float32)
	        convert_element_type_3: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_28, torch.float16);  add_28 = None
	        return (None, convert_element_type_3, None)
	        
V0326 23:29:16.332000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "57b4de6d818b786108a97eba7db13801"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992156331806.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.332000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "ea6ffdcf52afb60777291d3bccbf985a"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992156332428.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.332000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "60570c16eaf6028cc51716814e978593"}
	{
	"name": "inductor_compile",
	"ts": 1742992156332428.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.637000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/ol/colxjf5anh2mdbnjhp6dmderb4e35tzxhwlbgmfqa7pp47rrypuj.py"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "b98c3b54eb76476c8d4e0e65aacd5726"}
	# AOT ID: ['0_forward']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/cf/ccfxfxxhz45i7ck6up4hcseaddtu7awjgch7xkmk7ogkvm3vpyve.py
	# Topologically Sorted Source Nodes: [hidden_states, pow_1, variance, add, rsqrt, hidden_states_1, to_1, mul_1], Original ATen: [aten._to_copy, aten.pow, aten.mean, aten.add, aten.rsqrt, aten.mul]
	# Source node to ATen node mapping:
	#   add => add_9
	#   hidden_states => convert_element_type
	#   hidden_states_1 => mul_15
	#   mul_1 => mul_22
	#   pow_1 => pow_1
	#   rsqrt => rsqrt
	#   to_1 => convert_element_type_1
	#   variance => mean
	# Graph fragment:
	#   %convert_element_type : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_2, torch.float32), kwargs = {})
	#   %pow_1 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%convert_element_type, 2), kwargs = {})
	#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_1, [-1], True), kwargs = {})
	#   %add_9 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean, 1e-05), kwargs = {})
	#   %rsqrt : [num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_9,), kwargs = {})
	#   %mul_15 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type, %rsqrt), kwargs = {})
	#   %convert_element_type_1 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_15, torch.float16), kwargs = {})
	#   %mul_22 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_3, %convert_element_type_1), kwargs = {})
	triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.reduction(
	    size_hints=[128, 2048],
	    reduction_hint=ReductionHint.INNER,
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp32', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_0', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
	)
	@triton.jit
	def triton_(in_out_ptr0, in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
	    rnumel = 2048
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = xindex < xnumel
	    rbase = tl.arange(0, RBLOCK)[None, :]
	    x0 = xindex
	    _tmp4 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
	    for roffset in range(0, rnumel, RBLOCK):
	        rindex = roffset + rbase
	        rmask = rindex < rnumel
	        r1 = rindex
	        tmp0 = tl.load(in_ptr0 + (r1 + (2048*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
	        tmp1 = tmp0.to(tl.float32)
	        tmp2 = tmp1 * tmp1
	        tmp3 = tl.broadcast_to(tmp2, [XBLOCK, RBLOCK])
	        tmp5 = _tmp4 + tmp3
	        _tmp4 = tl.where(rmask & xmask, tmp5, _tmp4)
	    tmp4 = tl.sum(_tmp4, 1)[:, None]
	    tmp6 = 2048.0
	    tmp7 = tmp4 / tmp6
	    tmp8 = 1e-05
	    tmp9 = tmp7 + tmp8
	    tmp10 = libdevice.rsqrt(tmp9)
	    tl.debug_barrier()
	    tl.store(in_out_ptr0 + (x0), tmp10, xmask)
	    for roffset in range(0, rnumel, RBLOCK):
	        rindex = roffset + rbase
	        rmask = rindex < rnumel
	        r1 = rindex
	        tmp11 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
	        tmp12 = tl.load(in_ptr0 + (r1 + (2048*x0)), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
	        tmp13 = tmp12.to(tl.float32)
	        tmp14 = tmp13 * tmp10
	        tmp15 = tmp14.to(tl.float32)
	        tmp16 = tmp11 * tmp15
	        tl.store(out_ptr0 + (r1 + (2048*x0)), tmp16, rmask & xmask)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    primals_1, primals_2, primals_3 = args
	    args.clear()
	    s0 = primals_1
	    assert_size_stride(primals_2, (1, s0, 2048), (2048*s0, 2048, 1))
	    assert_size_stride(primals_3, (2048, ), (1, ))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((1, s0, 1), (s0, 1, s0), torch.float32)
	        buf1 = reinterpret_tensor(buf0, (1, s0, 1), (s0, 1, 1), 0); del buf0  # reuse
	        buf2 = empty_strided_cuda((1, s0, 2048), (2048*s0, 2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [hidden_states, pow_1, variance, add, rsqrt, hidden_states_1, to_1, mul_1], Original ATen: [aten._to_copy, aten.pow, aten.mean, aten.add, aten.rsqrt, aten.mul]
	        stream0 = get_raw_stream(0)
	        triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_0.run(buf1, primals_2, primals_3, buf2, s0, 2048, grid=grid(s0), stream=stream0)
	    return (buf2, primals_2, primals_3, buf1, s0, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    primals_1 = 100
	    primals_2 = rand_strided((1, 100, 2048), (204800, 2048, 1), device='cuda:0', dtype=torch.float16)
	    primals_3 = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
	    fn = lambda: call([primals_1, primals_2, primals_3])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:29:16.638000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "20eed3cceb595d64b7583269400e5c67"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742992156638001.0,
	"args": {
	"key": "fd3xqug2n3niasr2o2256x5c2rwhs6jkrqjoda67njvfakanm7qg",
	"components": [
	"[ljcxhldnyyhoosb2awv34zperzctwcujy2e2ntokpenfwbzyniy] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3):\n    convert_element_type = torch.ops.prims.convert_element_type.default(primals_2, torch.float32)\n    pow_1 = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 2)\n    mean = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None\n    add_9 = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None\n    rsqrt = torch.ops.aten.rsqrt.default(add_9);  add_9 = None\n    mul_15 = torch.ops.aten.mul.Tensor(convert_element_type, rsqrt);  convert_element_type = None\n    convert_element_type_1 = torch.ops.prims.convert_element_type.default(mul_15, torch.float16);  mul_15 = None\n    mul_22 = torch.ops.aten.mul.Tensor(primals_3, convert_element_type_1);  convert_element_type_1 = None\n    return (mul_22, primals_2, primals_3, rsqrt, primals_1)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[y2xmti2d5nfvoydvw5fwmyz3r65x7l5lz4kteznxv3qzpai5dku] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[4vufduoeo5swmf7jicwhszdzxvtgpacvh5sgv4h7fniqr6z35sv] fx_kwargs[static_input_idxs]: [2]",
	"[vj2utvrrmkqiulu3xnz7mk3wbv3zlgeqljk6t5ixxgrruq43k7b] fx_kwargs[user_visible_outputs]: {'mul_22': None}",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 528910043,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:29:16.638000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "0fd04df68080b5e4e96a1698930c3328"}
	{"key": "fd3xqug2n3niasr2o2256x5c2rwhs6jkrqjoda67njvfakanm7qg", "components": ["[ljcxhldnyyhoosb2awv34zperzctwcujy2e2ntokpenfwbzyniy] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3):\n    convert_element_type = torch.ops.prims.convert_element_type.default(primals_2, torch.float32)\n    pow_1 = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 2)\n    mean = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None\n    add_9 = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None\n    rsqrt = torch.ops.aten.rsqrt.default(add_9);  add_9 = None\n    mul_15 = torch.ops.aten.mul.Tensor(convert_element_type, rsqrt);  convert_element_type = None\n    convert_element_type_1 = torch.ops.prims.convert_element_type.default(mul_15, torch.float16);  mul_15 = None\n    mul_22 = torch.ops.aten.mul.Tensor(primals_3, convert_element_type_1);  convert_element_type_1 = None\n    return (mul_22, primals_2, primals_3, rsqrt, primals_1)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[y2xmti2d5nfvoydvw5fwmyz3r65x7l5lz4kteznxv3qzpai5dku] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[4vufduoeo5swmf7jicwhszdzxvtgpacvh5sgv4h7fniqr6z35sv] fx_kwargs[static_input_idxs]: [2]", "[vj2utvrrmkqiulu3xnz7mk3wbv3zlgeqljk6t5ixxgrruq43k7b] fx_kwargs[user_visible_outputs]: {'mul_22': None}", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 528910043, "cache_state": "hit"}
V0326 23:29:16.639000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "23d2b1be588faea478c2eaa0872d8f72"}
	{
	"name": "inductor_compile",
	"ts": 1742992156639907.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 1,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.640000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "25e31b9df83db8308b0b49ccd56d570e"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992156640248.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 1,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.640000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "551a2fa668b436f518d995a5274ad02d"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992156640769.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 1,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.641000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "d0f674b7a6f99a2fc9637a861eb7ea10"}
	{
	"name": "compile_fx.<locals>.bw_compiler",
	"ts": 1742992156641372.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.641000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "08e03c6efa8937d3fd032b5595427a76"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992156641785.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.642000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "f94e4dd6e51c9edc75d226179328a53c"}
	{
	"name": "inductor_compile",
	"ts": 1742992156641785.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.661000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/2j/c2j2yc6bujankaeuksegulsdlhmfbtuxl7di5ye75akihhdxc7g2.py"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "f21d95a51e415dc2d04dcbd06043e91d"}
	# AOT ID: ['0_backward']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/ig/cigvf5bf5wgns5w5s6rzi5dmp46jajb43gyhameg45qklmsz7xdm.py
	# Topologically Sorted Source Nodes: [hidden_states], Original ATen: [aten.mul, aten._to_copy, aten.sum, aten.div, aten.pow, aten.add]
	# Source node to ATen node mapping:
	#   hidden_states => convert_element_type
	# Graph fragment:
	#   %mul_26 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%tangents_1, %primals_3), kwargs = {})
	#   %convert_element_type_2 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_26, torch.float32), kwargs = {})
	#   %convert_element_type : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_2, torch.float32), kwargs = {})
	#   %mul_27 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_2, %convert_element_type), kwargs = {})
	#   %mul_28 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_2, %rsqrt), kwargs = {})
	#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_27, [2], True), kwargs = {dtype: torch.float32})
	#   %div : [num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand, 2048), kwargs = {})
	#   %pow_3 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%convert_element_type, 1.0), kwargs = {})
	#   %mul_31 : [num_users=1] = call_function[target=torch.ops.aten.mul.Scalar](args = (%pow_3, 2.0), kwargs = {})
	#   %mul_32 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div, %mul_31), kwargs = {})
	#   %add_28 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_28, %mul_32), kwargs = {})
	#   %convert_element_type_3 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_28, torch.float16), kwargs = {})
	triton_red_fused__to_copy_add_div_mul_pow_sum_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.reduction(
	    size_hints=[128, 2048],
	    reduction_hint=ReductionHint.INNER,
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp32', 4: '*fp16', 5: 'i32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused__to_copy_add_div_mul_pow_sum_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 1, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
	)
	@triton.jit
	def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
	    rnumel = 2048
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = xindex < xnumel
	    rbase = tl.arange(0, RBLOCK)[None, :]
	    x0 = xindex
	    _tmp8 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
	    for roffset in range(0, rnumel, RBLOCK):
	        rindex = roffset + rbase
	        rmask = rindex < rnumel
	        r1 = rindex
	        tmp0 = tl.load(in_ptr0 + (r1 + (2048*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
	        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
	        tmp4 = tl.load(in_ptr2 + (r1 + (2048*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
	        tmp2 = tmp0 * tmp1
	        tmp3 = tmp2.to(tl.float32)
	        tmp5 = tmp4.to(tl.float32)
	        tmp6 = tmp3 * tmp5
	        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])
	        tmp9 = _tmp8 + tmp7
	        _tmp8 = tl.where(rmask & xmask, tmp9, _tmp8)
	    tmp8 = tl.sum(_tmp8, 1)[:, None]
	    tmp14 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
	    for roffset in range(0, rnumel, RBLOCK):
	        rindex = roffset + rbase
	        rmask = rindex < rnumel
	        r1 = rindex
	        tmp10 = tl.load(in_ptr0 + (r1 + (2048*x0)), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
	        tmp11 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
	        tmp23 = tl.load(in_ptr2 + (r1 + (2048*x0)), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
	        tmp12 = tmp10 * tmp11
	        tmp13 = tmp12.to(tl.float32)
	        tmp15 = tmp13 * tmp14
	        tmp16 = -0.5
	        tmp17 = tmp8 * tmp16
	        tmp18 = tmp14 * tmp14
	        tmp19 = tmp18 * tmp14
	        tmp20 = tmp17 * tmp19
	        tmp21 = 0.00048828125
	        tmp22 = tmp20 * tmp21
	        tmp24 = tmp23.to(tl.float32)
	        tmp25 = 2.0
	        tmp26 = tmp24 * tmp25
	        tmp27 = tmp22 * tmp26
	        tmp28 = tmp15 + tmp27
	        tmp29 = tmp28.to(tl.float32)
	        tl.store(out_ptr1 + (r1 + (2048*x0)), tmp29, rmask & xmask)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    primals_1, primals_2, primals_3, rsqrt, tangents_1 = args
	    args.clear()
	    s0 = primals_1
	    assert_size_stride(primals_2, (1, s0, 2048), (2048*s0, 2048, 1))
	    assert_size_stride(primals_3, (2048, ), (1, ))
	    assert_size_stride(rsqrt, (1, s0, 1), (s0, 1, 1))
	    assert_size_stride(tangents_1, (1, s0, 2048), (2048*s0, 2048, 1))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf1 = empty_strided_cuda((1, s0, 2048), (2048*s0, 2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [hidden_states], Original ATen: [aten.mul, aten._to_copy, aten.sum, aten.div, aten.pow, aten.add]
	        stream0 = get_raw_stream(0)
	        triton_red_fused__to_copy_add_div_mul_pow_sum_0.run(tangents_1, primals_3, primals_2, rsqrt, buf1, s0, 2048, grid=grid(s0), stream=stream0)
	        del primals_2
	        del primals_3
	        del rsqrt
	        del tangents_1
	    return (None, buf1, None, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    primals_1 = 100
	    primals_2 = rand_strided((1, 100, 2048), (204800, 2048, 1), device='cuda:0', dtype=torch.float16)
	    primals_3 = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
	    rsqrt = rand_strided((1, 100, 1), (100, 1, 1), device='cuda:0', dtype=torch.float32)
	    tangents_1 = rand_strided((1, 100, 2048), (204800, 2048, 1), device='cuda:0', dtype=torch.float16)
	    fn = lambda: call([primals_1, primals_2, primals_3, rsqrt, tangents_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:29:16.661000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "fb29839ca18f6a9d6d34909f57f45873"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742992156661515.8,
	"args": {
	"key": "f3hbkqnj3awzuwzxb6sotpsj3vylnuhmlmz4f5ydv2ptm47zw3qn",
	"components": [
	"[4plki53queiockxg4eroemrjav5skevi7ca4tonjgzft3exor6s] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3, rsqrt, tangents_1):\n    mul_26 = torch.ops.aten.mul.Tensor(tangents_1, primals_3);  tangents_1 = primals_3 = None\n    convert_element_type_2 = torch.ops.prims.convert_element_type.default(mul_26, torch.float32);  mul_26 = None\n    convert_element_type = torch.ops.prims.convert_element_type.default(primals_2, torch.float32);  primals_2 = None\n    mul_27 = torch.ops.aten.mul.Tensor(convert_element_type_2, convert_element_type)\n    mul_28 = torch.ops.aten.mul.Tensor(convert_element_type_2, rsqrt);  convert_element_type_2 = None\n    sum_1 = torch.ops.aten.sum.dim_IntList(mul_27, [2], True, dtype = torch.float32);  mul_27 = None\n    pow_2 = torch.ops.aten.pow.Tensor_Scalar(rsqrt, 3);  rsqrt = None\n    mul_29 = torch.ops.aten.mul.Scalar(sum_1, -0.5);  sum_1 = None\n    mul_30 = torch.ops.aten.mul.Tensor(mul_29, pow_2);  mul_29 = pow_2 = None\n    expand = torch.ops.aten.expand.default(mul_30, [1, primals_1, 2048]);  mul_30 = primals_1 = None\n    div = torch.ops.aten.div.Scalar(expand, 2048);  expand = None\n    pow_3 = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 1.0);  convert_element_type = None\n    mul_31 = torch.ops.aten.mul.Scalar(pow_3, 2.0);  pow_3 = None\n    mul_32 = torch.ops.aten.mul.Tensor(div, mul_31);  div = mul_31 = None\n    add_28 = torch.ops.aten.add.Tensor(mul_28, mul_32);  mul_28 = mul_32 = None\n    convert_element_type_3 = torch.ops.prims.convert_element_type.default(add_28, torch.float16);  add_28 = None\n    return (None, convert_element_type_3, None)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[xrcn5kean45trgkul3lxpg7fpefuuthsckieyj3ktmatzm2m27j] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4096*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[y2xmti2d5nfvoydvw5fwmyz3r65x7l5lz4kteznxv3qzpai5dku] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[fbavznyoas5zlq2ukff4higuprx6o7ghcq5att5foipfmcaeil7] example_inputs[3]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1, s0, 1]), stride=(s0, 1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_backward]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[rsuyy3mwcmazob2rjj4zloxesnv43jcmbn6pbssjvcym5t6zngb] fx_kwargs[static_input_idxs]: [0, 1, 2, 3]",
	"[rprlxq2vwpy5onx3seju2nad6bzjzrtekxvtmzeyhkcrcv6gu46] fx_kwargs[user_visible_outputs]: {'convert_element_type_3': None}",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[0]: 4",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 497150911,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:29:16.661000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "bb2bf452705b72f04980f12ca4d74d69"}
	{"key": "f3hbkqnj3awzuwzxb6sotpsj3vylnuhmlmz4f5ydv2ptm47zw3qn", "components": ["[4plki53queiockxg4eroemrjav5skevi7ca4tonjgzft3exor6s] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3, rsqrt, tangents_1):\n    mul_26 = torch.ops.aten.mul.Tensor(tangents_1, primals_3);  tangents_1 = primals_3 = None\n    convert_element_type_2 = torch.ops.prims.convert_element_type.default(mul_26, torch.float32);  mul_26 = None\n    convert_element_type = torch.ops.prims.convert_element_type.default(primals_2, torch.float32);  primals_2 = None\n    mul_27 = torch.ops.aten.mul.Tensor(convert_element_type_2, convert_element_type)\n    mul_28 = torch.ops.aten.mul.Tensor(convert_element_type_2, rsqrt);  convert_element_type_2 = None\n    sum_1 = torch.ops.aten.sum.dim_IntList(mul_27, [2], True, dtype = torch.float32);  mul_27 = None\n    pow_2 = torch.ops.aten.pow.Tensor_Scalar(rsqrt, 3);  rsqrt = None\n    mul_29 = torch.ops.aten.mul.Scalar(sum_1, -0.5);  sum_1 = None\n    mul_30 = torch.ops.aten.mul.Tensor(mul_29, pow_2);  mul_29 = pow_2 = None\n    expand = torch.ops.aten.expand.default(mul_30, [1, primals_1, 2048]);  mul_30 = primals_1 = None\n    div = torch.ops.aten.div.Scalar(expand, 2048);  expand = None\n    pow_3 = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 1.0);  convert_element_type = None\n    mul_31 = torch.ops.aten.mul.Scalar(pow_3, 2.0);  pow_3 = None\n    mul_32 = torch.ops.aten.mul.Tensor(div, mul_31);  div = mul_31 = None\n    add_28 = torch.ops.aten.add.Tensor(mul_28, mul_32);  mul_28 = mul_32 = None\n    convert_element_type_3 = torch.ops.prims.convert_element_type.default(add_28, torch.float16);  add_28 = None\n    return (None, convert_element_type_3, None)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[xrcn5kean45trgkul3lxpg7fpefuuthsckieyj3ktmatzm2m27j] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4096*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[y2xmti2d5nfvoydvw5fwmyz3r65x7l5lz4kteznxv3qzpai5dku] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[fbavznyoas5zlq2ukff4higuprx6o7ghcq5att5foipfmcaeil7] example_inputs[3]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1, s0, 1]), stride=(s0, 1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_backward]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[rsuyy3mwcmazob2rjj4zloxesnv43jcmbn6pbssjvcym5t6zngb] fx_kwargs[static_input_idxs]: [0, 1, 2, 3]", "[rprlxq2vwpy5onx3seju2nad6bzjzrtekxvtmzeyhkcrcv6gu46] fx_kwargs[user_visible_outputs]: {'convert_element_type_3': None}", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[0]: 4", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 497150911, "cache_state": "hit"}
V0326 23:29:16.662000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "dbed1440c8d2318206ef6dbd11eac4e1"}
	{
	"name": "inductor_compile",
	"ts": 1742992156662756.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.663000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "05a05afb4b43e972453345763600dc4f"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992156663052.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.663000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"bwd_compilation_metrics": {"compile_id": "0/0", "inductor_compile_time_s": 0.02092719078063965, "code_gen_time_s": null, "fail_type": null, "fail_reason": null}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.663000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "9ace4835d9504071b359676eeffac18e"}
	{
	"name": "compile_fx.<locals>.bw_compiler",
	"ts": 1742992156663665.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.665000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "a666d52aca4f6425b25a9c1b4deb78f9"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992156665336.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.665000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "fd4b6cda2ad87519783bbfd71b0456d9"}
	{
	"name": "backend_compile",
	"ts": 1742992156665731.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.666000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "d19bc182e49bc3491cbbe8d83ee45592"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992156665980.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.670000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "945972a5987855066cedeb441686e93b"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)
	| | +- TYPE_MATCH: ___check_type_id(L['self'], 105190333747792)                
	| | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | +- GuardManager: source=L['self']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | +- DICT_LENGTH: len(L['self']._parameters) == 1                             
	| | | | +- GuardManager: source=L['self']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | +- TENSOR_MATCH: check_tensor(L['self']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[2048], stride=[1])
	| | | | | +- NO_TENSOR_ALIASING: check_no_aliasing(L['hidden_states'], L['self']._parameters['weight'])
	| | | +- GuardManager: source=L['self'].variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)
	| | | | +- EQUALS_MATCH: L['self'].variance_epsilon == 1e-05                         
	| +- GuardManager: source=L['hidden_states'], accessed_by=DictGetItemGuardAccessor(hidden_states)
	| | +- TENSOR_MATCH: check_tensor(L['hidden_states'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=True, size=[1, None, 2048], stride=[None, 2048, 1])
	| | +- NO_HASATTR: hasattr(L['hidden_states'], '_dynamo_dynamic_indices') == False
	| | +- NO_TENSOR_ALIASING
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)
	| | | +- ID_MATCH: ___check_obj_id(G['torch'], 127872889574384)                
	| | | +- GuardManager: source=G['torch'].rsqrt, accessed_by=GetAttrGuardAccessor(rsqrt)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].rsqrt, 127872890503872)          
	| | | +- GuardManager: source=G['torch'].float32, accessed_by=GetAttrGuardAccessor(float32)
	| | | | +- EQUALS_MATCH: G['torch'].float32 == torch.float32                         
	+- LAMBDA_GUARD: L['hidden_states'].stride()[0] == 2048*L['hidden_states'].size()[1]  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['hidden_states'].size()[1] <= 1048575                  # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0326 23:29:16.670000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "5d9a619be520d69688753d9c66bde713"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992156670591.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.670000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "e2669290171d36e3f6b4fb867607ea4a"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992156670831.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.671000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "0/0", "frame_key": "1", "co_name": "compiled_rms_layernorm", "co_filename": "/tmp/ipykernel_324469/3685874839.py", "co_firstlineno": 1, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": 14, "shape_env_guard_count": 2, "graph_op_count": 8, "graph_node_count": 12, "graph_input_count": 3, "start_time": 1742992155.6899745, "entire_frame_compile_time_s": 0.9805521965026855, "backend_compile_time_s": 0.911175012588501, "inductor_compile_time_s": 0.30730724334716797, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.672000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py", 39]}
V0326 23:29:16.672000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/tmp/ipykernel_324469/2489781049.py", 40]}
V0326 23:29:16.672000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 336, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 270, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 496, "name": "forward", "filename": 39}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 102, "name": "inner_transpose_forward", "filename": 40}, {"line": 69, "name": "get_data_transposed", "filename": 40}]}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.673000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "11ea023c7b61582274f12218afe73da5"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992156673106.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.673000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "de9e4c3638d0d55fc94015d926da2a6e"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992156673106.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.675000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 4, "size": 2097152}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.676000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [2097152, 1], "is_leaf": true, "stride": [1, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6fa64d20>", "describer_id": 4}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.676000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 4, "id": 0, "source": "L['param']._some_data"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.679000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_param_some_data": [2097152, 1], "t": [1, 2097152]}}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "64427a5a76bb541fe382003dbb03f0cb"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_param_some_data: "u8[2097152, 1][1, 1]cuda:0"):
	        l_param_some_data = L_param_some_data
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        t: "u8[1, 2097152][1, 1]cuda:0" = l_param_some_data.t();  l_param_some_data = None
	        return (t,)
	        
V0326 23:29:16.680000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "1dfdeed6105440bad45660b78bb3cab3"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992156680168.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.680000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "508b8aec5d6644f6104ffda2f9c3f3e2"}
	{
	"name": "backend_compile",
	"ts": 1742992156680168.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.682000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "e2dd0792b0846a2cc9d710b178bd9e8c"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992156682203.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.687000 324469 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:215] {"aot_forward_graph": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "37fa7791ad010b42ad78cb72f5710ba2"}
	class <lambda>(torch.nn.Module):
	    def forward(self, arg0_1: "u8[2097152, 1][1, 1]cuda:0"):
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute: "u8[1, 2097152][1, 1]cuda:0" = torch.ops.aten.permute.default(arg0_1, [1, 0]);  arg0_1 = None
	        return (permute,)
	        
V0326 23:29:16.687000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "369631cc01c7ae9fb80782fcdc2ca198"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992156687780.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.688000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "0e6ca94fc192b2dc1416c019634019c1"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992156688400.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.688000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "d64b7b56f7f0b4eb4728e30f7ee78474"}
	{
	"name": "inductor_compile",
	"ts": 1742992156688400.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.691000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/t5/ct5cdrlvhzqgati3qobbqjmjthnaa573nkk6dwx7azkdhct72btj.py"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "6f1ae6738be728abd508dcc7a1da35cd"}
	# AOT ID: ['1_inference']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    arg0_1, = args
	    args.clear()
	    assert_size_stride(arg0_1, (2097152, 1), (1, 1))
	    return (reinterpret_tensor(arg0_1, (1, 2097152), (1, 1), 0), )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    arg0_1 = rand_strided((2097152, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    fn = lambda: call([arg0_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:29:16.692000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "bba0cdc1a7ec26c1ca290f30302cdb83"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742992156692218.2,
	"args": {
	"key": "frdyc5x44cxbjq26p2wj6co5c3qi53vxxjy7dm4vmjuzn6pgktgo",
	"components": [
	"[omke3y723kztjz7vngax6cqwepxyk5c3ln6ujnyovguytfqrfsu] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1):\n    permute = torch.ops.aten.permute.default(arg0_1, [1, 0]);  arg0_1 = None\n    return (permute,)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[ezlhk63hcc4wxnhvfwvow767xoda5sijwwnfqpayfzwwvfslyks] example_inputs[0]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([2097152, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []",
	"[5ckznxnwokx4wcssswt7y7bj7a6njtitxnutjq3p4vcfi2uszig] fx_kwargs[user_visible_outputs]: {'permute': None}",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 6391657,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:29:16.692000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "5adacf143046cfe094a0c14ce4377cd8"}
	{"key": "frdyc5x44cxbjq26p2wj6co5c3qi53vxxjy7dm4vmjuzn6pgktgo", "components": ["[omke3y723kztjz7vngax6cqwepxyk5c3ln6ujnyovguytfqrfsu] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1):\n    permute = torch.ops.aten.permute.default(arg0_1, [1, 0]);  arg0_1 = None\n    return (permute,)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[ezlhk63hcc4wxnhvfwvow767xoda5sijwwnfqpayfzwwvfslyks] example_inputs[0]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([2097152, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []", "[5ckznxnwokx4wcssswt7y7bj7a6njtitxnutjq3p4vcfi2uszig] fx_kwargs[user_visible_outputs]: {'permute': None}", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 6391657, "cache_state": "hit"}
V0326 23:29:16.693000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "3f03acf92b4458a3dba4b6a01958e7d8"}
	{
	"name": "inductor_compile",
	"ts": 1742992156693122.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.693000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "28c9c1a8642d38ab88722d44645d0074"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992156693362.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.693000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "60794eea7e719c3ce977ff9bc352e3d8"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992156693693.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.694000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "3acb628243c1b3d58006148ffc5d63b2"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992156694926.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.695000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "eacdc9bb33a5b54732f593cbe4c97df8"}
	{
	"name": "backend_compile",
	"ts": 1742992156695262.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.695000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "15bed2ba93377b6d883f24da383ab1a5"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992156695574.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.697000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "4df13709e1eec24e46a8f62c7d81ecaf"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['param'], accessed_by=DictGetItemGuardAccessor(param)
	| | +- TYPE_MATCH: ___check_type_id(L['param'], 105190296990752)               
	| | +- GuardManager: source=L['param']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | +- TENSOR_MATCH: check_tensor(L['param']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[2097152, 1], stride=[1, 1])
	| | | +- NO_HASATTR: hasattr(L['param']._some_data, '_dynamo_dynamic_indices') == False
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['Params4bit'], accessed_by=DictGetItemGuardAccessor(Params4bit)
	| | | +- ID_MATCH: ___check_obj_id(G['Params4bit'], 105190296990752)           
	| | +- GuardManager: source=G['__builtins_dict___2'], accessed_by=DictGetItemGuardAccessor(__builtins_dict___2)
	| | | +- GuardManager: source=G['__builtins_dict___2']['isinstance'], accessed_by=DictGetItemGuardAccessor(isinstance)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___2']['isinstance'], 127873218841152)
	
V0326 23:29:16.697000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "e4ee4d842ea758303dd9e4e7197b33d6"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992156697547.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.697000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "7bd43bd619d2cfb69c874b065d3743cb"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992156697785.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.698000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "1/0", "frame_key": "2", "co_name": "get_data_transposed", "co_filename": "/tmp/ipykernel_324469/2489781049.py", "co_firstlineno": 69, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": 9, "shape_env_guard_count": 0, "graph_op_count": 1, "graph_node_count": 3, "graph_input_count": 1, "start_time": 1742992156.6731024, "entire_frame_compile_time_s": 0.024410486221313477, "backend_compile_time_s": 0.01506805419921875, "inductor_compile_time_s": 0.004683017730712891, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.698000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/autograd/function.py", 41]}
V0326 23:29:16.699000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/tmp/ipykernel_324469/636066363.py", 42]}
V0326 23:29:16.699000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/submit/Problem_C/kernels.py", 43]}
V0326 23:29:16.699000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 336, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 270, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 496, "name": "forward", "filename": 39}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 110, "name": "inner_transpose_forward", "filename": 40}, {"line": 575, "name": "apply", "filename": 41}, {"line": 34, "name": "forward", "filename": 40}, {"line": 21, "name": "augmented_dequantize_4bit", "filename": 42}, {"line": 330, "name": "fused_dequantize", "filename": 43}]}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.699000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "0792f8598ab2a60d5633d56942bb36f4"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992156699803.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.700000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "84ebcbca8c3e112325efffa749ee158f"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992156699803.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.702000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 6, "size": 2097152}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.702000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [2097152, 1], "is_leaf": true, "stride": [1, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6fa64d20>", "describer_id": 6}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.703000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [1, 2097152], "is_leaf": true, "is_view": true, "stride": [1, 1], "storage": 0, "base": 1, "creation_meta": "CreationMeta.DEFAULT", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744afe569590>", "describer_id": 6}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.703000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 6, "id": 0, "source": "L['A']"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.705000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 6, "size": 65536}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.705000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [65536], "is_leaf": true, "stride": [1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c259680>", "describer_id": 6}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.705000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 6, "id": 3, "source": "L['quant_state'].absmax"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.706000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 6, "size": 1024}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.706000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c259630>", "describer_id": 6}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.706000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 6, "id": 4, "source": "L['quant_state'].state2.code"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.707000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 6, "size": 1024}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.707000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c259450>", "describer_id": 6}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.707000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 6, "id": 5, "source": "L['quant_state'].state2.absmax"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.708000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 6, "size": 2}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.708000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c2581e0>", "describer_id": 6}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.708000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 6, "id": 6, "source": "L['quant_state'].offset"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.709000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 6, "size": 64}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.709000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c2596d0>", "describer_id": 6}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.709000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 6, "id": 7, "source": "L['quant_state'].code"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.716000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_a_": [1, 2097152], "l_quant_state_absmax": [65536], "l_quant_state_state2_code": [256], "l_quant_state_state2_absmax": [256], "l_quant_state_offset": [], "l_quant_state_code": [16], "output_ptr": [2048, 2048]}}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "d0a69d02c4165ad106b39184953050e9"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_A_: "u8[1, 2097152][1, 1]cuda:0", L_quant_state_absmax: "u8[65536][1]cuda:0", L_quant_state_state2_code: "f32[256][1]cuda:0", L_quant_state_state2_absmax: "f32[256][1]cuda:0", L_quant_state_offset: "f16[][]cuda:0", L_quant_state_code: "f32[16][1]cuda:0"):
	        l_a_ = L_A_
	        l_quant_state_absmax = L_quant_state_absmax
	        l_quant_state_state2_code = L_quant_state_state2_code
	        l_quant_state_state2_absmax = L_quant_state_state2_absmax
	        l_quant_state_offset = L_quant_state_offset
	        l_quant_state_code = L_quant_state_code
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	        output_ptr: "bf16[2048, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_quant_state_absmax, l_quant_state_state2_code, l_quant_state_state2_absmax, l_quant_state_offset, l_a_, l_quant_state_code, 4194304, 256, 32, 16384, 8192, 2048, 2048, torch.bfloat16);  l_quant_state_absmax = l_quant_state_state2_code = l_quant_state_state2_absmax = l_quant_state_offset = l_a_ = l_quant_state_code = None
	        return (output_ptr,)
	        
V0326 23:29:16.717000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "4e7947b80a4355eba9fd89f4295f17f9"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992156717469.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.717000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "23e82e8289c70513d94044b1fa792695"}
	{
	"name": "backend_compile",
	"ts": 1742992156717469.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.719000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "269daaeaf9756707a4e8483a2d322063"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992156719574.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.727000 324469 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:215] {"aot_forward_graph": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "ad54c546273a9367de3e60c0105d5280"}
	class <lambda>(torch.nn.Module):
	    def forward(self, arg0_1: "u8[1, 2097152][1, 1]cuda:0", arg1_1: "u8[65536][1]cuda:0", arg2_1: "f32[256][1]cuda:0", arg3_1: "f32[256][1]cuda:0", arg4_1: "f16[][]cuda:0", arg5_1: "f32[16][1]cuda:0"):
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	        fused_dequantize_op: "bf16[2048, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(arg1_1, arg2_1, arg3_1, arg4_1, arg0_1, arg5_1, 4194304, 256, 32, 16384, 8192, 2048, 2048, torch.bfloat16);  arg1_1 = arg2_1 = arg3_1 = arg4_1 = arg0_1 = arg5_1 = None
	        return (fused_dequantize_op,)
	        
V0326 23:29:16.728000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "14891125236b70febd198370aeb356d0"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992156728119.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.728000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "a390cd50deda6f4439e110bfcb41778d"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992156728780.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.729000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "1ea295f461fa2385fe1a31015c1351cc"}
	{
	"name": "inductor_compile",
	"ts": 1742992156728780.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.736000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/tu/ctunny5zrn6m4zbxe55td4fbnmrbnmej3onm4j537eilj3omfiad.py"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "b638b9a588e9cbc9c705eefdb1592930"}
	# AOT ID: ['2_inference']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/xa/cxaqzqrucz62vf6o3o6sl2ihhxptbp2wjf7gaoczyt27xbq6aopg.py
	# Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	# Source node to ATen node mapping:
	#   output_ptr => fused_dequantize_op
	# Graph fragment:
	#   %fused_dequantize_op : [num_users=1] = call_function[target=torch.ops.mylib.fused_dequantize_op.default](args = (%arg1_1, %arg2_1, %arg3_1, %arg4_1, %arg0_1, %arg5_1, 4194304, 256, 32, 16384, 8192, 2048, 2048, torch.bfloat16), kwargs = {})
	triton_poi_fused_fused_dequantize_op_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {2: 1}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=(2,))]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_fused_dequantize_op_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 1
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    tmp0 = tl.load(in_ptr0 + (0)).to(tl.float32)
	    tmp1 = tl.broadcast_to(tmp0, [XBLOCK])
	    tl.store(out_ptr0 + (tl.full([XBLOCK], 0, tl.int32)), tmp1, None)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1 = args
	    args.clear()
	    assert_size_stride(arg0_1, (1, 2097152), (1, 1))
	    assert_size_stride(arg1_1, (65536, ), (1, ))
	    assert_size_stride(arg2_1, (256, ), (1, ))
	    assert_size_stride(arg3_1, (256, ), (1, ))
	    assert_size_stride(arg4_1, (), ())
	    assert_size_stride(arg5_1, (16, ), (1, ))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((), (), torch.float16)
	        # Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_fused_dequantize_op_0.run(arg4_1, buf0, 1, grid=grid(1), stream=stream0)
	        del arg4_1
	        # Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	        buf1 = torch.ops.mylib.fused_dequantize_op.default(arg1_1, arg2_1, arg3_1, buf0, arg0_1, arg5_1, 4194304, 256, 32, 16384, 8192, 2048, 2048, torch.bfloat16)
	        del arg0_1
	        del arg1_1
	        del arg2_1
	        del arg3_1
	        del arg5_1
	        del buf0
	        buf2 = buf1
	        del buf1
	    return (buf2, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    arg0_1 = rand_strided((1, 2097152), (1, 1), device='cuda:0', dtype=torch.uint8)
	    arg1_1 = rand_strided((65536, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    arg2_1 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg3_1 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg4_1 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    arg5_1 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:29:16.737000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "1af7e98946839f3bb778734c38b4808a"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742992156737192.8,
	"args": {
	"key": "fdh2uwta5azcf55ly2rwah3ufrucn3zo5edy7spszb463gmznw6v",
	"components": [
	"[2bp3t6p5cxblmeq7qhno6dbosphmpkxzycvz54fqxuqeaocqtnp] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1):\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(arg1_1, arg2_1, arg3_1, arg4_1, arg0_1, arg5_1, 4194304, 256, 32, 16384, 8192, 2048, 2048, torch.bfloat16);  arg1_1 = arg2_1 = arg3_1 = arg4_1 = arg0_1 = arg5_1 = None\n    return (fused_dequantize_op,)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[4gvebibmcz6gre4gya45qksbmfls7626p25ek6tw3nszlewizy7] example_inputs[0]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([1, 2097152]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[53cooickqlnvm7pjzvcat3ztcgrqimbrib43vb3txz7otk7tjkk] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([65536]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[2]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[3]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[j23zbqrguggah6nscnzj4y55d5chhoardpgvlji4ilrfwrr6asz] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[2ylzn3y4vcv2t7rbq447jvkl6by5qf32ycwks3h2nbcmm6wvmnk] example_inputs[5]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []",
	"[jan4rfottd4caig5fc5h3ieb5ephfyhn7rn27jmu2ehbwgssm5j] fx_kwargs[user_visible_outputs]: {'fused_dequantize_op': None}",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[1]: 1",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inputs_to_check[2]: 2",
	"[kcuxe2zwm3mzv2uk6adm6iskoy35bqfv725twacrdewod2dbl5d] inputs_to_check[3]: 3",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[4]: 4",
	"[qs5hilycp4ew4ivtc7m5jaxp7q4pm5slioxw3fi3ur6ei65ybz4] inputs_to_check[5]: 5",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 75306289,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:29:16.737000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "b22c51cf728b0ed337ff337d8b7d1154"}
	{"key": "fdh2uwta5azcf55ly2rwah3ufrucn3zo5edy7spszb463gmznw6v", "components": ["[2bp3t6p5cxblmeq7qhno6dbosphmpkxzycvz54fqxuqeaocqtnp] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1):\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(arg1_1, arg2_1, arg3_1, arg4_1, arg0_1, arg5_1, 4194304, 256, 32, 16384, 8192, 2048, 2048, torch.bfloat16);  arg1_1 = arg2_1 = arg3_1 = arg4_1 = arg0_1 = arg5_1 = None\n    return (fused_dequantize_op,)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[4gvebibmcz6gre4gya45qksbmfls7626p25ek6tw3nszlewizy7] example_inputs[0]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([1, 2097152]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[53cooickqlnvm7pjzvcat3ztcgrqimbrib43vb3txz7otk7tjkk] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([65536]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[2]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[3]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[j23zbqrguggah6nscnzj4y55d5chhoardpgvlji4ilrfwrr6asz] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[2ylzn3y4vcv2t7rbq447jvkl6by5qf32ycwks3h2nbcmm6wvmnk] example_inputs[5]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []", "[jan4rfottd4caig5fc5h3ieb5ephfyhn7rn27jmu2ehbwgssm5j] fx_kwargs[user_visible_outputs]: {'fused_dequantize_op': None}", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[1]: 1", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inputs_to_check[2]: 2", "[kcuxe2zwm3mzv2uk6adm6iskoy35bqfv725twacrdewod2dbl5d] inputs_to_check[3]: 3", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[4]: 4", "[qs5hilycp4ew4ivtc7m5jaxp7q4pm5slioxw3fi3ur6ei65ybz4] inputs_to_check[5]: 5", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 75306289, "cache_state": "hit"}
V0326 23:29:16.738000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "6976619a468064dd512fc893d3d2e689"}
	{
	"name": "inductor_compile",
	"ts": 1742992156737989.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.738000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "0e967b3506db9b4bfcfff52108d23ceb"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992156738224.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.738000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "1fa2ebc006c428bf8f3de78cea3ef841"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992156738553.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.739000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "77c64bc23c9f57f2d010e1804ace1cc4"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992156739745.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.740000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "25897a38135017bc4a8f6b06f47e03eb"}
	{
	"name": "backend_compile",
	"ts": 1742992156740126.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.740000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "70566d809f9395ca6f201feb10fde8e9"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992156740386.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.744000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "396a9be52815c97e2328550ce295101d"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['A'], accessed_by=DictGetItemGuardAccessor(A)
	| | +- TENSOR_MATCH: check_tensor(L['A'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[1, 2097152], stride=[1, 1])
	| | +- NO_HASATTR: hasattr(L['A'], '_dynamo_dynamic_indices') == False         
	| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['A'], L['quant_state'].code, L['quant_state'].absmax, L['quant_state'].offset, L['quant_state'].state2.code, L['quant_state'].state2.absmax)
	| +- GuardManager: source=L['quant_state'], accessed_by=DictGetItemGuardAccessor(quant_state)
	| | +- TYPE_MATCH: ___check_type_id(L['quant_state'], 105190295709968)         
	| | +- GuardManager: source=L['quant_state'].code, accessed_by=GetAttrGuardAccessor(code)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].code, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | +- EQUALS_MATCH: L['quant_state'].dtype == torch.bfloat16                    
	| | +- GuardManager: source=L['quant_state'].shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].shape, 127872448968448)   
	| | | +- LENGTH_CHECK: len(L['quant_state'].shape) == 2                            
	| | | +- GuardManager: source=L['quant_state'].shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | +- EQUALS_MATCH: L['quant_state'].shape[0] == 2048                           
	| | | +- GuardManager: source=L['quant_state'].shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | +- EQUALS_MATCH: L['quant_state'].shape[1] == 2048                           
	| | +- GuardManager: source=L['quant_state'].absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[65536], stride=[1])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].absmax, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].offset, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].state2, 105190295709968)  
	| | | +- GuardManager: source=L['quant_state'].state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | +- TENSOR_MATCH: check_tensor(L['quant_state'].state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | +- NO_HASATTR: hasattr(L['quant_state'].state2.code, '_dynamo_dynamic_indices') == False
	| | | | +- NO_TENSOR_ALIASING
	| | | +- GuardManager: source=L['quant_state'].state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | +- TENSOR_MATCH: check_tensor(L['quant_state'].state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | +- NO_HASATTR: hasattr(L['quant_state'].state2.absmax, '_dynamo_dynamic_indices') == False
	| | | | +- NO_TENSOR_ALIASING
	| | | +- GuardManager: source=L['quant_state'].state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | +- EQUALS_MATCH: L['quant_state'].state2.blocksize == 256                    
	| | +- GuardManager: source=L['quant_state'].blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | +- EQUALS_MATCH: L['quant_state'].blocksize == 64                            
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)
	| | | +- ID_MATCH: ___check_obj_id(G['torch'], 127872889574384)                
	| | | +- GuardManager: source=G['torch'].numel, accessed_by=GetAttrGuardAccessor(numel)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].numel, 127872890415168)          
	| | +- GuardManager: source=G['DEBUG_FLAG'], accessed_by=DictGetItemGuardAccessor(DEBUG_FLAG)
	| | | +- ID_MATCH: ___check_obj_id(G['DEBUG_FLAG'], 127873236421344)           
	| | +- GuardManager: source=G['fused_dequantize_op'], accessed_by=DictGetItemGuardAccessor(fused_dequantize_op)
	| | | +- TYPE_MATCH: ___check_type_id(G['fused_dequantize_op'], 105190280786160) 
	| | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize_op'], 127867502131200)  
	| | | +- GuardManager: source=G['fused_dequantize_op']._opoverload, accessed_by=GetAttrGuardAccessor(_opoverload)
	| | | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize_op']._opoverload, 127867502233600)
	
V0326 23:29:16.744000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "2a84c89af9609cd267f2e03d4d7c6586"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992156744285.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.744000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "3b21bf19d04c7bca503199782056f9b6"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992156744526.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.744000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "2/0", "frame_key": "3", "co_name": "fused_dequantize", "co_filename": "/home/tom/unsloth-challenges/submit/Problem_C/kernels.py", "co_firstlineno": 330, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": 25, "shape_env_guard_count": 0, "graph_op_count": 1, "graph_node_count": 8, "graph_input_count": 6, "start_time": 1742992156.6997995, "entire_frame_compile_time_s": 0.04445362091064453, "backend_compile_time_s": 0.022631168365478516, "inductor_compile_time_s": 0.009172201156616211, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": ["mylib::fused_dequantize_op"], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.814000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2817] {"artifact": {"name": "recompile_reasons", "encoding": "json"}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "30ea098d0804bda888895ce1fd541df6"}
	[
	"1/0: tensor 'L['param']._some_data' size mismatch at index 0. expected 2097152, actual 524288"
	]
V0326 23:29:16.815000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 336, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 271, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 496, "name": "forward", "filename": 39}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 102, "name": "inner_transpose_forward", "filename": 40}, {"line": 69, "name": "get_data_transposed", "filename": 40}]}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.815000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "af4a4cf274f8ce5179210a2a48977799"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992156815775.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.816000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "8bd3b27bc7f0fcc2ead861e0a40d0add"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992156815775.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.818000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 8, "size": 524288}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.818000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [524288, 1], "is_leaf": true, "stride": [1, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6fa64eb0>", "describer_id": 8}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.818000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 8, "id": 0, "source": "L['param']._some_data"}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.822000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_param_some_data": ["s0", 1], "t": [1, "s0"]}}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "7abc5c1eb659429356fc0025df103c6d"}
	class GraphModule(torch.nn.Module):
	    def forward(self, s0: "Sym(s0)", L_param_some_data: "u8[s0, 1][1, 1]cuda:0"):
	        l_param_some_data = L_param_some_data
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        t: "u8[1, s0][1, 1]cuda:0" = l_param_some_data.t();  l_param_some_data = None
	        return (t,)
	        
V0326 23:29:16.823000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "1c1b83b589b09df9aed0a03e688d0f0a"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992156823420.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.823000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "c771e7710c5759cbdd6bf3c0e8df709c"}
	{
	"name": "backend_compile",
	"ts": 1742992156823420.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.825000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "200e28702637465c14c57e0874725dd7"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992156825665.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.832000 324469 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:215] {"aot_forward_graph": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "008f0bae62b8f912d8966d0ef5680793"}
	class <lambda>(torch.nn.Module):
	    def forward(self, arg0_1: "Sym(s0)", arg1_1: "u8[s0, 1][1, 1]cuda:0"):
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute: "u8[1, s0][1, 1]cuda:0" = torch.ops.aten.permute.default(arg1_1, [1, 0]);  arg1_1 = None
	        return (permute,)
	        
V0326 23:29:16.832000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "e946b1d5233fa652195dcad70bb9378b"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992156832337.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.832000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "5c4633c273aca37e0f0caf5b84d28bb9"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992156832962.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.833000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "4a66565074c11d9d8ff48a20073ceee1"}
	{
	"name": "inductor_compile",
	"ts": 1742992156832962.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.836000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/ro/cro4d6xcajw34kn3amfm77p5rbikp6iqqc45sigdaropf6dafvxw.py"}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "0a39d0091a3c2b437e370a8559861577"}
	# AOT ID: ['3_inference']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    arg0_1, arg1_1 = args
	    args.clear()
	    s0 = arg0_1
	    assert_size_stride(arg1_1, (s0, 1), (1, 1))
	    return (reinterpret_tensor(arg1_1, (1, s0), (1, 1), 0), )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    arg0_1 = 524288
	    arg1_1 = rand_strided((524288, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    fn = lambda: call([arg0_1, arg1_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:29:16.836000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "cfd80f0daa9245b89acb91e13af63648"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742992156836663.5,
	"args": {
	"key": "feywvgdkn732db3gwlpsdt4okq5o257p7ly7o33m6etbej3zcs6o",
	"components": [
	"[t5qozjxe7r42o4bunbvqfqasfgh5rki4fvvhuseni4bop27xuky] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1):\n    permute = torch.ops.aten.permute.default(arg1_1, [1, 0]);  arg1_1 = None\n    return (permute,)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[ra3m7brimdni4uv5mdit5d3d5ljftmi2buup3o3swwfvmvhdelk] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([s0, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []",
	"[5ckznxnwokx4wcssswt7y7bj7a6njtitxnutjq3p4vcfi2uszig] fx_kwargs[user_visible_outputs]: {'permute': None}",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 7272635,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:29:16.837000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "cc80d91c1e03448928ba963e755f925d"}
	{"key": "feywvgdkn732db3gwlpsdt4okq5o257p7ly7o33m6etbej3zcs6o", "components": ["[t5qozjxe7r42o4bunbvqfqasfgh5rki4fvvhuseni4bop27xuky] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1):\n    permute = torch.ops.aten.permute.default(arg1_1, [1, 0]);  arg1_1 = None\n    return (permute,)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[ra3m7brimdni4uv5mdit5d3d5ljftmi2buup3o3swwfvmvhdelk] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([s0, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []", "[5ckznxnwokx4wcssswt7y7bj7a6njtitxnutjq3p4vcfi2uszig] fx_kwargs[user_visible_outputs]: {'permute': None}", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 7272635, "cache_state": "hit"}
V0326 23:29:16.837000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "351f69e6e3010ae4af011283337a8828"}
	{
	"name": "inductor_compile",
	"ts": 1742992156837457.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.837000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "94b3873d41d1d61f3b873adae3d4e3fc"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992156837698.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.838000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "fc45d499ebabf2390be4aa819b4caae3"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992156838042.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.839000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "ca8436f545db5f26bf17b0890f0735f0"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992156839244.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.839000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "65cb641c4172b89d560619a3af7d0cb4"}
	{
	"name": "backend_compile",
	"ts": 1742992156839574.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.839000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "998d56482c3663f8bca8ac82e2a9cfe1"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992156839878.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.842000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "ba150ee8bf2677b3711ae3153b179b0c"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['param'], accessed_by=DictGetItemGuardAccessor(param)
	| | +- TYPE_MATCH: ___check_type_id(L['param'], 105190296990752)               
	| | +- GuardManager: source=L['param']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | +- TENSOR_MATCH: check_tensor(L['param']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[None, 1], stride=[1, 1])
	| | | +- NO_HASATTR: hasattr(L['param']._some_data, '_dynamo_dynamic_indices') == False
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['Params4bit'], accessed_by=DictGetItemGuardAccessor(Params4bit)
	| | | +- ID_MATCH: ___check_obj_id(G['Params4bit'], 105190296990752)           
	| | +- GuardManager: source=G['__builtins_dict___6'], accessed_by=DictGetItemGuardAccessor(__builtins_dict___6)
	| | | +- GuardManager: source=G['__builtins_dict___6']['isinstance'], accessed_by=DictGetItemGuardAccessor(isinstance)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___6']['isinstance'], 127873218841152)
	+- LAMBDA_GUARD: 2 <= L['param']._some_data.size()[0]                          # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0326 23:29:16.842000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "75670a6e865e7fb2c7d2f7d4a7a977f9"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992156842445.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.842000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "f57da3f9b67e1369088d5b96f2207a1a"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992156842692.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.842000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "1/1", "frame_key": "4", "co_name": "get_data_transposed", "co_filename": "/tmp/ipykernel_324469/2489781049.py", "co_firstlineno": 69, "cache_size": 1, "accumulated_cache_size": 1, "guard_count": 9, "shape_env_guard_count": 0, "graph_op_count": 1, "graph_node_count": 4, "graph_input_count": 2, "start_time": 1742992156.8157663, "entire_frame_compile_time_s": 0.026616334915161133, "backend_compile_time_s": 0.01612687110900879, "inductor_compile_time_s": 0.0044553279876708984, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.844000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2817] {"artifact": {"name": "recompile_reasons", "encoding": "json"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "e18c77bc8630f1456518e51d5e3eb07c"}
	[
	"2/0: tensor 'L['A']' size mismatch at index 1. expected 2097152, actual 524288"
	]
V0326 23:29:16.844000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 336, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 271, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 496, "name": "forward", "filename": 39}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 110, "name": "inner_transpose_forward", "filename": 40}, {"line": 575, "name": "apply", "filename": 41}, {"line": 34, "name": "forward", "filename": 40}, {"line": 21, "name": "augmented_dequantize_4bit", "filename": 42}, {"line": 330, "name": "fused_dequantize", "filename": 43}]}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.845000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "f1bd33b2de511a427940108ebabfc1f8"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992156844981.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.845000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "2edc0e8beb3ed093a5a0af060f000816"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992156844981.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.847000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 10, "size": 524288}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.847000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [524288, 1], "is_leaf": true, "stride": [1, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6fa64eb0>", "describer_id": 10}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.847000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [1, 524288], "is_leaf": true, "is_view": true, "stride": [1, 1], "storage": 0, "base": 1, "creation_meta": "CreationMeta.DEFAULT", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744afe593840>", "describer_id": 10}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.848000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 0, "source": "L['A']"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.850000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 10, "size": 16384}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.850000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [16384], "is_leaf": true, "stride": [1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c255c20>", "describer_id": 10}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.850000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 3, "source": "L['quant_state'].absmax"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.852000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 10, "size": 1024}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.852000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c255360>", "describer_id": 10}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.853000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 4, "source": "L['quant_state'].state2.code"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.853000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 10, "size": 256}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.853000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [64], "is_leaf": true, "stride": [1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c38fb60>", "describer_id": 10}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.854000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 5, "source": "L['quant_state'].state2.absmax"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.855000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 10, "size": 2}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.856000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c38ff70>", "describer_id": 10}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.856000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 6, "source": "L['quant_state'].offset"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.856000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 10, "size": 64}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.857000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c254640>", "describer_id": 10}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.857000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 7, "source": "L['quant_state'].code"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.864000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_a_": [1, "s1"], "l_quant_state_absmax": ["s2"], "l_quant_state_state2_code": [256], "l_quant_state_state2_absmax": ["s3"], "l_quant_state_offset": [], "l_quant_state_code": [16], "output_ptr": ["s4", 2048]}}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "7cb44ba18984348ae7f41158206ad8aa"}
	class GraphModule(torch.nn.Module):
	    def forward(self, s1: "Sym(s1)", L_A_: "u8[1, s1][1, 1]cuda:0", s2: "Sym(s2)", L_quant_state_absmax: "u8[s2][1]cuda:0", L_quant_state_state2_code: "f32[256][1]cuda:0", s3: "Sym(s3)", L_quant_state_state2_absmax: "f32[s3][1]cuda:0", L_quant_state_offset: "f16[][]cuda:0", L_quant_state_code: "f32[16][1]cuda:0", L_quant_state_shape_0_: "Sym(s4)"):
	        l_a_ = L_A_
	        l_quant_state_absmax = L_quant_state_absmax
	        l_quant_state_state2_code = L_quant_state_state2_code
	        l_quant_state_state2_absmax = L_quant_state_state2_absmax
	        l_quant_state_offset = L_quant_state_offset
	        l_quant_state_code = L_quant_state_code
	        l_quant_state_shape_0_ = L_quant_state_shape_0_
	        
	         # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:332 in fused_dequantize, code: n_elements = torch.numel(A) * 2
	        mul: "Sym(2*s1)" = s1 * 2;  s1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	        output_ptr: "bf16[s4, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_quant_state_absmax, l_quant_state_state2_code, l_quant_state_state2_absmax, l_quant_state_offset, l_a_, l_quant_state_code, mul, 256, 32, 16384, 8192, l_quant_state_shape_0_, 2048, torch.bfloat16);  l_quant_state_absmax = l_quant_state_state2_code = l_quant_state_state2_absmax = l_quant_state_offset = l_a_ = l_quant_state_code = mul = l_quant_state_shape_0_ = None
	        return (output_ptr,)
	        
V0326 23:29:16.865000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "1783e251ad2d8f9b9845b295625c9119"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992156864978.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.865000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "cb9534376bc30247903dad9b3782dcfb"}
	{
	"name": "backend_compile",
	"ts": 1742992156864978.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.868000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "d118e1f74cec723ff7a6084f61c5e5e4"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992156868189.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.882000 324469 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:215] {"aot_forward_graph": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "3d5c1887ea22b2eca1234c87f471283a"}
	class <lambda>(torch.nn.Module):
	    def forward(self, arg0_1: "Sym(s1)", arg1_1: "u8[1, s1][1, 1]cuda:0", arg2_1: "Sym(s2)", arg3_1: "u8[s2][1]cuda:0", arg4_1: "f32[256][1]cuda:0", arg5_1: "Sym(s3)", arg6_1: "f32[s3][1]cuda:0", arg7_1: "f16[][]cuda:0", arg8_1: "f32[16][1]cuda:0", arg9_1: "Sym(s4)"):
	         # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:332 in fused_dequantize, code: n_elements = torch.numel(A) * 2
	        mul: "Sym(2*s1)" = arg0_1 * 2;  arg0_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	        fused_dequantize_op: "bf16[s4, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, arg7_1, arg1_1, arg8_1, mul, 256, 32, 16384, 8192, arg9_1, 2048, torch.bfloat16);  arg3_1 = arg4_1 = arg6_1 = arg7_1 = arg1_1 = arg8_1 = mul = arg9_1 = None
	        return (fused_dequantize_op,)
	        
V0326 23:29:16.882000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "d24e476b3cb1171a835635ac3aae78f2"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992156882467.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.883000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "7b5bb354426b48892d91c5f72677762c"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992156883243.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.883000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "8471d81132bd6cdc67b0baa60c018182"}
	{
	"name": "inductor_compile",
	"ts": 1742992156883243.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.893000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/we/cwejxmelilirrhy2igx7nsmi6g5rwqrfsgnr4fcqgfkbrwlsjorv.py"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "fac5e3747c2228c1dbcfc8fd69f50258"}
	# AOT ID: ['4_inference']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/xa/cxaqzqrucz62vf6o3o6sl2ihhxptbp2wjf7gaoczyt27xbq6aopg.py
	# Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	# Source node to ATen node mapping:
	#   output_ptr => fused_dequantize_op
	# Graph fragment:
	#   %fused_dequantize_op : [num_users=1] = call_function[target=torch.ops.mylib.fused_dequantize_op.default](args = (%arg3_1, %arg4_1, %arg6_1, %arg7_1, %arg1_1, %arg8_1, %mul, 256, 32, 16384, 8192, %arg9_1, 2048, torch.bfloat16), kwargs = {})
	triton_poi_fused_fused_dequantize_op_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {2: 1}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=(2,))]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_fused_dequantize_op_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 1
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    tmp0 = tl.load(in_ptr0 + (0)).to(tl.float32)
	    tmp1 = tl.broadcast_to(tmp0, [XBLOCK])
	    tl.store(out_ptr0 + (tl.full([XBLOCK], 0, tl.int32)), tmp1, None)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1 = args
	    args.clear()
	    s1 = arg0_1
	    s2 = arg2_1
	    s3 = arg5_1
	    s4 = arg9_1
	    assert_size_stride(arg1_1, (1, s1), (1, 1))
	    assert_size_stride(arg3_1, (s2, ), (1, ))
	    assert_size_stride(arg4_1, (256, ), (1, ))
	    assert_size_stride(arg6_1, (s3, ), (1, ))
	    assert_size_stride(arg7_1, (), ())
	    assert_size_stride(arg8_1, (16, ), (1, ))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((), (), torch.float16)
	        # Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_fused_dequantize_op_0.run(arg7_1, buf0, 1, grid=grid(1), stream=stream0)
	        del arg7_1
	        # Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	        buf1 = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, buf0, arg1_1, arg8_1, 2*s1, 256, 32, 16384, 8192, s4, 2048, torch.bfloat16)
	        del arg1_1
	        del arg3_1
	        del arg4_1
	        del arg6_1
	        del arg8_1
	        del buf0
	        buf2 = buf1
	        del buf1
	    return (buf2, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    arg0_1 = 524288
	    arg1_1 = rand_strided((1, 524288), (1, 1), device='cuda:0', dtype=torch.uint8)
	    arg2_1 = 16384
	    arg3_1 = rand_strided((16384, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    arg4_1 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg5_1 = 64
	    arg6_1 = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg7_1 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    arg8_1 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg9_1 = 512
	    fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:29:16.893000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "db8ef9581581ee3f1f228bd9626419bc"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742992156893576.5,
	"args": {
	"key": "fodgjswdodzqgx3iq4x2v2ulrxwzzqobix2qlzyjko7fokxzlgfp",
	"components": [
	"[grf5bl6rpigmyiaz6c6m4gfmm52jce23fz2stipebcebees47ma] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1):\n    mul = arg0_1 * 2;  arg0_1 = None\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, arg7_1, arg1_1, arg8_1, mul, 256, 32, 16384, 8192, arg9_1, 2048, torch.bfloat16);  arg3_1 = arg4_1 = arg6_1 = arg7_1 = arg1_1 = arg8_1 = mul = arg9_1 = None\n    return (fused_dequantize_op,)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[tpvnjs264ct5lugh57xhauij37yhdwzbkxdyhhhhj5de2tskcds] example_inputs[0]: ('s1',)",
	"[7vwriczjssi3zjl7yvm6w4lda6j63tvcm76dxaglqafuowfzcmv] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([1, s1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[2]: ('s2',)",
	"[sqttp5i5wnsdcripsqrsj7sbdbvyr6p2v547c3del3vagfumq23] example_inputs[3]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([s2]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[4]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[5]: ('s3',)",
	"[osj4eueidclo5irb65z7qycfmcyzhzy3mt2msh2p7b3yhteohpt] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([s3]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[j23zbqrguggah6nscnzj4y55d5chhoardpgvlji4ilrfwrr6asz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[2ylzn3y4vcv2t7rbq447jvkl6by5qf32ycwks3h2nbcmm6wvmnk] example_inputs[8]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ioa426km4idgc234gbdjtmikotjmsh7hn3tdqahy4f4zia42h53] example_inputs[9]: ('s4',)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []",
	"[jan4rfottd4caig5fc5h3ieb5ephfyhn7rn27jmu2ehbwgssm5j] fx_kwargs[user_visible_outputs]: {'fused_dequantize_op': None}",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1",
	"[kcuxe2zwm3mzv2uk6adm6iskoy35bqfv725twacrdewod2dbl5d] inputs_to_check[1]: 3",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[2]: 4",
	"[agkvbkaha53nbz3aeeuhvxjvvc4glhfjofzkg6g2qjoo2e5otcx] inputs_to_check[3]: 6",
	"[j3s5elu6itwgjafc7rzhy4whrbufl6kfmlufjhh25grt643bk5f] inputs_to_check[4]: 7",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inputs_to_check[5]: 8",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 36553287,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:29:16.894000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "479c7fc7b4aeb8b6d83b60fc11accd35"}
	{"key": "fodgjswdodzqgx3iq4x2v2ulrxwzzqobix2qlzyjko7fokxzlgfp", "components": ["[grf5bl6rpigmyiaz6c6m4gfmm52jce23fz2stipebcebees47ma] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1):\n    mul = arg0_1 * 2;  arg0_1 = None\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, arg7_1, arg1_1, arg8_1, mul, 256, 32, 16384, 8192, arg9_1, 2048, torch.bfloat16);  arg3_1 = arg4_1 = arg6_1 = arg7_1 = arg1_1 = arg8_1 = mul = arg9_1 = None\n    return (fused_dequantize_op,)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[tpvnjs264ct5lugh57xhauij37yhdwzbkxdyhhhhj5de2tskcds] example_inputs[0]: ('s1',)", "[7vwriczjssi3zjl7yvm6w4lda6j63tvcm76dxaglqafuowfzcmv] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([1, s1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[2]: ('s2',)", "[sqttp5i5wnsdcripsqrsj7sbdbvyr6p2v547c3del3vagfumq23] example_inputs[3]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([s2]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[4]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[5]: ('s3',)", "[osj4eueidclo5irb65z7qycfmcyzhzy3mt2msh2p7b3yhteohpt] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([s3]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[j23zbqrguggah6nscnzj4y55d5chhoardpgvlji4ilrfwrr6asz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[2ylzn3y4vcv2t7rbq447jvkl6by5qf32ycwks3h2nbcmm6wvmnk] example_inputs[8]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ioa426km4idgc234gbdjtmikotjmsh7hn3tdqahy4f4zia42h53] example_inputs[9]: ('s4',)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []", "[jan4rfottd4caig5fc5h3ieb5ephfyhn7rn27jmu2ehbwgssm5j] fx_kwargs[user_visible_outputs]: {'fused_dequantize_op': None}", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1", "[kcuxe2zwm3mzv2uk6adm6iskoy35bqfv725twacrdewod2dbl5d] inputs_to_check[1]: 3", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[2]: 4", "[agkvbkaha53nbz3aeeuhvxjvvc4glhfjofzkg6g2qjoo2e5otcx] inputs_to_check[3]: 6", "[j3s5elu6itwgjafc7rzhy4whrbufl6kfmlufjhh25grt643bk5f] inputs_to_check[4]: 7", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inputs_to_check[5]: 8", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 36553287, "cache_state": "hit"}
V0326 23:29:16.894000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "b57c94d30bf8bf9ebedbe2db71a5138c"}
	{
	"name": "inductor_compile",
	"ts": 1742992156894408.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.894000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "2f7df4eedcf285edbfc24b4f1fe4fb2e"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992156894649.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.895000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "aeef7e94e0b60f7a354c70e4cb8b849e"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992156894983.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.896000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "fbfa3e4042b87d68cc6fd8ba133369d5"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992156896202.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.896000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "229cdd49cbf7683a61bfabab6f11cb01"}
	{
	"name": "backend_compile",
	"ts": 1742992156896592.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.896000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "a291febe9d6b9b0574bc566d6e883066"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992156896875.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.905000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "cb9d0b0e47810045b352b5da232e2938"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['A'], accessed_by=DictGetItemGuardAccessor(A)
	| | +- TENSOR_MATCH: check_tensor(L['A'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[1, None], stride=[1, 1])
	| | +- NO_HASATTR: hasattr(L['A'], '_dynamo_dynamic_indices') == False         
	| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['A'], L['quant_state'].code, L['quant_state'].absmax, L['quant_state'].offset, L['quant_state'].state2.code, L['quant_state'].state2.absmax)
	| +- GuardManager: source=L['quant_state'], accessed_by=DictGetItemGuardAccessor(quant_state)
	| | +- TYPE_MATCH: ___check_type_id(L['quant_state'], 105190295709968)         
	| | +- GuardManager: source=L['quant_state'].code, accessed_by=GetAttrGuardAccessor(code)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].code, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | +- EQUALS_MATCH: L['quant_state'].dtype == torch.bfloat16                    
	| | +- GuardManager: source=L['quant_state'].shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].shape, 127872448968448)   
	| | | +- LENGTH_CHECK: len(L['quant_state'].shape) == 2                            
	| | | +- GuardManager: source=L['quant_state'].shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].shape[0], 127873236398144)
	| | | +- GuardManager: source=L['quant_state'].shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | +- EQUALS_MATCH: L['quant_state'].shape[1] == 2048                           
	| | +- GuardManager: source=L['quant_state'].absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[None], stride=[1])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].absmax, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].offset, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].state2, 105190295709968)  
	| | | +- GuardManager: source=L['quant_state'].state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | +- TENSOR_MATCH: check_tensor(L['quant_state'].state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | +- NO_HASATTR: hasattr(L['quant_state'].state2.code, '_dynamo_dynamic_indices') == False
	| | | | +- NO_TENSOR_ALIASING
	| | | +- GuardManager: source=L['quant_state'].state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | +- TENSOR_MATCH: check_tensor(L['quant_state'].state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[None], stride=[1])
	| | | | +- NO_HASATTR: hasattr(L['quant_state'].state2.absmax, '_dynamo_dynamic_indices') == False
	| | | | +- NO_TENSOR_ALIASING
	| | | +- GuardManager: source=L['quant_state'].state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | +- EQUALS_MATCH: L['quant_state'].state2.blocksize == 256                    
	| | +- GuardManager: source=L['quant_state'].blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | +- EQUALS_MATCH: L['quant_state'].blocksize == 64                            
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)
	| | | +- ID_MATCH: ___check_obj_id(G['torch'], 127872889574384)                
	| | | +- GuardManager: source=G['torch'].numel, accessed_by=GetAttrGuardAccessor(numel)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].numel, 127872890415168)          
	| | +- GuardManager: source=G['DEBUG_FLAG'], accessed_by=DictGetItemGuardAccessor(DEBUG_FLAG)
	| | | +- ID_MATCH: ___check_obj_id(G['DEBUG_FLAG'], 127873236421344)           
	| | +- GuardManager: source=G['fused_dequantize_op'], accessed_by=DictGetItemGuardAccessor(fused_dequantize_op)
	| | | +- TYPE_MATCH: ___check_type_id(G['fused_dequantize_op'], 105190280786160) 
	| | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize_op'], 127867502131200)  
	| | | +- GuardManager: source=G['fused_dequantize_op']._opoverload, accessed_by=GetAttrGuardAccessor(_opoverload)
	| | | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize_op']._opoverload, 127867502233600)
	+- LAMBDA_GUARD: Ne(2048*L['quant_state'].shape[0], 0)                         # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: Ne(L['quant_state'].shape[0], 1)                              # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2048*L['quant_state'].shape[0] >= 2                           # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['A'].size()[1]                                         # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['quant_state'].absmax.size()[0]                        # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['quant_state'].state2.absmax.size()[0]                 # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 0 <= L['quant_state'].shape[0]                                # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0326 23:29:16.905000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "4dd5af323d5fa6f3f6143ce3aa3fb3b3"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992156905384.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.905000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "dabe2b5ea25fba42c161726c702b4c84"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992156905628.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.905000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "2/1", "frame_key": "5", "co_name": "fused_dequantize", "co_filename": "/home/tom/unsloth-challenges/submit/Problem_C/kernels.py", "co_firstlineno": 330, "cache_size": 1, "accumulated_cache_size": 1, "guard_count": 25, "shape_env_guard_count": 6, "graph_op_count": 2, "graph_node_count": 13, "graph_input_count": 10, "start_time": 1742992156.8449767, "entire_frame_compile_time_s": 0.060373783111572266, "backend_compile_time_s": 0.031586408615112305, "inductor_compile_time_s": 0.011125564575195312, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": ["mylib::fused_dequantize_op"], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:16.915000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/tmp/ipykernel_324469/318589162.py", 44]}
V0326 23:29:16.915000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 10, "name": "compiled_llama_mlp", "filename": 44}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.915000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "abf3ceded8ec495649e8531815a1f7ea"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992156915711.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.916000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "7dd7c8c297fe09de7804935d7038d464"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992156915711.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:16.930000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 12, "size": 409600}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.931000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 3, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [1, 100, 2048], "requires_grad": true, "stride": [204800, 2048, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744afe59c4b0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.931000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 0, "source": "L['x']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.939000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 12, "size": 8388608}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.939000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [8388608, 1], "is_leaf": true, "stride": [1, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6fa65590>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:16.939000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 1, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.127000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 12, "size": 262144}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.127000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [262144], "is_leaf": true, "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c246760>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.127000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 3, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.128000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 12, "size": 1024}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.128000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c247480>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.128000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 4, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.129000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 12, "size": 4096}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.129000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024], "is_leaf": true, "stride": [1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c360230>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.130000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 5, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.130000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 12, "size": 2}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.131000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c362530>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.131000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 6, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.131000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 6, "describer_id": 12, "size": 64}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.131000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 6, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c247f70>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.132000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 7, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.185000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 7, "describer_id": 12, "size": 262144}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.185000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 19, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 2048], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2048, 1], "storage": 7, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x744b6c23c5a0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.185000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 19, "source": "L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.190000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/lazy.py", 45]}
V0326 23:29:17.190000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/nn_module.py", 46]}
V0326 23:29:17.190000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py", 47]}
V0326 23:29:17.191000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/torch.py", 48]}
V0326 23:29:17.191000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_prims_common/wrappers.py", 49]}
V0326 23:29:17.191000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_decomp/decompositions.py", 50]}
V0326 23:29:17.191000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_ops.py", 51]}
V0326 23:29:17.191000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_meta_registrations.py", 52]}
V0326 23:29:17.191000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", 53]}
V0326 23:29:17.192000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s1", "sources": ["L['x'].size()[2]"], "value": "2048", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 897, "name": "call_function", "filename": 48}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 271, "name": "_fn", "filename": 49}, {"line": 4364, "name": "matmul", "filename": 50}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 2013, "name": "_dispatch_impl", "filename": 32}, {"line": 716, "name": "__call__", "filename": 51}, {"line": 273, "name": "_fn", "filename": 49}, {"line": 2100, "name": "meta_mm", "filename": 52}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 518, "name": "forward", "filename": 39}, {"line": 125, "name": "forward", "filename": 53}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.194000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 8, "describer_id": 12, "size": 1048576}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.194000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 24, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [8192, 32], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [32, 1], "storage": 8, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x744b6c245f90>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.195000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 24, "source": "L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.200000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s5", "sources": ["L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0]"], "value": "8192", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2220, "name": "BINARY_OP", "filename": 27}, {"line": 301, "name": "impl", "filename": 27}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 943, "name": "_handle_insert_op_in_graph", "filename": 28}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 1924, "name": "_dispatch_impl", "filename": 32}, {"line": 831, "name": "fast_binary_impl", "filename": 33}, {"line": 785, "name": "infer_size", "filename": 33}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 536, "name": "forward", "filename": 39}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.224000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 9, "describer_id": 12, "size": 8388608}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.224000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 29, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [8388608, 1], "is_leaf": true, "stride": [1, 1], "storage": 9, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6fa64f50>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.225000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 29, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.231000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 10, "describer_id": 12, "size": 262144}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.231000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 30, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [262144], "is_leaf": true, "stride": [1], "storage": 10, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c2504b0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.232000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 30, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.232000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 11, "describer_id": 12, "size": 1024}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.233000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 31, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 11, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c35e300>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.233000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 31, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.233000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 12, "describer_id": 12, "size": 4096}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.234000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 32, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024], "is_leaf": true, "stride": [1], "storage": 12, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c35dd60>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.234000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 32, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.234000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 13, "describer_id": 12, "size": 2}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.235000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 33, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 13, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c35d6d0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.235000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 33, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.235000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 14, "describer_id": 12, "size": 64}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.236000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 34, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 14, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c252620>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.236000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 34, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.255000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/misc.py", 54]}
V0326 23:29:17.255000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/higher_order_ops.py", 55]}
V0326 23:29:17.255000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s8", "sources": ["L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1]"], "value": "2048", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 1680, "name": "CALL_FUNCTION_EX", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 1024, "name": "call_function", "filename": 54}, {"line": 774, "name": "call_method", "filename": 54}, {"line": 699, "name": "call_apply", "filename": 54}, {"line": 2015, "name": "call_function", "filename": 55}, {"line": 462, "name": "speculate_subgraph", "filename": 55}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 897, "name": "call_function", "filename": 48}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 271, "name": "_fn", "filename": 49}, {"line": 4364, "name": "matmul", "filename": 50}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 2013, "name": "_dispatch_impl", "filename": 32}, {"line": 716, "name": "__call__", "filename": 51}, {"line": 273, "name": "_fn", "filename": 49}, {"line": 2100, "name": "meta_mm", "filename": 52}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 496, "name": "forward", "filename": 39}, {"line": 110, "name": "inner_transpose_forward", "filename": 40}, {"line": 34, "name": "forward", "filename": 40}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.278000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 15, "describer_id": 12, "size": 262144}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.279000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 46, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 2048], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2048, 1], "storage": 15, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x744b6c247660>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.279000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 46, "source": "L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.282000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 16, "describer_id": 12, "size": 1048576}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.282000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 50, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [8192, 32], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [32, 1], "storage": 16, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x744b6c2459f0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.283000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 50, "source": "L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.287000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s9", "sources": ["L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0]"], "value": "8192", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2220, "name": "BINARY_OP", "filename": 27}, {"line": 301, "name": "impl", "filename": 27}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 943, "name": "_handle_insert_op_in_graph", "filename": 28}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 1924, "name": "_dispatch_impl", "filename": 32}, {"line": 831, "name": "fast_binary_impl", "filename": 33}, {"line": 785, "name": "infer_size", "filename": 33}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 536, "name": "forward", "filename": 39}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.301000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 17, "describer_id": 12, "size": 8388608}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.302000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 54, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [8388608, 1], "is_leaf": true, "stride": [1, 1], "storage": 17, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6fa65270>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.302000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 54, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.308000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 18, "describer_id": 12, "size": 262144}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.309000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 55, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [262144], "is_leaf": true, "stride": [1], "storage": 18, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6fa3be30>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.309000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 55, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.310000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 19, "describer_id": 12, "size": 1024}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.310000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 56, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 19, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b78f51db0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.310000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 56, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.311000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 20, "describer_id": 12, "size": 4096}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.311000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 57, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024], "is_leaf": true, "stride": [1], "storage": 20, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c24d720>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.311000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 57, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.312000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 21, "describer_id": 12, "size": 2}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.312000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 58, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 21, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c24ddb0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.312000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 58, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.313000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 22, "describer_id": 12, "size": 64}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.313000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 59, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 22, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c390e60>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.313000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 59, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.328000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s12", "sources": ["L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1]"], "value": "8192", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 1680, "name": "CALL_FUNCTION_EX", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 1024, "name": "call_function", "filename": 54}, {"line": 774, "name": "call_method", "filename": 54}, {"line": 699, "name": "call_apply", "filename": 54}, {"line": 2015, "name": "call_function", "filename": 55}, {"line": 462, "name": "speculate_subgraph", "filename": 55}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 897, "name": "call_function", "filename": 48}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 271, "name": "_fn", "filename": 49}, {"line": 4364, "name": "matmul", "filename": 50}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 2013, "name": "_dispatch_impl", "filename": 32}, {"line": 716, "name": "__call__", "filename": 51}, {"line": 273, "name": "_fn", "filename": 49}, {"line": 2100, "name": "meta_mm", "filename": 52}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 496, "name": "forward", "filename": 39}, {"line": 110, "name": "inner_transpose_forward", "filename": 40}, {"line": 34, "name": "forward", "filename": 40}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.353000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 23, "describer_id": 12, "size": 1048576}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.353000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 71, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 8192], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [8192, 1], "storage": 23, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x744b6c27c500>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.353000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 71, "source": "L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.357000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 24, "describer_id": 12, "size": 262144}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.357000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 76, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [2048, 32], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [32, 1], "storage": 24, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x744b6faa4190>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.357000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 76, "source": "L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.362000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s13", "sources": ["L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0]"], "value": "2048", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2220, "name": "BINARY_OP", "filename": 27}, {"line": 301, "name": "impl", "filename": 27}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 943, "name": "_handle_insert_op_in_graph", "filename": 28}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 1924, "name": "_dispatch_impl", "filename": 32}, {"line": 831, "name": "fast_binary_impl", "filename": 33}, {"line": 785, "name": "infer_size", "filename": 33}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 536, "name": "forward", "filename": 39}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.367000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", 56]}
V0326 23:29:17.367000 324469 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/fx/passes/runtime_assert.py", 57]}
V0326 23:29:17.367000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s4", "sources": ["L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1]"], "value": "2048", "reason": "find", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2987, "name": "RETURN_VALUE", "filename": 27}, {"line": 2972, "name": "_return", "filename": 27}, {"line": 1142, "name": "compile_subgraph", "filename": 56}, {"line": 1317, "name": "compile_and_call_fx_graph", "filename": 56}, {"line": 287, "name": "insert_deferred_runtime_asserts", "filename": 57}, {"line": 280, "name": "match_symbol", "filename": 57}, {"line": 169, "name": "expr", "filename": 36}, {"line": 1532, "name": "wrapper", "filename": 35}, {"line": 4572, "name": "replace", "filename": 35}, {"line": 1532, "name": "wrapper", "filename": 35}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 4885, "name": "_find", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": null}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:17.377000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_x_": [1, "s0", "2048"], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data": [8388608, 1], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax": [262144], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code": [256], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax": [1024], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset": [], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code": [16], "l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_": [32, 2048], "l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_": [8192, 32], "l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data": [8388608, 1], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax": [262144], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code": [256], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax": [1024], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset": [], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code": [16], "l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_": [32, 2048], "l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_": [8192, 32], "l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data": [8388608, 1], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax": [262144], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code": [256], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax": [1024], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset": [], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code": [16], "l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_": [32, 8192], "l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_": [2048, 32], "x": [1, "s0", "2048"], "B": [1, 8388608], "autograd_function_apply": [1, "s0", "8192"], "result": [1, "s0", "8192"], "result_1": [1, "s0", "8192"], "linear": [1, "s0", 32], "linear_1": [1, "s0", 8192], "output": [1, "s0", 8192], "result_2": [1, "s0", 8192], "silu": [1, "s0", 8192], "x_1": [1, "s0", 2048], "B_1": [1, 8388608], "autograd_function_apply_1": [1, "s0", "8192"], "result_3": [1, "s0", "8192"], "result_4": [1, "s0", "8192"], "linear_2": [1, "s0", 32], "linear_3": [1, "s0", 8192], "output_1": [1, "s0", 8192], "result_5": [1, "s0", 8192], "mul_2": [1, "s0", 8192], "x_2": [1, "s0", 8192], "B_2": [1, 8388608], "autograd_function_apply_2": [1, "s0", "2048"], "result_6": [1, "s0", "2048"], "result_7": [1, "s0", "2048"], "linear_4": [1, "s0", 32], "linear_5": [1, "s0", 2048], "output_2": [1, "s0", 2048], "result_8": [1, "s0", 2048]}}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "b14c1f4a858a8cd2ba579afbc0427fea"}
	class GraphModule(torch.nn.Module):
	    def forward(self, s0: "Sym(s0)", L_x_: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data: "u8[8388608, 1][1, 1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s2)", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s3)", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", L_self_modules_gate_proj_modules_lora_A_modules_default_parameters_weight_: "f32[32, 2048][2048, 1]cuda:0", L_self_modules_gate_proj_modules_lora_B_modules_default_parameters_weight_: "f32[8192, 32][32, 1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_some_data: "u8[8388608, 1][1, 1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s6)", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s7)", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", L_self_modules_up_proj_modules_lora_A_modules_default_parameters_weight_: "f32[32, 2048][2048, 1]cuda:0", L_self_modules_up_proj_modules_lora_B_modules_default_parameters_weight_: "f32[8192, 32][32, 1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_some_data: "u8[8388608, 1][1, 1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s10)", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s11)", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(8192)", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(2048)", L_self_modules_down_proj_modules_lora_A_modules_default_parameters_weight_: "f32[32, 8192][8192, 1]cuda:0", L_self_modules_down_proj_modules_lora_B_modules_default_parameters_weight_: "f32[2048, 32][32, 1]cuda:0"):
	        l_x_ = L_x_
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data = L_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_
	        l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_ = L_self_modules_gate_proj_modules_lora_A_modules_default_parameters_weight_
	        l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_ = L_self_modules_gate_proj_modules_lora_B_modules_default_parameters_weight_
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data = L_self_modules_up_proj_modules_base_layer_parameters_weight_some_data
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_
	        l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_ = L_self_modules_up_proj_modules_lora_A_modules_default_parameters_weight_
	        l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_ = L_self_modules_up_proj_modules_lora_B_modules_default_parameters_weight_
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data = L_self_modules_down_proj_modules_base_layer_parameters_weight_some_data
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_
	        l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_ = L_self_modules_down_proj_modules_lora_A_modules_default_parameters_weight_
	        l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_ = L_self_modules_down_proj_modules_lora_B_modules_default_parameters_weight_
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        x: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = l_x_.to(torch.bfloat16)
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        B: "u8[1, 8388608][1, 1]cuda:0" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data.t();  l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        function_ctx = torch.autograd.function.FunctionCtx();  function_ctx = None
	        fwd_body_0 = self.fwd_body_0
	        bwd_body_0 = self.bwd_body_0
	        autograd_function_apply: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.higher_order.autograd_function_apply(fwd_body_0, bwd_body_0, x, B, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, args_tensor_mask = [True, True, False, False, False], non_differentiable_idx = []);  fwd_body_0 = bwd_body_0 = x = B = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	        result: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = autograd_function_apply.to(torch.float16);  autograd_function_apply = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        result_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result.clone();  result = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        linear: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_, None);  l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_ = None
	        linear_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(linear, l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_, None);  linear = l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_ = None
	        output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = linear_1 * 2.0;  linear_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        result_2: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result_1 + output;  result_1 = output = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        silu: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.nn.functional.silu(result_2, inplace = False);  result_2 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        x_1: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = l_x_.to(torch.bfloat16)
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        B_1: "u8[1, 8388608][1, 1]cuda:0" = l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data.t();  l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        function_ctx_1 = torch.autograd.function.FunctionCtx();  function_ctx_1 = None
	        fwd_body_1 = self.fwd_body_1
	        bwd_body_1 = self.bwd_body_1
	        autograd_function_apply_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.higher_order.autograd_function_apply(fwd_body_1, bwd_body_1, x_1, B_1, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, args_tensor_mask = [True, True, False, False, False], non_differentiable_idx = []);  fwd_body_1 = bwd_body_1 = x_1 = B_1 = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	        result_3: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = autograd_function_apply_1.to(torch.float16);  autograd_function_apply_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        result_4: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result_3.clone();  result_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        linear_2: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_, None);  l_x_ = l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_ = None
	        linear_3: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(linear_2, l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_, None);  linear_2 = l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_ = None
	        output_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = linear_3 * 2.0;  linear_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        result_5: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result_4 + output_1;  result_4 = output_1 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_2: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = silu * result_5;  silu = result_5 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        x_2: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = mul_2.to(torch.bfloat16)
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        B_2: "u8[1, 8388608][1, 1]cuda:0" = l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data.t();  l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        function_ctx_2 = torch.autograd.function.FunctionCtx();  function_ctx_2 = None
	        fwd_body_2 = self.fwd_body_2
	        bwd_body_2 = self.bwd_body_2
	        autograd_function_apply_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.higher_order.autograd_function_apply(fwd_body_2, bwd_body_2, x_2, B_2, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, args_tensor_mask = [True, True, False, False, False], non_differentiable_idx = []);  fwd_body_2 = bwd_body_2 = x_2 = B_2 = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	        result_6: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = autograd_function_apply_2.to(torch.float16);  autograd_function_apply_2 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        result_7: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = result_6.clone();  result_6 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        linear_4: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch._C._nn.linear(mul_2, l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_, None);  mul_2 = l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_ = None
	        linear_5: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch._C._nn.linear(linear_4, l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_, None);  linear_4 = l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_ = None
	        output_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = linear_5 * 2.0;  linear_5 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        result_8: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = result_7 + output_2;  result_7 = output_2 = None
	        return (result_8,)
	        
	    class fwd_body_0(torch.nn.Module):
	        def forward(self, ctx, A: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", B: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s3)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s2)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            a = A
	            b = B
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:21 in forward, code: if prod(A.shape) == 0:
	            size = a.size()
	            getitem = size[0];  getitem = None
	            getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
	            getitem_2: "Sym(2048)" = size[2];  getitem_2 = None
	            prod: "Sym(2048*s0)" = math_prod(size);  size = None
	            eq: "Sym(False)" = prod == 0;  prod = eq = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s3/2))" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s2*s3)" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s2*s3/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  rshift = mul = rshift_1 = None
	            
	             # File: /tmp/ipykernel_324469/636066363.py:21 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:34 in forward, code: output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias) # NOTE the transposition
	            to: "bf16[2048, 8192][1, 2048]cuda:0" = t.to(torch.bfloat16);  t = None
	            t_1: "bf16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(a, t_1, None);  a = t_1 = None
	            return (output, [l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_])
	            
	    class bwd_body_0(torch.nn.Module):
	        def forward(self, ctx, grad_output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s3)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s2)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", b: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            # No stacktrace found for following nodes
	            _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s3/2))" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s2*s3)" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize;  l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s2*s3/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset = b = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = rshift = mul = rshift_1 = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	            
	             # File: /tmp/ipykernel_324469/636066363.py:21 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:65 in backward, code: grad_A = torch.matmul(grad_output, F.dequantize_4bit(B, ctx.state).to(grad_output.dtype).t()) # NOTE the transposition
	            to: "f16[2048, 8192][1, 2048]cuda:0" = t.to(torch.float16);  t = None
	            t_1: "f16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            grad_A: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.matmul(grad_output, t_1);  grad_output = t_1 = None
	            
	            # No stacktrace found for following nodes
	            _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None
	            return (grad_A, None)
	            
	    class fwd_body_1(torch.nn.Module):
	        def forward(self, ctx, A: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", B: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s7)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s6)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            a = A
	            b = B
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:21 in forward, code: if prod(A.shape) == 0:
	            size = a.size()
	            getitem = size[0];  getitem = None
	            getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
	            getitem_2 = size[2];  getitem_2 = None
	            prod: "Sym(2048*s0)" = math_prod(size);  size = None
	            eq: "Sym(False)" = prod == 0;  prod = eq = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s7/2))" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s6*s7)" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s6*s7/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  rshift = mul = rshift_1 = None
	            
	             # File: /tmp/ipykernel_324469/636066363.py:21 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:34 in forward, code: output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias) # NOTE the transposition
	            to: "bf16[2048, 8192][1, 2048]cuda:0" = t.to(torch.bfloat16);  t = None
	            t_1: "bf16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(a, t_1, None);  a = t_1 = None
	            return (output, [l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_])
	            
	    class bwd_body_1(torch.nn.Module):
	        def forward(self, ctx, grad_output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s7)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s6)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", b: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            # No stacktrace found for following nodes
	            _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s7/2))" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s6*s7)" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize;  l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s6*s7/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset = b = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = rshift = mul = rshift_1 = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	            
	             # File: /tmp/ipykernel_324469/636066363.py:21 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:65 in backward, code: grad_A = torch.matmul(grad_output, F.dequantize_4bit(B, ctx.state).to(grad_output.dtype).t()) # NOTE the transposition
	            to: "f16[2048, 8192][1, 2048]cuda:0" = t.to(torch.float16);  t = None
	            t_1: "f16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            grad_A: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.matmul(grad_output, t_1);  grad_output = t_1 = None
	            
	            # No stacktrace found for following nodes
	            _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None
	            return (grad_A, None)
	            
	    class fwd_body_2(torch.nn.Module):
	        def forward(self, ctx, A: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", B: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s11)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s10)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(2048)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(8192)"):
	            a = A
	            b = B
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:21 in forward, code: if prod(A.shape) == 0:
	            size = a.size()
	            getitem = size[0];  getitem = None
	            getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
	            getitem_2 = size[2];  getitem_2 = None
	            prod: "Sym(8192*s0)" = math_prod(size);  size = None
	            eq: "Sym(False)" = prod == 0;  prod = eq = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s11/2))" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s10*s11)" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s10*s11/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  rshift = mul = rshift_1 = None
	            
	             # File: /tmp/ipykernel_324469/636066363.py:21 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[8192, 2048][1, 8192]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:34 in forward, code: output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias) # NOTE the transposition
	            to: "bf16[8192, 2048][1, 8192]cuda:0" = t.to(torch.bfloat16);  t = None
	            t_1: "bf16[2048, 8192][8192, 1]cuda:0" = to.t();  to = None
	            output: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch._C._nn.linear(a, t_1, None);  a = t_1 = None
	            return (output, [l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_])
	            
	    class bwd_body_2(torch.nn.Module):
	        def forward(self, ctx, grad_output: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s11)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s10)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", b: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(2048)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(8192)"):
	            # No stacktrace found for following nodes
	            _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s11/2))" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s10*s11)" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize;  l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s10*s11/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset = b = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = rshift = mul = rshift_1 = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	            
	             # File: /tmp/ipykernel_324469/636066363.py:21 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[8192, 2048][1, 8192]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:65 in backward, code: grad_A = torch.matmul(grad_output, F.dequantize_4bit(B, ctx.state).to(grad_output.dtype).t()) # NOTE the transposition
	            to: "f16[8192, 2048][1, 8192]cuda:0" = t.to(torch.float16);  t = None
	            t_1: "f16[2048, 8192][8192, 1]cuda:0" = to.t();  to = None
	            grad_A: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.matmul(grad_output, t_1);  grad_output = t_1 = None
	            
	            # No stacktrace found for following nodes
	            _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None
	            return (grad_A, None)
	            
V0326 23:29:17.378000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "9c39a1955a1585f1bafc9df4f298e0e6"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992157378227.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:17.378000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "b727fc07d3cd2015b6a8e9c9535eff60"}
	{
	"name": "backend_compile",
	"ts": 1742992157378227.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:17.390000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "7e4e4c7f7f7c893500b76cf4a299e663"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992157390352.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:17.641000 324469 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "249aa606ddadedc59d4b20c6a60fdc51"}
	class joint_helper(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "Sym(s0)"; primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"; primals_3: "u8[8388608, 1][1, 1]cuda:0"; primals_4: "Sym(s2)"; primals_5: "Sym(s3)"; primals_6: "u8[262144][1]cuda:0"; primals_7: "f32[256][1]cuda:0"; primals_8: "f32[1024][1]cuda:0"; primals_9: "f16[][]cuda:0"; primals_10: "f32[16][1]cuda:0"; primals_11: "Sym(2048)"; primals_12: "Sym(8192)"; primals_13: "f32[32, 2048][2048, 1]cuda:0"; primals_14: "f32[8192, 32][32, 1]cuda:0"; primals_15: "u8[8388608, 1][1, 1]cuda:0"; primals_16: "Sym(s6)"; primals_17: "Sym(s7)"; primals_18: "u8[262144][1]cuda:0"; primals_19: "f32[256][1]cuda:0"; primals_20: "f32[1024][1]cuda:0"; primals_21: "f16[][]cuda:0"; primals_22: "f32[16][1]cuda:0"; primals_23: "Sym(2048)"; primals_24: "Sym(8192)"; primals_25: "f32[32, 2048][2048, 1]cuda:0"; primals_26: "f32[8192, 32][32, 1]cuda:0"; primals_27: "u8[8388608, 1][1, 1]cuda:0"; primals_28: "Sym(s10)"; primals_29: "Sym(s11)"; primals_30: "u8[262144][1]cuda:0"; primals_31: "f32[256][1]cuda:0"; primals_32: "f32[1024][1]cuda:0"; primals_33: "f16[][]cuda:0"; primals_34: "f32[16][1]cuda:0"; primals_35: "Sym(8192)"; primals_36: "Sym(2048)"; primals_37: "f32[32, 8192][8192, 1]cuda:0"; primals_38: "f32[2048, 32][32, 1]cuda:0"; tangents_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.bfloat16)
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift: "Sym(floor(s3/2))" = primals_5 >> 1
	        mul_4: "Sym(s2*s3)" = primals_4 * primals_5;  primals_5 = None
	        rshift_1: "Sym(floor(s2*s3/2))" = mul_4 >> 1
	        fused_dequantize_op: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16)
	        permute_1: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None
	        permute_2: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0]);  permute_1 = None
	        convert_element_type_1: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None
	        convert_element_type_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type, torch.float16);  convert_element_type = None
	        permute_3: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None
	        view: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2, [primals_1, 2048]);  convert_element_type_2 = None
	        mm: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view, permute_3);  view = permute_3 = None
	        view_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        clone: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.clone.default(view_1);  view_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_5: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None
	        permute_4: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None
	        view_2: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(primals_2, [primals_1, 2048])
	        mm_1: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_2, permute_4)
	        view_3: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None
	        convert_element_type_8: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None
	        permute_5: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None
	        view_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None
	        mm_2: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_4, permute_5)
	        view_5: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None
	        mul_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(clone, mul_39);  clone = mul_39 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        convert_element_type_11: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_13: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.bfloat16)
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_6: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_2: "Sym(floor(s7/2))" = primals_17 >> 1
	        mul_54: "Sym(s6*s7)" = primals_16 * primals_17;  primals_17 = None
	        rshift_3: "Sym(floor(s6*s7/2))" = mul_54 >> 1
	        fused_dequantize_op_1: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16)
	        permute_7: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None
	        permute_8: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0]);  permute_7 = None
	        convert_element_type_14: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None
	        convert_element_type_15: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_13, torch.float16);  convert_element_type_13 = None
	        permute_9: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None
	        view_6: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_15, [primals_1, 2048]);  convert_element_type_15 = None
	        mm_3: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_6, permute_9);  view_6 = permute_9 = None
	        view_7: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        clone_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.clone.default(view_7);  view_7 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_18: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None
	        permute_10: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None
	        view_8: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None
	        mm_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_8, permute_10)
	        view_9: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None
	        convert_element_type_21: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None
	        permute_11: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None
	        view_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None
	        mm_5: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_10, permute_11)
	        view_11: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None
	        mul_87: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_85: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(clone_1, mul_87);  clone_1 = mul_87 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_94: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85)
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_24: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_94, torch.bfloat16)
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_12: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_4: "Sym(floor(s11/2))" = primals_29 >> 1
	        mul_102: "Sym(s10*s11)" = primals_28 * primals_29;  primals_29 = None
	        rshift_5: "Sym(floor(s10*s11/2))" = mul_102 >> 1
	        fused_dequantize_op_2: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16)
	        permute_13: "bf16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None
	        permute_14: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_13, [1, 0]);  permute_13 = None
	        convert_element_type_25: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None
	        convert_element_type_26: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_24, torch.float16);  convert_element_type_24 = None
	        permute_15: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None
	        view_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_26, [primals_1, 8192]);  convert_element_type_26 = None
	        mm_6: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None
	        view_13: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        clone_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.clone.default(view_13);  view_13 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_29: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None
	        permute_16: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None
	        view_14: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None
	        mm_7: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_14, permute_16)
	        view_15: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None
	        convert_element_type_32: "f16[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None
	        permute_17: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None
	        view_16: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None
	        mm_8: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_16, permute_17)
	        view_17: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None
	        mul_137: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_131: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(clone_2, mul_137);  clone_2 = mul_137 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_144: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, 2.0)
	        view_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None
	        permute_18: "f16[2048, s0][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_18, [1, 0])
	        mm_9: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None
	        permute_19: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None
	        permute_20: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None
	        mm_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None
	        view_19: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None
	        permute_21: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None
	        convert_element_type_39: "f32[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None
	        view_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None
	        permute_22: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_20, [1, 0])
	        mm_11: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None
	        permute_23: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None
	        permute_24: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None
	        mm_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None
	        view_21: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None
	        permute_25: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None
	        convert_element_type_44: "f32[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        fused_dequantize_op_3: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None
	        permute_26: "bf16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_3, [1, 0]);  fused_dequantize_op_3 = None
	        convert_element_type_45: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.prims.convert_element_type.default(permute_26, torch.float16);  permute_26 = None
	        permute_27: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None
	        view_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None
	        mm_13: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None
	        view_23: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None
	        convert_element_type_48: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_23, torch.bfloat16);  view_23 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_49: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_48, torch.float16);  convert_element_type_48 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_135: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_21, convert_element_type_49);  view_21 = convert_element_type_49 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_146: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None
	        mul_147: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_148: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_146, 2.0)
	        view_24: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None
	        permute_28: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_24, [1, 0])
	        mm_14: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None
	        permute_29: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None
	        permute_30: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None
	        mm_15: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None
	        view_25: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None
	        permute_31: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None
	        convert_element_type_54: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None
	        view_26: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None
	        permute_32: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_26, [1, 0])
	        mm_16: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_32, view_8);  permute_32 = view_8 = None
	        permute_33: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None
	        permute_34: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None
	        mm_17: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None
	        view_27: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None
	        permute_35: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None
	        convert_element_type_59: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        fused_dequantize_op_4: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None
	        permute_36: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_4, [1, 0]);  fused_dequantize_op_4 = None
	        convert_element_type_60: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_36, torch.float16);  permute_36 = None
	        permute_37: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None
	        view_28: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None
	        mm_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None
	        view_29: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None
	        convert_element_type_63: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_29, torch.bfloat16);  view_29 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_64: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_63, torch.float16);  convert_element_type_63 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_136: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_27, convert_element_type_64);  view_27 = convert_element_type_64 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        sigmoid_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(add_39)
	        full: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_44: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(full, sigmoid_1);  full = None
	        mul_150: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None
	        add_137: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None
	        mul_151: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None
	        mul_152: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_153: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_152, 2.0)
	        view_30: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None
	        permute_39: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_30, [1, 0])
	        mm_19: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None
	        permute_40: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None
	        permute_41: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None
	        mm_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None
	        view_31: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None
	        permute_42: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None
	        convert_element_type_69: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None
	        view_32: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None
	        permute_43: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_32, [1, 0])
	        mm_21: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None
	        permute_44: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None
	        permute_45: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None
	        mm_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None
	        view_33: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        add_138: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_46: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None
	        convert_element_type_74: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        fused_dequantize_op_5: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None
	        permute_47: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_5, [1, 0]);  fused_dequantize_op_5 = None
	        convert_element_type_75: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_47, torch.float16);  permute_47 = None
	        permute_48: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None
	        view_34: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None
	        mm_23: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None
	        view_35: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None
	        convert_element_type_78: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_35, torch.bfloat16);  view_35 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_79: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_78, torch.float16);  convert_element_type_78 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_139: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_138, convert_element_type_79);  add_138 = convert_element_type_79 = None
	        return pytree.tree_unflatten([add_131, None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39], self._out_spec)
	        
V0326 23:29:17.708000 324469 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "da0cd3eb0c08434120e89e55970e93b0"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "Sym(s0)", primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", primals_3: "u8[8388608, 1][1, 1]cuda:0", primals_4: "Sym(s2)", primals_5: "Sym(s3)", primals_6: "u8[262144][1]cuda:0", primals_7: "f32[256][1]cuda:0", primals_8: "f32[1024][1]cuda:0", primals_9: "f16[][]cuda:0", primals_10: "f32[16][1]cuda:0", primals_11: "Sym(2048)", primals_12: "Sym(8192)", primals_13: "f32[32, 2048][2048, 1]cuda:0", primals_14: "f32[8192, 32][32, 1]cuda:0", primals_15: "u8[8388608, 1][1, 1]cuda:0", primals_16: "Sym(s6)", primals_17: "Sym(s7)", primals_18: "u8[262144][1]cuda:0", primals_19: "f32[256][1]cuda:0", primals_20: "f32[1024][1]cuda:0", primals_21: "f16[][]cuda:0", primals_22: "f32[16][1]cuda:0", primals_23: "Sym(2048)", primals_24: "Sym(8192)", primals_25: "f32[32, 2048][2048, 1]cuda:0", primals_26: "f32[8192, 32][32, 1]cuda:0", primals_27: "u8[8388608, 1][1, 1]cuda:0", primals_28: "Sym(s10)", primals_29: "Sym(s11)", primals_30: "u8[262144][1]cuda:0", primals_31: "f32[256][1]cuda:0", primals_32: "f32[1024][1]cuda:0", primals_33: "f16[][]cuda:0", primals_34: "f32[16][1]cuda:0", primals_35: "Sym(8192)", primals_36: "Sym(2048)", primals_37: "f32[32, 8192][8192, 1]cuda:0", primals_38: "f32[2048, 32][32, 1]cuda:0"):
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift: "Sym(floor(s3/2))" = primals_5 >> 1
	        mul_4: "Sym(s2*s3)" = primals_4 * primals_5;  primals_5 = None
	        rshift_1: "Sym(floor(s2*s3/2))" = mul_4 >> 1
	        fused_dequantize_op: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None
	        permute_1: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None
	        permute_2: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0])
	        convert_element_type_1: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None
	        convert_element_type_default_5: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.float16)
	        permute_3: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None
	        view: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_default_5, [primals_1, 2048]);  convert_element_type_default_5 = None
	        mm: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view, permute_3);  permute_3 = None
	        view_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_5: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None
	        permute_4: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None
	        view_2: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None
	        mm_1: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_2, permute_4)
	        view_3: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None
	        convert_element_type_8: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None
	        permute_5: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None
	        view_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None
	        mm_2: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_4, permute_5)
	        view_5: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None
	        mul_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1, mul_39);  view_1 = mul_39 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        convert_element_type_11: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_6: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_2: "Sym(floor(s7/2))" = primals_17 >> 1
	        mul_54: "Sym(s6*s7)" = primals_16 * primals_17;  primals_17 = None
	        rshift_3: "Sym(floor(s6*s7/2))" = mul_54 >> 1
	        fused_dequantize_op_1: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None
	        permute_7: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None
	        permute_8: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0])
	        convert_element_type_14: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None
	        permute_9: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None
	        mm_3: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view, permute_9);  view = permute_9 = None
	        view_7: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_18: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None
	        permute_10: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None
	        mm_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_2, permute_10)
	        view_9: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None
	        convert_element_type_21: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None
	        permute_11: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None
	        view_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None
	        mm_5: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_10, permute_11)
	        view_11: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None
	        mul_87: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_85: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_7, mul_87);  view_7 = mul_87 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_94: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85);  convert_element_type_12 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_12: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_4: "Sym(floor(s11/2))" = primals_29 >> 1
	        mul_102: "Sym(s10*s11)" = primals_28 * primals_29;  primals_29 = None
	        rshift_5: "Sym(floor(s10*s11/2))" = mul_102 >> 1
	        fused_dequantize_op_2: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None
	        permute_13: "bf16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None
	        permute_14: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_13, [1, 0])
	        convert_element_type_25: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None
	        convert_element_type_default_3: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_94, torch.float16)
	        permute_15: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None
	        view_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_default_3, [primals_1, 8192]);  convert_element_type_default_3 = None
	        mm_6: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None
	        view_13: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_29: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None
	        permute_16: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None
	        view_14: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None
	        mm_7: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_14, permute_16)
	        view_15: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None
	        convert_element_type_32: "f16[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None
	        permute_17: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None
	        view_16: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None
	        mm_8: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_16, permute_17)
	        view_17: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None
	        mul_137: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_131: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_13, mul_137);  view_13 = mul_137 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_20: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None
	        permute_24: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_45: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.prims.convert_element_type.default(permute_13, torch.float16);  permute_13 = None
	        permute_27: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_30: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None
	        permute_34: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_60: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_7, torch.float16);  permute_7 = None
	        permute_37: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_41: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None
	        permute_45: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_75: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_1, torch.float16);  permute_1 = None
	        permute_48: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None
	        return (add_131, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, primals_1)
	        
V0326 23:29:17.711000 324469 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "2959bc24a322398834c0f5283239895e"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "Sym(s0)", view_2: "f16[s0, 2048][2048, 1]cuda:0", view_4: "f16[s0, 32][32, 1]cuda:0", add_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", view_10: "f16[s0, 32][32, 1]cuda:0", add_85: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", view_14: "f16[s0, 8192][8192, 1]cuda:0", view_16: "f16[s0, 32][32, 1]cuda:0", permute_20: "f16[2048, 32][32, 1]cuda:0", permute_24: "f16[32, 8192][8192, 1]cuda:0", permute_27: "f16[2048, 8192][8192, 1]cuda:0", permute_30: "f16[8192, 32][32, 1]cuda:0", permute_34: "f16[32, 2048][2048, 1]cuda:0", permute_37: "f16[8192, 2048][2048, 1]cuda:0", permute_41: "f16[8192, 32][32, 1]cuda:0", permute_45: "f16[32, 2048][2048, 1]cuda:0", permute_48: "f16[8192, 2048][2048, 1]cuda:0", tangents_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"):
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_144: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, 2.0)
	        view_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None
	        permute_18: "f16[2048, s0][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_18, [1, 0])
	        mm_9: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None
	        permute_19: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None
	        mm_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None
	        view_19: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None
	        permute_21: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None
	        convert_element_type_39: "f32[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None
	        view_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None
	        permute_22: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_20, [1, 0])
	        mm_11: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None
	        permute_23: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None
	        mm_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None
	        view_21: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None
	        permute_25: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None
	        convert_element_type_44: "f32[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None
	        mm_13: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None
	        view_23: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_default_2: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_23, torch.float16);  view_23 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_135: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_21, convert_element_type_default_2);  view_21 = convert_element_type_default_2 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        convert_element_type_11: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        mul_146: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None
	        mul_147: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_148: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_146, 2.0)
	        view_24: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None
	        permute_28: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_24, [1, 0])
	        mm_14: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None
	        permute_29: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None
	        mm_15: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None
	        view_25: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None
	        permute_31: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None
	        convert_element_type_54: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None
	        view_26: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None
	        permute_32: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_26, [1, 0])
	        mm_16: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_32, view_2);  permute_32 = None
	        permute_33: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None
	        mm_17: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None
	        view_27: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None
	        permute_35: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None
	        convert_element_type_59: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_28: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None
	        mm_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None
	        view_29: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_default_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_29, torch.float16);  view_29 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_136: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_27, convert_element_type_default_1);  view_27 = convert_element_type_default_1 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        sigmoid_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(add_39)
	        full_default: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_44: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_default, sigmoid_1);  full_default = None
	        mul_150: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None
	        add_137: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None
	        mul_151: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None
	        mul_152: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_153: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_152, 2.0)
	        view_30: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None
	        permute_39: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_30, [1, 0])
	        mm_19: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None
	        permute_40: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None
	        mm_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None
	        view_31: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None
	        permute_42: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None
	        convert_element_type_69: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None
	        view_32: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None
	        permute_43: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_32, [1, 0])
	        mm_21: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None
	        permute_44: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None
	        mm_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None
	        view_33: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        add_138: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_46: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None
	        convert_element_type_74: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_34: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None
	        mm_23: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None
	        view_35: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_default: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_35, torch.float16);  view_35 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_139: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_138, convert_element_type_default);  add_138 = convert_element_type_default = None
	        return (None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39)
	        
V0326 23:29:17.712000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "56b7a9b54648908b22483c608ed2ffc7"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992157711989.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:17.712000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "7a49a18abcc40cf6d44437c3f7fe88ec"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992157712448.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:17.712000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "c30f16a37dd8627a3bc4d90b0a36e130"}
	{
	"name": "inductor_compile",
	"ts": 1742992157712448.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:17.729000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:742] {"artifact": {"name": "fx_graph_runnable", "encoding": "string"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "c0f22b5dae5e10d3af66dc1d808043a5"}
	
	import torch
	from torch import tensor, device
	import torch.fx as fx
	from torch._dynamo.testing import rand_strided
	from math import inf
	import torch._inductor.inductor_prims
	
	import torch._dynamo.config
	import torch._inductor.config
	import torch._functorch.config
	import torch.fx.experimental._config
	torch._dynamo.config.verbose = True
	torch._dynamo.config.assume_static_by_default = False
	torch._inductor.config.debug = True
	torch._inductor.config.max_autotune = True
	torch._inductor.config.trace.enabled = True
	torch._functorch.config.unlift_effect_tokens = True
	
	
	
	isolate_fails_code_str = None
	
	
	
	# torch version: 2.5.1+cu124
	# torch cuda version: 12.4
	# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
	
	
	# CUDA Info: 
	# nvcc: NVIDIA (R) Cuda compiler driver 
	# Copyright (c) 2005-2024 NVIDIA Corporation 
	# Built on Tue_Oct_29_23:50:19_PDT_2024 
	# Cuda compilation tools, release 12.6, V12.6.85 
	# Build cuda_12.6.r12.6/compiler.35059454_0 
	
	# GPU Hardware Info: 
	# NVIDIA GeForce RTX 3060 : 1 
	
	
	from torch.nn import *
	class Repro(torch.nn.Module):
	    def __init__(self) -> None:
	        super().__init__()
	
	    
	    
	    def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38):
	        permute = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None
	        rshift = primals_5 >> 1
	        mul_4 = primals_4 * primals_5;  primals_5 = None
	        rshift_1 = mul_4 >> 1
	        fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None
	        permute_1 = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None
	        permute_2 = torch.ops.aten.permute.default(permute_1, [1, 0])
	        convert_element_type_1 = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None
	        convert_element_type_default_5 = torch.ops.prims.convert_element_type.default(primals_2, torch.float16)
	        permute_3 = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None
	        view = torch.ops.aten.view.default(convert_element_type_default_5, [primals_1, 2048]);  convert_element_type_default_5 = None
	        mm = torch.ops.aten.mm.default(view, permute_3);  permute_3 = None
	        view_1 = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None
	        convert_element_type_5 = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None
	        permute_4 = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None
	        view_2 = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None
	        mm_1 = torch.ops.aten.mm.default(view_2, permute_4)
	        view_3 = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None
	        convert_element_type_8 = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None
	        permute_5 = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None
	        view_4 = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None
	        mm_2 = torch.ops.aten.mm.default(view_4, permute_5)
	        view_5 = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None
	        mul_39 = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None
	        add_39 = torch.ops.aten.add.Tensor(view_1, mul_39);  view_1 = mul_39 = None
	        convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        permute_6 = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None
	        rshift_2 = primals_17 >> 1
	        mul_54 = primals_16 * primals_17;  primals_17 = None
	        rshift_3 = mul_54 >> 1
	        fused_dequantize_op_1 = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None
	        permute_7 = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None
	        permute_8 = torch.ops.aten.permute.default(permute_7, [1, 0])
	        convert_element_type_14 = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None
	        permute_9 = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None
	        mm_3 = torch.ops.aten.mm.default(view, permute_9);  view = permute_9 = None
	        view_7 = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None
	        convert_element_type_18 = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None
	        permute_10 = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None
	        mm_4 = torch.ops.aten.mm.default(view_2, permute_10)
	        view_9 = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None
	        convert_element_type_21 = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None
	        permute_11 = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None
	        view_10 = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None
	        mm_5 = torch.ops.aten.mm.default(view_10, permute_11)
	        view_11 = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None
	        mul_87 = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None
	        add_85 = torch.ops.aten.add.Tensor(view_7, mul_87);  view_7 = mul_87 = None
	        mul_94 = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85);  convert_element_type_12 = None
	        permute_12 = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None
	        rshift_4 = primals_29 >> 1
	        mul_102 = primals_28 * primals_29;  primals_29 = None
	        rshift_5 = mul_102 >> 1
	        fused_dequantize_op_2 = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None
	        permute_13 = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None
	        permute_14 = torch.ops.aten.permute.default(permute_13, [1, 0])
	        convert_element_type_25 = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None
	        convert_element_type_default_3 = torch.ops.prims.convert_element_type.default(mul_94, torch.float16)
	        permute_15 = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None
	        view_12 = torch.ops.aten.view.default(convert_element_type_default_3, [primals_1, 8192]);  convert_element_type_default_3 = None
	        mm_6 = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None
	        view_13 = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None
	        convert_element_type_29 = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None
	        permute_16 = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None
	        view_14 = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None
	        mm_7 = torch.ops.aten.mm.default(view_14, permute_16)
	        view_15 = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None
	        convert_element_type_32 = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None
	        permute_17 = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None
	        view_16 = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None
	        mm_8 = torch.ops.aten.mm.default(view_16, permute_17)
	        view_17 = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None
	        mul_137 = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None
	        add_131 = torch.ops.aten.add.Tensor(view_13, mul_137);  view_13 = mul_137 = None
	        permute_20 = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None
	        permute_24 = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None
	        convert_element_type_45 = torch.ops.prims.convert_element_type.default(permute_13, torch.float16);  permute_13 = None
	        permute_27 = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None
	        permute_30 = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None
	        permute_34 = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None
	        convert_element_type_60 = torch.ops.prims.convert_element_type.default(permute_7, torch.float16);  permute_7 = None
	        permute_37 = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None
	        permute_41 = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None
	        permute_45 = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None
	        convert_element_type_75 = torch.ops.prims.convert_element_type.default(permute_1, torch.float16);  permute_1 = None
	        permute_48 = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None
	        return (add_131, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, primals_1)
	        
	def load_args(reader):
	    reader.symint(100)  # primals_1
	    buf0 = reader.storage(None, 4096*s0, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf0, (1, s0, 2048), dtype=torch.float16, is_leaf=True)  # primals_2
	    buf1 = reader.storage(None, 8388608, device=device(type='cuda', index=0), dtype_hint=torch.uint8)
	    reader.tensor(buf1, (8388608, 1), dtype=torch.uint8, is_leaf=True)  # primals_3
	    reader.symint(256)  # primals_4
	    reader.symint(64)  # primals_5
	    buf2 = reader.storage(None, 262144, device=device(type='cuda', index=0), dtype_hint=torch.uint8)
	    reader.tensor(buf2, (262144,), dtype=torch.uint8, is_leaf=True)  # primals_6
	    buf3 = reader.storage(None, 1024, device=device(type='cuda', index=0))
	    reader.tensor(buf3, (256,), is_leaf=True)  # primals_7
	    buf4 = reader.storage(None, 4096, device=device(type='cuda', index=0))
	    reader.tensor(buf4, (1024,), is_leaf=True)  # primals_8
	    buf5 = reader.storage(None, 2, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf5, (), dtype=torch.float16, is_leaf=True)  # primals_9
	    buf6 = reader.storage(None, 64, device=device(type='cuda', index=0))
	    reader.tensor(buf6, (16,), is_leaf=True)  # primals_10
	    reader.symint(2048)  # primals_11
	    reader.symint(8192)  # primals_12
	    buf7 = reader.storage(None, 262144, device=device(type='cuda', index=0))
	    reader.tensor(buf7, (32, 2048), is_leaf=True)  # primals_13
	    buf8 = reader.storage(None, 1048576, device=device(type='cuda', index=0))
	    reader.tensor(buf8, (8192, 32), is_leaf=True)  # primals_14
	    buf9 = reader.storage(None, 8388608, device=device(type='cuda', index=0), dtype_hint=torch.uint8)
	    reader.tensor(buf9, (8388608, 1), dtype=torch.uint8, is_leaf=True)  # primals_15
	    reader.symint(256)  # primals_16
	    reader.symint(64)  # primals_17
	    buf10 = reader.storage(None, 262144, device=device(type='cuda', index=0), dtype_hint=torch.uint8)
	    reader.tensor(buf10, (262144,), dtype=torch.uint8, is_leaf=True)  # primals_18
	    buf11 = reader.storage(None, 1024, device=device(type='cuda', index=0))
	    reader.tensor(buf11, (256,), is_leaf=True)  # primals_19
	    buf12 = reader.storage(None, 4096, device=device(type='cuda', index=0))
	    reader.tensor(buf12, (1024,), is_leaf=True)  # primals_20
	    buf13 = reader.storage(None, 2, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf13, (), dtype=torch.float16, is_leaf=True)  # primals_21
	    buf14 = reader.storage(None, 64, device=device(type='cuda', index=0))
	    reader.tensor(buf14, (16,), is_leaf=True)  # primals_22
	    reader.symint(2048)  # primals_23
	    reader.symint(8192)  # primals_24
	    buf15 = reader.storage(None, 262144, device=device(type='cuda', index=0))
	    reader.tensor(buf15, (32, 2048), is_leaf=True)  # primals_25
	    buf16 = reader.storage(None, 1048576, device=device(type='cuda', index=0))
	    reader.tensor(buf16, (8192, 32), is_leaf=True)  # primals_26
	    buf17 = reader.storage(None, 8388608, device=device(type='cuda', index=0), dtype_hint=torch.uint8)
	    reader.tensor(buf17, (8388608, 1), dtype=torch.uint8, is_leaf=True)  # primals_27
	    reader.symint(256)  # primals_28
	    reader.symint(64)  # primals_29
	    buf18 = reader.storage(None, 262144, device=device(type='cuda', index=0), dtype_hint=torch.uint8)
	    reader.tensor(buf18, (262144,), dtype=torch.uint8, is_leaf=True)  # primals_30
	    buf19 = reader.storage(None, 1024, device=device(type='cuda', index=0))
	    reader.tensor(buf19, (256,), is_leaf=True)  # primals_31
	    buf20 = reader.storage(None, 4096, device=device(type='cuda', index=0))
	    reader.tensor(buf20, (1024,), is_leaf=True)  # primals_32
	    buf21 = reader.storage(None, 2, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf21, (), dtype=torch.float16, is_leaf=True)  # primals_33
	    buf22 = reader.storage(None, 64, device=device(type='cuda', index=0))
	    reader.tensor(buf22, (16,), is_leaf=True)  # primals_34
	    reader.symint(8192)  # primals_35
	    reader.symint(2048)  # primals_36
	    buf23 = reader.storage(None, 1048576, device=device(type='cuda', index=0))
	    reader.tensor(buf23, (32, 8192), is_leaf=True)  # primals_37
	    buf24 = reader.storage(None, 262144, device=device(type='cuda', index=0))
	    reader.tensor(buf24, (2048, 32), is_leaf=True)  # primals_38
	load_args._version = 0
	mod = Repro()
	if __name__ == '__main__':
	    from torch._dynamo.repro.after_aot import run_repro
	    with torch.no_grad():
	        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # To run it separately, do 
	        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # mod(*args)
V0326 23:29:17.780000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:801] {"inductor_post_grad_graph": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "f80a1434855f8f4a1d49a5110027e5c1"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "Sym(s0)", primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", primals_3: "u8[8388608, 1][1, 1]cuda:0", primals_4: "Sym(s2)", primals_5: "Sym(s3)", primals_6: "u8[262144][1]cuda:0", primals_7: "f32[256][1]cuda:0", primals_8: "f32[1024][1]cuda:0", primals_9: "f16[][]cuda:0", primals_10: "f32[16][1]cuda:0", primals_11: "Sym(2048)", primals_12: "Sym(8192)", primals_13: "f32[32, 2048][2048, 1]cuda:0", primals_14: "f32[8192, 32][32, 1]cuda:0", primals_15: "u8[8388608, 1][1, 1]cuda:0", primals_16: "Sym(s6)", primals_17: "Sym(s7)", primals_18: "u8[262144][1]cuda:0", primals_19: "f32[256][1]cuda:0", primals_20: "f32[1024][1]cuda:0", primals_21: "f16[][]cuda:0", primals_22: "f32[16][1]cuda:0", primals_23: "Sym(2048)", primals_24: "Sym(8192)", primals_25: "f32[32, 2048][2048, 1]cuda:0", primals_26: "f32[8192, 32][32, 1]cuda:0", primals_27: "u8[8388608, 1][1, 1]cuda:0", primals_28: "Sym(s10)", primals_29: "Sym(s11)", primals_30: "u8[262144][1]cuda:0", primals_31: "f32[256][1]cuda:0", primals_32: "f32[1024][1]cuda:0", primals_33: "f16[][]cuda:0", primals_34: "f32[16][1]cuda:0", primals_35: "Sym(8192)", primals_36: "Sym(2048)", primals_37: "f32[32, 8192][8192, 1]cuda:0", primals_38: "f32[2048, 32][32, 1]cuda:0"):
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift: "Sym(floor(s3/2))" = primals_5 >> 1
	        mul_4: "Sym(s2*s3)" = primals_4 * primals_5;  primals_5 = None
	        rshift_1: "Sym(floor(s2*s3/2))" = mul_4 >> 1
	        fused_dequantize_op: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None
	        permute_1: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None
	        permute_2: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0])
	        convert_element_type_1: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None
	        permute_3: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None
	        view: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.reshape.default(primals_2, [primals_1, 2048])
	        mm: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view, permute_3);  permute_3 = None
	        view_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.reshape.default(mm, [1, primals_1, 8192]);  mm = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_5: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None
	        permute_4: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None
	        view_2: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.reshape.default(primals_2, [primals_1, 2048]);  primals_2 = None
	        mm_1: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_2, permute_4)
	        view_3: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.reshape.default(mm_1, [1, primals_1, 32]);  mm_1 = None
	        convert_element_type_8: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None
	        permute_5: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None
	        view_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.reshape.default(view_3, [primals_1, 32]);  view_3 = None
	        mm_2: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_4, permute_5)
	        view_5: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.reshape.default(mm_2, [1, primals_1, 8192]);  mm_2 = None
	        mul_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1, mul_39);  view_1 = mul_39 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        convert_element_type_11: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_6: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_2: "Sym(floor(s7/2))" = primals_17 >> 1
	        mul_54: "Sym(s6*s7)" = primals_16 * primals_17;  primals_17 = None
	        rshift_3: "Sym(floor(s6*s7/2))" = mul_54 >> 1
	        fused_dequantize_op_1: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None
	        permute_7: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None
	        permute_8: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0])
	        convert_element_type_14: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None
	        permute_9: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None
	        mm_3: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view, permute_9);  view = permute_9 = None
	        view_7: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.reshape.default(mm_3, [1, primals_1, 8192]);  mm_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_18: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None
	        permute_10: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None
	        mm_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_2, permute_10)
	        view_9: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.reshape.default(mm_4, [1, primals_1, 32]);  mm_4 = None
	        convert_element_type_21: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None
	        permute_11: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None
	        view_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.reshape.default(view_9, [primals_1, 32]);  view_9 = None
	        mm_5: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_10, permute_11)
	        view_11: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.reshape.default(mm_5, [1, primals_1, 8192]);  mm_5 = None
	        mul_87: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_85: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_7, mul_87);  view_7 = mul_87 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_94: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85);  convert_element_type_12 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_12: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_4: "Sym(floor(s11/2))" = primals_29 >> 1
	        mul_102: "Sym(s10*s11)" = primals_28 * primals_29;  primals_29 = None
	        rshift_5: "Sym(floor(s10*s11/2))" = mul_102 >> 1
	        fused_dequantize_op_2: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None
	        permute_13: "bf16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None
	        permute_14: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_13, [1, 0])
	        convert_element_type_25: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None
	        permute_15: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None
	        view_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.reshape.default(mul_94, [primals_1, 8192])
	        mm_6: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None
	        view_13: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.reshape.default(mm_6, [1, primals_1, 2048]);  mm_6 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_29: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None
	        permute_16: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None
	        view_14: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.reshape.default(mul_94, [primals_1, 8192]);  mul_94 = None
	        mm_7: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_14, permute_16)
	        view_15: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.reshape.default(mm_7, [1, primals_1, 32]);  mm_7 = None
	        convert_element_type_32: "f16[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None
	        permute_17: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None
	        view_16: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.reshape.default(view_15, [primals_1, 32]);  view_15 = None
	        mm_8: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_16, permute_17)
	        view_17: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.reshape.default(mm_8, [1, primals_1, 2048]);  mm_8 = None
	        mul_137: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_131: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_13, mul_137);  view_13 = mul_137 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_20: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None
	        permute_24: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_45: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.prims.convert_element_type.default(permute_13, torch.float16);  permute_13 = None
	        permute_27: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_30: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None
	        permute_34: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_60: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_7, torch.float16);  permute_7 = None
	        permute_37: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_41: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None
	        permute_45: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_75: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_1, torch.float16);  permute_1 = None
	        permute_48: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None
	        return (add_131, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, primals_1)
	        
V0326 23:29:17.783000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "4875aae540849a7b36cb78ee875f7d25"}
	{
	"name": "GraphLowering.run",
	"ts": 1742992157783870.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:17.810000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "e9783d2dc58bf92adb2662f578950ed0"}
	{
	"name": "GraphLowering.run",
	"ts": 1742992157810615.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:17.811000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "f2373acb69f1e060a94f7b80c1414252"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1742992157811124.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:17.811000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "5f7c860915a7067b560ddee33f35e7d3"}
	{
	"name": "code_gen",
	"ts": 1742992157811124.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:17.812000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "2221486c49bddf6f967ef46a66a6e25c"}
	{
	"name": "Scheduler.__init__",
	"ts": 1742992157812743.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:17.949000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "672b994dc287b24a5b8f43ed90395920"}
	{
	"name": "Scheduler.__init__",
	"ts": 1742992157949135.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:17.949000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "14e52366785529ea6f0222c098af6db9"}
	{
	"name": "Scheduler.codegen",
	"ts": 1742992157949484.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.010000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "44f8b7a0b53ce1a98e8553a4f1346496"}
	{
	"name": "Scheduler.codegen",
	"ts": 1742992158010299.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.010000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "098f939047c2053121b0d0bf41dd72cd"}
	{
	"name": "WrapperCodeGen.generate",
	"ts": 1742992158010653.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.013000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "addb1f49a60fd515f744ec897e53f587"}
	{
	"name": "WrapperCodeGen.generate",
	"ts": 1742992158013599.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.014000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/graph.py:1861] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/6t/c6ti5avby2m7pt3ut73u7cv6pskab7oqrj32flivbrhi5omzahyd.py"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "2cfff416bd40de843c972c9ab081a285"}
	# AOT ID: ['5_forward']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/tc/ctc3wd5ie7s2fjcr2f4u2qxws22nb2vss5um5266to6nceu4xt3o.py
	# Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [mylib.fused_dequantize_op]
	# Source node to ATen node mapping:
	#   autograd_function_apply => fused_dequantize_op
	# Graph fragment:
	#   %fused_dequantize_op : [num_users=1] = call_function[target=torch.ops.mylib.fused_dequantize_op.default](args = (%primals_6, %primals_7, %primals_8, %primals_9, %permute, %primals_10, 16777216, %primals_4, %rshift, %mul_4, %rshift_1, 8192, 2048, torch.bfloat16), kwargs = {})
	triton_poi_fused_fused_dequantize_op_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {2: 1}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=(2,))]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_fused_dequantize_op_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 1
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    tmp0 = tl.load(in_ptr0 + (0)).to(tl.float32)
	    tmp1 = tl.broadcast_to(tmp0, [XBLOCK])
	    tl.store(out_ptr0 + (tl.full([XBLOCK], 0, tl.int32)), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/4i/c4ighga7fod2ed36hdz67v2ofqmyk7mlh2kc24zdiinrkgsdjog7.py
	# Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	#   autograd_function_apply => convert_element_type_1
	# Graph fragment:
	#   %convert_element_type_1 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_2, torch.float16), kwargs = {})
	triton_poi_fused__to_copy_1 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[16777216], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*bf16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_1', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 16777216
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/dm/cdm5ebpcdjlaqnrcavlevwhnlhakhmbh235waumdxi2dlz4oqdc3.py
	# Topologically Sorted Source Nodes: [linear], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	#   linear => convert_element_type_5, permute_4
	# Graph fragment:
	#   %convert_element_type_5 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_13, torch.float16), kwargs = {})
	#   %permute_4 : [num_users=2] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_5, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_2 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[65536], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp32', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_2', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 65536
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/4v/c4vy7sd4hdenvrautochni2fe3ce3tkssjud74ei4t45g5jo4bxq.py
	# Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	#   linear_1 => convert_element_type_8, permute_5
	# Graph fragment:
	#   %convert_element_type_8 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_14, torch.float16), kwargs = {})
	#   %permute_5 : [num_users=2] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_8, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_3 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp32', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_3', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 262144
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/5b/c5bxmvhk7qwah3r5zprbeosehsg4smud74ljqbertg7zky4hs4tq.py
	# Topologically Sorted Source Nodes: [output, result_2, silu, output_1, result_5, mul_2], Original ATen: [aten.mul, aten.add, aten.silu]
	# Source node to ATen node mapping:
	#   mul_2 => mul_94
	#   output => mul_39
	#   output_1 => mul_87
	#   result_2 => add_39
	#   result_5 => add_85
	#   silu => convert_element_type_11, convert_element_type_12, mul_46, sigmoid
	# Graph fragment:
	#   %mul_39 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_5, 2.0), kwargs = {})
	#   %add_39 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_1, %mul_39), kwargs = {})
	#   %convert_element_type_11 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_39, torch.float32), kwargs = {})
	#   %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_11,), kwargs = {})
	#   %mul_46 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_11, %sigmoid), kwargs = {})
	#   %convert_element_type_12 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_46, torch.float16), kwargs = {})
	#   %mul_87 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_11, 2.0), kwargs = {})
	#   %add_85 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_7, %mul_87), kwargs = {})
	#   %mul_94 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_12, %add_85), kwargs = {})
	triton_poi_fused_add_mul_silu_4 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1048576], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: '*fp16', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mul_silu_4', 'mutated_arg_names': ['in_out_ptr0', 'in_out_ptr1'], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_out_ptr0, in_out_ptr1, in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_out_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp5 = tl.load(in_out_ptr1 + (x0), None).to(tl.float32)
	    tmp6 = tl.load(in_ptr1 + (x0), None).to(tl.float32)
	    tmp2 = 2.0
	    tmp3 = tmp1 * tmp2
	    tmp4 = tmp0 + tmp3
	    tmp7 = tmp6 * tmp2
	    tmp8 = tmp5 + tmp7
	    tmp9 = tmp4.to(tl.float32)
	    tmp10 = tl.sigmoid(tmp9)
	    tmp11 = tmp9 * tmp10
	    tmp12 = tmp11.to(tl.float32)
	    tmp13 = tmp12 * tmp8
	    tl.store(in_out_ptr0 + (x0), tmp4, None)
	    tl.store(in_out_ptr1 + (x0), tmp8, None)
	    tl.store(out_ptr0 + (x0), tmp13, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/3b/c3bktvkn3n6hu4v7tiwk7rpos2wp4lajxxzrdgoicggbyxk5d3cb.py
	# Topologically Sorted Source Nodes: [output_2, result_8], Original ATen: [aten.mul, aten.add]
	# Source node to ATen node mapping:
	#   output_2 => mul_137
	#   result_8 => add_131
	# Graph fragment:
	#   %mul_137 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_17, 2.0), kwargs = {})
	#   %add_131 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_13, %mul_137), kwargs = {})
	triton_poi_fused_add_mul_5 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mul_5', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_out_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp2 = 2.0
	    tmp3 = tmp1 * tmp2
	    tmp4 = tmp0 + tmp3
	    tl.store(in_out_ptr0 + (x0), tmp4, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/j7/cj776okodjzzoh7cfozhudvywfzftzc6cabab2mmcdjjege6gbpi.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_45 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_13, torch.float16), kwargs = {})
	triton_poi_fused__to_copy_6 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[8192, 2048], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*bf16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_6', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 8192
	    xnumel = 2048
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (8192*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (2048*y0)), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/mi/cmiz6hpfs7acuhgi57mdp7dpphsyd43zzboziig5wum6ghgma4e2.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_45 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_13, torch.float16), kwargs = {})
	#   %permute_27 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_45, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_7 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[2048, 8192], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_7', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 2048
	    xnumel = 8192
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (2048*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (8192*y0)), tmp0, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/vd/cvds2jaf5sirynr2qnt7rpujbamyslbkrjowetcdyb6ep4yhq4gq.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_60 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_7, torch.float16), kwargs = {})
	triton_poi_fused__to_copy_8 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[2048, 8192], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*bf16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_8', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 2048
	    xnumel = 8192
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (2048*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (8192*y0)), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/ai/caikh3jmq5xv3uwakclrkwfn3zmg36elfcn5skhch7uvaivem3uj.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_60 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_7, torch.float16), kwargs = {})
	#   %permute_37 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_60, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_9 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[8192, 2048], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_9', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 8192
	    xnumel = 2048
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (8192*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (2048*y0)), tmp0, None)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38 = args
	    args.clear()
	    s0 = primals_1
	    s2 = primals_4
	    s3 = primals_5
	    s6 = primals_16
	    s7 = primals_17
	    s10 = primals_28
	    s11 = primals_29
	    assert_size_stride(primals_2, (1, s0, 2048), (2048*s0, 2048, 1))
	    assert_size_stride(primals_3, (8388608, 1), (1, 1))
	    assert_size_stride(primals_6, (262144, ), (1, ))
	    assert_size_stride(primals_7, (256, ), (1, ))
	    assert_size_stride(primals_8, (1024, ), (1, ))
	    assert_size_stride(primals_9, (), ())
	    assert_size_stride(primals_10, (16, ), (1, ))
	    assert_size_stride(primals_13, (32, 2048), (2048, 1))
	    assert_size_stride(primals_14, (8192, 32), (32, 1))
	    assert_size_stride(primals_15, (8388608, 1), (1, 1))
	    assert_size_stride(primals_18, (262144, ), (1, ))
	    assert_size_stride(primals_19, (256, ), (1, ))
	    assert_size_stride(primals_20, (1024, ), (1, ))
	    assert_size_stride(primals_21, (), ())
	    assert_size_stride(primals_22, (16, ), (1, ))
	    assert_size_stride(primals_25, (32, 2048), (2048, 1))
	    assert_size_stride(primals_26, (8192, 32), (32, 1))
	    assert_size_stride(primals_27, (8388608, 1), (1, 1))
	    assert_size_stride(primals_30, (262144, ), (1, ))
	    assert_size_stride(primals_31, (256, ), (1, ))
	    assert_size_stride(primals_32, (1024, ), (1, ))
	    assert_size_stride(primals_33, (), ())
	    assert_size_stride(primals_34, (16, ), (1, ))
	    assert_size_stride(primals_37, (32, 8192), (8192, 1))
	    assert_size_stride(primals_38, (2048, 32), (32, 1))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((), (), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [mylib.fused_dequantize_op]
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_fused_dequantize_op_0.run(primals_9, buf0, 1, grid=grid(1), stream=stream0)
	        del primals_9
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [mylib.fused_dequantize_op]
	        buf1 = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, buf0, reinterpret_tensor(primals_3, (1, 8388608), (1, 1), 0), primals_10, 16777216, s2, math.floor((1/2)*s3), s2*s3, math.floor((1/2)*s2*s3), 8192, 2048, torch.bfloat16)
	        del primals_10
	        del primals_3
	        del primals_6
	        del primals_7
	        del primals_8
	        buf2 = buf1
	        del buf1
	        buf3 = empty_strided_cuda((8192, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf2, buf3, 16777216, grid=grid(16777216), stream=stream0)
	        buf4 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), reinterpret_tensor(buf3, (2048, 8192), (1, 2048), 0), out=buf4)
	        buf5 = empty_strided_cuda((2048, 32), (1, 2048), torch.float16)
	        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_2.run(primals_13, buf5, 65536, grid=grid(65536), stream=stream0)
	        del primals_13
	        buf6 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), buf5, out=buf6)
	        buf7 = empty_strided_cuda((32, 8192), (1, 32), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_3.run(primals_14, buf7, 262144, grid=grid(262144), stream=stream0)
	        del primals_14
	        buf8 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten.mm]
	        extern_kernels.mm(buf6, buf7, out=buf8)
	        buf10 = buf0; del buf0  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [mylib.fused_dequantize_op]
	        triton_poi_fused_fused_dequantize_op_0.run(primals_21, buf10, 1, grid=grid(1), stream=stream0)
	        del primals_21
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [mylib.fused_dequantize_op]
	        buf11 = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, buf10, reinterpret_tensor(primals_15, (1, 8388608), (1, 1), 0), primals_22, 16777216, s6, math.floor((1/2)*s7), s6*s7, math.floor((1/2)*s6*s7), 8192, 2048, torch.bfloat16)
	        del primals_15
	        del primals_18
	        del primals_19
	        del primals_20
	        del primals_22
	        buf12 = buf11
	        del buf11
	        buf13 = buf3; del buf3  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf12, buf13, 16777216, grid=grid(16777216), stream=stream0)
	        buf14 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), reinterpret_tensor(buf13, (2048, 8192), (1, 2048), 0), out=buf14)
	        buf15 = empty_strided_cuda((2048, 32), (1, 2048), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_2], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_2.run(primals_25, buf15, 65536, grid=grid(65536), stream=stream0)
	        del primals_25
	        buf16 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_2], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), buf15, out=buf16)
	        buf17 = empty_strided_cuda((32, 8192), (1, 32), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_3], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_3.run(primals_26, buf17, 262144, grid=grid(262144), stream=stream0)
	        del primals_26
	        buf18 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_3], Original ATen: [aten.mm]
	        extern_kernels.mm(buf16, buf17, out=buf18)
	        buf9 = reinterpret_tensor(buf4, (1, s0, 8192), (8192*s0, 8192, 1), 0); del buf4  # reuse
	        buf19 = reinterpret_tensor(buf14, (1, s0, 8192), (8192*s0, 8192, 1), 0); del buf14  # reuse
	        buf23 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [output, result_2, silu, output_1, result_5, mul_2], Original ATen: [aten.mul, aten.add, aten.silu]
	        triton_poi_fused_add_mul_silu_4_xnumel = 8192*s0
	        triton_poi_fused_add_mul_silu_4.run(buf9, buf19, buf8, buf18, buf23, triton_poi_fused_add_mul_silu_4_xnumel, grid=grid(triton_poi_fused_add_mul_silu_4_xnumel), stream=stream0)
	        del buf18
	        del buf8
	        buf20 = buf10; del buf10  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [mylib.fused_dequantize_op]
	        triton_poi_fused_fused_dequantize_op_0.run(primals_33, buf20, 1, grid=grid(1), stream=stream0)
	        del primals_33
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [mylib.fused_dequantize_op]
	        buf21 = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, buf20, reinterpret_tensor(primals_27, (1, 8388608), (1, 1), 0), primals_34, 16777216, s10, math.floor((1/2)*s11), s10*s11, math.floor((1/2)*s10*s11), 2048, 8192, torch.bfloat16)
	        del buf20
	        del primals_27
	        del primals_30
	        del primals_31
	        del primals_32
	        del primals_34
	        buf22 = buf21
	        del buf21
	        buf24 = reinterpret_tensor(buf13, (2048, 8192), (8192, 1), 0); del buf13  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf22, buf24, 16777216, grid=grid(16777216), stream=stream0)
	        buf25 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf23, (s0, 8192), (8192, 1), 0), reinterpret_tensor(buf24, (8192, 2048), (1, 8192), 0), out=buf25)
	        buf26 = empty_strided_cuda((8192, 32), (1, 8192), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_3.run(primals_37, buf26, 262144, grid=grid(262144), stream=stream0)
	        del primals_37
	        buf27 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf23, (s0, 8192), (8192, 1), 0), buf26, out=buf27)
	        buf28 = empty_strided_cuda((32, 2048), (1, 32), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_5], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_2.run(primals_38, buf28, 65536, grid=grid(65536), stream=stream0)
	        del primals_38
	        buf29 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_5], Original ATen: [aten.mm]
	        extern_kernels.mm(buf27, buf28, out=buf29)
	        buf30 = reinterpret_tensor(buf25, (1, s0, 2048), (2048*s0, 2048, 1), 0); del buf25  # reuse
	        # Topologically Sorted Source Nodes: [output_2, result_8], Original ATen: [aten.mul, aten.add]
	        triton_poi_fused_add_mul_5_xnumel = 2048*s0
	        triton_poi_fused_add_mul_5.run(buf30, buf29, triton_poi_fused_add_mul_5_xnumel, grid=grid(triton_poi_fused_add_mul_5_xnumel), stream=stream0)
	        del buf29
	        buf31 = reinterpret_tensor(buf24, (8192, 2048), (2048, 1), 0); del buf24  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_6.run(buf22, buf31, 8192, 2048, grid=grid(8192, 2048), stream=stream0)
	        del buf22
	        buf32 = empty_strided_cuda((2048, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_7.run(buf31, buf32, 2048, 8192, grid=grid(2048, 8192), stream=stream0)
	        buf33 = reinterpret_tensor(buf31, (2048, 8192), (8192, 1), 0); del buf31  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_8.run(buf12, buf33, 2048, 8192, grid=grid(2048, 8192), stream=stream0)
	        del buf12
	        buf34 = empty_strided_cuda((8192, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_9.run(buf33, buf34, 8192, 2048, grid=grid(8192, 2048), stream=stream0)
	        buf35 = buf33; del buf33  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_8.run(buf2, buf35, 2048, 8192, grid=grid(2048, 8192), stream=stream0)
	        del buf2
	        buf36 = empty_strided_cuda((8192, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_9.run(buf35, buf36, 8192, 2048, grid=grid(8192, 2048), stream=stream0)
	        del buf35
	    return (buf30, reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), buf6, buf9, buf16, buf19, reinterpret_tensor(buf23, (s0, 8192), (8192, 1), 0), buf27, reinterpret_tensor(buf28, (2048, 32), (32, 1), 0), reinterpret_tensor(buf26, (32, 8192), (8192, 1), 0), buf32, reinterpret_tensor(buf17, (8192, 32), (32, 1), 0), reinterpret_tensor(buf15, (32, 2048), (2048, 1), 0), buf34, reinterpret_tensor(buf7, (8192, 32), (32, 1), 0), reinterpret_tensor(buf5, (32, 2048), (2048, 1), 0), buf36, s0, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    primals_1 = 100
	    primals_2 = rand_strided((1, 100, 2048), (204800, 2048, 1), device='cuda:0', dtype=torch.float16)
	    primals_3 = rand_strided((8388608, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    primals_4 = 256
	    primals_5 = 64
	    primals_6 = rand_strided((262144, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    primals_7 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_8 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_9 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    primals_10 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_11 = 2048
	    primals_12 = 8192
	    primals_13 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)
	    primals_14 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float32)
	    primals_15 = rand_strided((8388608, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    primals_16 = 256
	    primals_17 = 64
	    primals_18 = rand_strided((262144, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    primals_19 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_20 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_21 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    primals_22 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_23 = 2048
	    primals_24 = 8192
	    primals_25 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)
	    primals_26 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float32)
	    primals_27 = rand_strided((8388608, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    primals_28 = 256
	    primals_29 = 64
	    primals_30 = rand_strided((262144, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    primals_31 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_32 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_33 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    primals_34 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_35 = 8192
	    primals_36 = 2048
	    primals_37 = rand_strided((32, 8192), (8192, 1), device='cuda:0', dtype=torch.float32)
	    primals_38 = rand_strided((2048, 32), (32, 1), device='cuda:0', dtype=torch.float32)
	    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:29:18.405000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "1a014a8cf7658f0ca2a76e857463d74c"}
	{
	"name": "code_gen",
	"ts": 1742992158404946.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.405000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "acd96c405a321c2836b93dff2085d430"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1742992158405297.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.416000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "f0c798824d54b7b5e08d8c4ce398a54a"}
	{
	"name": "fx_graph_cache_miss",
	"ts": 1742992157726339.8,
	"args": {
	"key": "fibzwaj5j2pxd4mgs6jpctih67rusxg2lyckoxqaw3hr5wi42ks3",
	"components": [
	"[iu3kgzgo5x6udk6uhcxvnilg7lheh3rln2fhyjpg5qgapvm76o4] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38):\n    permute = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None\n    rshift = primals_5 >> 1\n    mul_4 = primals_4 * primals_5;  primals_5 = None\n    rshift_1 = mul_4 >> 1\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None\n    permute_1 = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None\n    permute_2 = torch.ops.aten.permute.default(permute_1, [1, 0])\n    convert_element_type_1 = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None\n    convert_element_type_default_5 = torch.ops.prims.convert_element_type.default(primals_2, torch.float16)\n    permute_3 = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None\n    view = torch.ops.aten.view.default(convert_element_type_default_5, [primals_1, 2048]);  convert_element_type_default_5 = None\n    mm = torch.ops.aten.mm.default(view, permute_3);  permute_3 = None\n    view_1 = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None\n    convert_element_type_5 = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None\n    permute_4 = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None\n    view_2 = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None\n    mm_1 = torch.ops.aten.mm.default(view_2, permute_4)\n    view_3 = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None\n    convert_element_type_8 = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None\n    permute_5 = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None\n    view_4 = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None\n    mm_2 = torch.ops.aten.mm.default(view_4, permute_5)\n    view_5 = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None\n    mul_39 = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None\n    add_39 = torch.ops.aten.add.Tensor(view_1, mul_39);  view_1 = mul_39 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    permute_6 = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None\n    rshift_2 = primals_17 >> 1\n    mul_54 = primals_16 * primals_17;  primals_17 = None\n    rshift_3 = mul_54 >> 1\n    fused_dequantize_op_1 = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None\n    permute_7 = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None\n    permute_8 = torch.ops.aten.permute.default(permute_7, [1, 0])\n    convert_element_type_14 = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None\n    permute_9 = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None\n    mm_3 = torch.ops.aten.mm.default(view, permute_9);  view = permute_9 = None\n    view_7 = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None\n    convert_element_type_18 = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None\n    permute_10 = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None\n    mm_4 = torch.ops.aten.mm.default(view_2, permute_10)\n    view_9 = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None\n    convert_element_type_21 = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None\n    permute_11 = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None\n    view_10 = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None\n    mm_5 = torch.ops.aten.mm.default(view_10, permute_11)\n    view_11 = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None\n    mul_87 = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None\n    add_85 = torch.ops.aten.add.Tensor(view_7, mul_87);  view_7 = mul_87 = None\n    mul_94 = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85);  convert_element_type_12 = None\n    permute_12 = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None\n    rshift_4 = primals_29 >> 1\n    mul_102 = primals_28 * primals_29;  primals_29 = None\n    rshift_5 = mul_102 >> 1\n    fused_dequantize_op_2 = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None\n    permute_13 = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None\n    permute_14 = torch.ops.aten.permute.default(permute_13, [1, 0])\n    convert_element_type_25 = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None\n    convert_element_type_default_3 = torch.ops.prims.convert_element_type.default(mul_94, torch.float16)\n    permute_15 = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None\n    view_12 = torch.ops.aten.view.default(convert_element_type_default_3, [primals_1, 8192]);  convert_element_type_default_3 = None\n    mm_6 = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None\n    view_13 = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None\n    convert_element_type_29 = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None\n    permute_16 = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None\n    view_14 = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None\n    mm_7 = torch.ops.aten.mm.default(view_14, permute_16)\n    view_15 = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None\n    convert_element_type_32 = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None\n    permute_17 = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None\n    view_16 = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None\n    mm_8 = torch.ops.aten.mm.default(view_16, permute_17)\n    view_17 = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None\n    mul_137 = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None\n    add_131 = torch.ops.aten.add.Tensor(view_13, mul_137);  view_13 = mul_137 = None\n    permute_20 = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None\n    permute_24 = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None\n    convert_element_type_45 = torch.ops.prims.convert_element_type.default(permute_13, torch.float16);  permute_13 = None\n    permute_27 = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None\n    permute_30 = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None\n    permute_34 = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None\n    convert_element_type_60 = torch.ops.prims.convert_element_type.default(permute_7, torch.float16);  permute_7 = None\n    permute_37 = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None\n    permute_41 = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None\n    permute_45 = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None\n    convert_element_type_75 = torch.ops.prims.convert_element_type.default(permute_1, torch.float16);  permute_1 = None\n    permute_48 = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None\n    return (add_131, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, primals_1)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[2]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[3]: ('s2',)",
	"[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[4]: ('s3',)",
	"[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[5]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[7]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[9]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[10]: ('2048',)",
	"[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[11]: ('8192',)",
	"[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[12]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[13]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[14]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[bp5m6l3n24hn6zbxwmg6wfbnn5g5tujtrpje3hczu5umrrf2qjx] example_inputs[15]: ('s6',)",
	"[i3xrsw5sisbji2ivhzqy3l7zuqzhmvjpsyt77nanlhezk7f6tur] example_inputs[16]: ('s7',)",
	"[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[17]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[18]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[19]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[20]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[21]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[22]: ('2048',)",
	"[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[23]: ('8192',)",
	"[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[24]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[25]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[26]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hj7nkho4e2pgg3rxjgt2xo2ofzqotfhatuhlt6bxcifwqbvipzw] example_inputs[27]: ('s10',)",
	"[kosoqok7aznz5iahzbmrdsrqfq3jkju4dvvd7cfij3xrvzxbmb4] example_inputs[28]: ('s11',)",
	"[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[29]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[30]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[31]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[32]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[33]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[34]: ('8192',)",
	"[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[35]: ('2048',)",
	"[zs5vjd7lduxixuzsgnizmtcmiocu75spefi5z3zyshqbfmva6ge] example_inputs[36]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[chru3fsd44dmnozqmsiynvahv26l3bk7j747je77m6edbjsd2kq] example_inputs[37]: TensorMetadata(dtype=torch.float32, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[34jl2fvzyx6odxtpy45nz66wulvmgs5bvsvvhx2zwehki6ym5kw] fx_kwargs[static_input_idxs]: [2, 5, 6, 7, 8, 9, 12, 13, 14, 17, 18, 19, 20, 21, 24, 25, 26, 29, 30, 31, 32, 33, 36, 37]",
	"[2ruvd6wcktb7xtp52gxtldu3ygrqbnwbiluwcx3yagimk7erlhx] fx_kwargs[user_visible_outputs]: {'add_131': None}",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_taken_ns": 688660460,
	"cache_state": "miss"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:29:18.416000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "e4416f343edc2b84d5db952c59ed7af6"}
	{"key": "fibzwaj5j2pxd4mgs6jpctih67rusxg2lyckoxqaw3hr5wi42ks3", "components": ["[iu3kgzgo5x6udk6uhcxvnilg7lheh3rln2fhyjpg5qgapvm76o4] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38):\n    permute = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None\n    rshift = primals_5 >> 1\n    mul_4 = primals_4 * primals_5;  primals_5 = None\n    rshift_1 = mul_4 >> 1\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None\n    permute_1 = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None\n    permute_2 = torch.ops.aten.permute.default(permute_1, [1, 0])\n    convert_element_type_1 = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None\n    convert_element_type_default_5 = torch.ops.prims.convert_element_type.default(primals_2, torch.float16)\n    permute_3 = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None\n    view = torch.ops.aten.view.default(convert_element_type_default_5, [primals_1, 2048]);  convert_element_type_default_5 = None\n    mm = torch.ops.aten.mm.default(view, permute_3);  permute_3 = None\n    view_1 = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None\n    convert_element_type_5 = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None\n    permute_4 = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None\n    view_2 = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None\n    mm_1 = torch.ops.aten.mm.default(view_2, permute_4)\n    view_3 = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None\n    convert_element_type_8 = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None\n    permute_5 = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None\n    view_4 = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None\n    mm_2 = torch.ops.aten.mm.default(view_4, permute_5)\n    view_5 = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None\n    mul_39 = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None\n    add_39 = torch.ops.aten.add.Tensor(view_1, mul_39);  view_1 = mul_39 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    permute_6 = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None\n    rshift_2 = primals_17 >> 1\n    mul_54 = primals_16 * primals_17;  primals_17 = None\n    rshift_3 = mul_54 >> 1\n    fused_dequantize_op_1 = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None\n    permute_7 = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None\n    permute_8 = torch.ops.aten.permute.default(permute_7, [1, 0])\n    convert_element_type_14 = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None\n    permute_9 = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None\n    mm_3 = torch.ops.aten.mm.default(view, permute_9);  view = permute_9 = None\n    view_7 = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None\n    convert_element_type_18 = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None\n    permute_10 = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None\n    mm_4 = torch.ops.aten.mm.default(view_2, permute_10)\n    view_9 = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None\n    convert_element_type_21 = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None\n    permute_11 = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None\n    view_10 = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None\n    mm_5 = torch.ops.aten.mm.default(view_10, permute_11)\n    view_11 = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None\n    mul_87 = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None\n    add_85 = torch.ops.aten.add.Tensor(view_7, mul_87);  view_7 = mul_87 = None\n    mul_94 = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85);  convert_element_type_12 = None\n    permute_12 = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None\n    rshift_4 = primals_29 >> 1\n    mul_102 = primals_28 * primals_29;  primals_29 = None\n    rshift_5 = mul_102 >> 1\n    fused_dequantize_op_2 = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None\n    permute_13 = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None\n    permute_14 = torch.ops.aten.permute.default(permute_13, [1, 0])\n    convert_element_type_25 = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None\n    convert_element_type_default_3 = torch.ops.prims.convert_element_type.default(mul_94, torch.float16)\n    permute_15 = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None\n    view_12 = torch.ops.aten.view.default(convert_element_type_default_3, [primals_1, 8192]);  convert_element_type_default_3 = None\n    mm_6 = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None\n    view_13 = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None\n    convert_element_type_29 = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None\n    permute_16 = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None\n    view_14 = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None\n    mm_7 = torch.ops.aten.mm.default(view_14, permute_16)\n    view_15 = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None\n    convert_element_type_32 = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None\n    permute_17 = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None\n    view_16 = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None\n    mm_8 = torch.ops.aten.mm.default(view_16, permute_17)\n    view_17 = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None\n    mul_137 = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None\n    add_131 = torch.ops.aten.add.Tensor(view_13, mul_137);  view_13 = mul_137 = None\n    permute_20 = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None\n    permute_24 = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None\n    convert_element_type_45 = torch.ops.prims.convert_element_type.default(permute_13, torch.float16);  permute_13 = None\n    permute_27 = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None\n    permute_30 = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None\n    permute_34 = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None\n    convert_element_type_60 = torch.ops.prims.convert_element_type.default(permute_7, torch.float16);  permute_7 = None\n    permute_37 = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None\n    permute_41 = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None\n    permute_45 = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None\n    convert_element_type_75 = torch.ops.prims.convert_element_type.default(permute_1, torch.float16);  permute_1 = None\n    permute_48 = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None\n    return (add_131, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, primals_1)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[2]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[3]: ('s2',)", "[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[4]: ('s3',)", "[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[5]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[7]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[9]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[10]: ('2048',)", "[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[11]: ('8192',)", "[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[12]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[13]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[14]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[bp5m6l3n24hn6zbxwmg6wfbnn5g5tujtrpje3hczu5umrrf2qjx] example_inputs[15]: ('s6',)", "[i3xrsw5sisbji2ivhzqy3l7zuqzhmvjpsyt77nanlhezk7f6tur] example_inputs[16]: ('s7',)", "[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[17]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[18]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[19]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[20]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[21]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[22]: ('2048',)", "[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[23]: ('8192',)", "[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[24]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[25]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[26]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hj7nkho4e2pgg3rxjgt2xo2ofzqotfhatuhlt6bxcifwqbvipzw] example_inputs[27]: ('s10',)", "[kosoqok7aznz5iahzbmrdsrqfq3jkju4dvvd7cfij3xrvzxbmb4] example_inputs[28]: ('s11',)", "[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[29]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[30]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[31]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[32]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[33]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[34]: ('8192',)", "[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[35]: ('2048',)", "[zs5vjd7lduxixuzsgnizmtcmiocu75spefi5z3zyshqbfmva6ge] example_inputs[36]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[chru3fsd44dmnozqmsiynvahv26l3bk7j747je77m6edbjsd2kq] example_inputs[37]: TensorMetadata(dtype=torch.float32, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[34jl2fvzyx6odxtpy45nz66wulvmgs5bvsvvhx2zwehki6ym5kw] fx_kwargs[static_input_idxs]: [2, 5, 6, 7, 8, 9, 12, 13, 14, 17, 18, 19, 20, 21, 24, 25, 26, 29, 30, 31, 32, 33, 36, 37]", "[2ruvd6wcktb7xtp52gxtldu3ygrqbnwbiluwcx3yagimk7erlhx] fx_kwargs[user_visible_outputs]: {'add_131': None}", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_taken_ns": 688660460, "cache_state": "miss"}
V0326 23:29:18.418000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "57cb26528a78d687aa10612eab1ef424"}
	{
	"name": "inductor_compile",
	"ts": 1742992158418430.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.418000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "704c854ff743440a6b5ed75d75e31c01"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992158418776.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.419000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "3b2770042232fe0f5015be705676ad19"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992158419203.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.420000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "e7d7f3b04640dc88a370a8d6785bcabb"}
	{
	"name": "compile_fx.<locals>.bw_compiler",
	"ts": 1742992158419888.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.420000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "b4e41cf6a903ceb40bf72b5087daeccb"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992158420298.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.420000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "25e3ca93696cdb812202604a6c88f454"}
	{
	"name": "inductor_compile",
	"ts": 1742992158420298.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.433000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:742] {"artifact": {"name": "fx_graph_runnable", "encoding": "string"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "9b6c6ecf8169a74ebcc599241a8ccd0e"}
	
	import torch
	from torch import tensor, device
	import torch.fx as fx
	from torch._dynamo.testing import rand_strided
	from math import inf
	import torch._inductor.inductor_prims
	
	import torch._dynamo.config
	import torch._inductor.config
	import torch._functorch.config
	import torch.fx.experimental._config
	torch._dynamo.config.verbose = True
	torch._dynamo.config.assume_static_by_default = False
	torch._inductor.config.debug = True
	torch._inductor.config.max_autotune = True
	torch._inductor.config.trace.enabled = True
	torch._functorch.config.unlift_effect_tokens = True
	
	
	
	isolate_fails_code_str = None
	
	
	
	# torch version: 2.5.1+cu124
	# torch cuda version: 12.4
	# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
	
	
	# CUDA Info: 
	# nvcc: NVIDIA (R) Cuda compiler driver 
	# Copyright (c) 2005-2024 NVIDIA Corporation 
	# Built on Tue_Oct_29_23:50:19_PDT_2024 
	# Cuda compilation tools, release 12.6, V12.6.85 
	# Build cuda_12.6.r12.6/compiler.35059454_0 
	
	# GPU Hardware Info: 
	# NVIDIA GeForce RTX 3060 : 1 
	
	
	from torch.nn import *
	class Repro(torch.nn.Module):
	    def __init__(self) -> None:
	        super().__init__()
	
	    
	    
	    def forward(self, primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1):
	        mul_144 = torch.ops.aten.mul.Tensor(tangents_1, 2.0)
	        view_18 = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None
	        permute_18 = torch.ops.aten.permute.default(view_18, [1, 0])
	        mm_9 = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None
	        permute_19 = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None
	        mm_10 = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None
	        view_19 = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None
	        permute_21 = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None
	        convert_element_type_39 = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None
	        view_20 = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None
	        permute_22 = torch.ops.aten.permute.default(view_20, [1, 0])
	        mm_11 = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None
	        permute_23 = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None
	        mm_12 = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None
	        view_21 = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None
	        permute_25 = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None
	        convert_element_type_44 = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None
	        view_22 = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None
	        mm_13 = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None
	        view_23 = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None
	        convert_element_type_default_2 = torch.ops.prims.convert_element_type.default(view_23, torch.float16);  view_23 = None
	        add_135 = torch.ops.aten.add.Tensor(view_21, convert_element_type_default_2);  view_21 = convert_element_type_default_2 = None
	        convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        mul_146 = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None
	        mul_147 = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None
	        mul_148 = torch.ops.aten.mul.Tensor(mul_146, 2.0)
	        view_24 = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None
	        permute_28 = torch.ops.aten.permute.default(view_24, [1, 0])
	        mm_14 = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None
	        permute_29 = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None
	        mm_15 = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None
	        view_25 = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None
	        permute_31 = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None
	        convert_element_type_54 = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None
	        view_26 = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None
	        permute_32 = torch.ops.aten.permute.default(view_26, [1, 0])
	        mm_16 = torch.ops.aten.mm.default(permute_32, view_2);  permute_32 = None
	        permute_33 = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None
	        mm_17 = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None
	        view_27 = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None
	        permute_35 = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None
	        convert_element_type_59 = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None
	        view_28 = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None
	        mm_18 = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None
	        view_29 = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None
	        convert_element_type_default_1 = torch.ops.prims.convert_element_type.default(view_29, torch.float16);  view_29 = None
	        add_136 = torch.ops.aten.add.Tensor(view_27, convert_element_type_default_1);  view_27 = convert_element_type_default_1 = None
	        sigmoid_1 = torch.ops.aten.sigmoid.default(add_39)
	        full_default = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_44 = torch.ops.aten.sub.Tensor(full_default, sigmoid_1);  full_default = None
	        mul_150 = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None
	        add_137 = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None
	        mul_151 = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None
	        mul_152 = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None
	        mul_153 = torch.ops.aten.mul.Tensor(mul_152, 2.0)
	        view_30 = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None
	        permute_39 = torch.ops.aten.permute.default(view_30, [1, 0])
	        mm_19 = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None
	        permute_40 = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None
	        mm_20 = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None
	        view_31 = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None
	        permute_42 = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None
	        convert_element_type_69 = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None
	        view_32 = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None
	        permute_43 = torch.ops.aten.permute.default(view_32, [1, 0])
	        mm_21 = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None
	        permute_44 = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None
	        mm_22 = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None
	        view_33 = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None
	        add_138 = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None
	        permute_46 = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None
	        convert_element_type_74 = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None
	        view_34 = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None
	        mm_23 = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None
	        view_35 = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None
	        convert_element_type_default = torch.ops.prims.convert_element_type.default(view_35, torch.float16);  view_35 = None
	        add_139 = torch.ops.aten.add.Tensor(add_138, convert_element_type_default);  add_138 = convert_element_type_default = None
	        return (None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39)
	        
	def load_args(reader):
	    reader.symint(100)  # primals_1
	    buf0 = reader.storage(None, 4096*s0, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf0, (s0, 2048), dtype=torch.float16, is_leaf=True)  # view_2
	    buf1 = reader.storage(None, 64*s0, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf1, (s0, 32), dtype=torch.float16, is_leaf=True)  # view_4
	    buf2 = reader.storage(None, 16384*s0, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf2, (1, s0, 8192), dtype=torch.float16, is_leaf=True)  # add_39
	    buf3 = reader.storage(None, 64*s0, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf3, (s0, 32), dtype=torch.float16, is_leaf=True)  # view_10
	    buf4 = reader.storage(None, 16384*s0, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf4, (1, s0, 8192), dtype=torch.float16, is_leaf=True)  # add_85
	    buf5 = reader.storage(None, 16384*s0, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf5, (s0, 8192), dtype=torch.float16, is_leaf=True)  # view_14
	    buf6 = reader.storage(None, 64*s0, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf6, (s0, 32), dtype=torch.float16, is_leaf=True)  # view_16
	    buf7 = reader.storage(None, 131072, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf7, (2048, 32), dtype=torch.float16, is_leaf=True)  # permute_20
	    buf8 = reader.storage(None, 524288, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf8, (32, 8192), dtype=torch.float16, is_leaf=True)  # permute_24
	    buf9 = reader.storage(None, 33554432, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf9, (2048, 8192), dtype=torch.float16, is_leaf=True)  # permute_27
	    buf10 = reader.storage(None, 524288, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf10, (8192, 32), dtype=torch.float16, is_leaf=True)  # permute_30
	    buf11 = reader.storage(None, 131072, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf11, (32, 2048), dtype=torch.float16, is_leaf=True)  # permute_34
	    buf12 = reader.storage(None, 33554432, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf12, (8192, 2048), dtype=torch.float16, is_leaf=True)  # permute_37
	    buf13 = reader.storage(None, 524288, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf13, (8192, 32), dtype=torch.float16, is_leaf=True)  # permute_41
	    buf14 = reader.storage(None, 131072, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf14, (32, 2048), dtype=torch.float16, is_leaf=True)  # permute_45
	    buf15 = reader.storage(None, 33554432, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf15, (8192, 2048), dtype=torch.float16, is_leaf=True)  # permute_48
	    buf16 = reader.storage(None, 4096*s0, device=device(type='cuda', index=0), dtype_hint=torch.float16)
	    reader.tensor(buf16, (1, s0, 2048), dtype=torch.float16, is_leaf=True)  # tangents_1
	load_args._version = 0
	mod = Repro()
	if __name__ == '__main__':
	    from torch._dynamo.repro.after_aot import run_repro
	    with torch.no_grad():
	        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # To run it separately, do 
	        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # mod(*args)
V0326 23:29:18.477000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:801] {"inductor_post_grad_graph": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "8332632f6c682f852826e9ffbdb07c8a"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "Sym(s0)", view_2: "f16[s0, 2048][2048, 1]cuda:0", view_4: "f16[s0, 32][32, 1]cuda:0", add_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", view_10: "f16[s0, 32][32, 1]cuda:0", add_85: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", view_14: "f16[s0, 8192][8192, 1]cuda:0", view_16: "f16[s0, 32][32, 1]cuda:0", permute_20: "f16[2048, 32][32, 1]cuda:0", permute_24: "f16[32, 8192][8192, 1]cuda:0", permute_27: "f16[2048, 8192][8192, 1]cuda:0", permute_30: "f16[8192, 32][32, 1]cuda:0", permute_34: "f16[32, 2048][2048, 1]cuda:0", permute_37: "f16[8192, 2048][2048, 1]cuda:0", permute_41: "f16[8192, 32][32, 1]cuda:0", permute_45: "f16[32, 2048][2048, 1]cuda:0", permute_48: "f16[8192, 2048][2048, 1]cuda:0", tangents_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"):
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_144: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, 2.0)
	        view_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.reshape.default(mul_144, [primals_1, 2048]);  mul_144 = None
	        permute_18: "f16[2048, s0][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_18, [1, 0])
	        mm_9: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None
	        permute_19: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None
	        mm_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None
	        view_19: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.reshape.default(mm_10, [1, primals_1, 32]);  mm_10 = None
	        permute_21: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None
	        convert_element_type_39: "f32[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None
	        view_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.reshape.default(view_19, [primals_1, 32]);  view_19 = None
	        permute_22: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_20, [1, 0])
	        mm_11: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None
	        permute_23: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None
	        mm_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None
	        view_21: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.reshape.default(mm_12, [1, primals_1, 8192]);  mm_12 = None
	        permute_25: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None
	        convert_element_type_44: "f32[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.reshape.default(tangents_1, [primals_1, 2048]);  tangents_1 = None
	        mm_13: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None
	        view_23: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.reshape.default(mm_13, [1, primals_1, 8192]);  mm_13 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_135: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_21, view_23);  view_21 = view_23 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        convert_element_type_11: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        mul_146: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None
	        mul_147: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_148: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_146, 2.0)
	        view_24: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.reshape.default(mul_148, [primals_1, 8192]);  mul_148 = None
	        permute_28: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_24, [1, 0])
	        mm_14: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None
	        permute_29: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None
	        mm_15: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None
	        view_25: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.reshape.default(mm_15, [1, primals_1, 32]);  mm_15 = None
	        permute_31: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None
	        convert_element_type_54: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None
	        view_26: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.reshape.default(view_25, [primals_1, 32]);  view_25 = None
	        permute_32: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_26, [1, 0])
	        mm_16: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_32, view_2);  permute_32 = None
	        permute_33: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None
	        mm_17: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None
	        view_27: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.reshape.default(mm_17, [1, primals_1, 2048]);  mm_17 = None
	        permute_35: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None
	        convert_element_type_59: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_28: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.reshape.default(mul_146, [primals_1, 8192]);  mul_146 = None
	        mm_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None
	        view_29: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.reshape.default(mm_18, [1, primals_1, 2048]);  mm_18 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_136: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_27, view_29);  view_27 = view_29 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        sigmoid_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(add_39)
	        full_default: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_44: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_default, sigmoid_1);  full_default = None
	        mul_150: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None
	        add_137: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None
	        mul_151: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None
	        mul_152: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_153: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_152, 2.0)
	        view_30: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.reshape.default(mul_153, [primals_1, 8192]);  mul_153 = None
	        permute_39: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_30, [1, 0])
	        mm_19: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None
	        permute_40: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None
	        mm_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None
	        view_31: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.reshape.default(mm_20, [1, primals_1, 32]);  mm_20 = None
	        permute_42: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None
	        convert_element_type_69: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None
	        view_32: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.reshape.default(view_31, [primals_1, 32]);  view_31 = None
	        permute_43: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_32, [1, 0])
	        mm_21: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None
	        permute_44: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None
	        mm_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None
	        view_33: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.reshape.default(mm_22, [1, primals_1, 2048]);  mm_22 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        add_138: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_46: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None
	        convert_element_type_74: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_34: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.reshape.default(mul_152, [primals_1, 8192]);  mul_152 = None
	        mm_23: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None
	        view_35: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.reshape.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_139: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_138, view_35);  add_138 = view_35 = None
	        return (None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39)
	        
V0326 23:29:18.477000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "871a9d2b2e6b5f164aa565416233f7ca"}
	{
	"name": "GraphLowering.run",
	"ts": 1742992158477774.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.493000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "d96359d38139e964c6f7f8a40f54d505"}
	{
	"name": "GraphLowering.run",
	"ts": 1742992158492975.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.493000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "99bddadef19658f11133352c9e24e959"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1742992158493402.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.493000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "c65350d58c8708bdffc0eb1ce7123918"}
	{
	"name": "code_gen",
	"ts": 1742992158493402.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.494000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "3d60bd570178ef49695dc6f5fafecd38"}
	{
	"name": "Scheduler.__init__",
	"ts": 1742992158494642.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.571000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "3d9bc5800b84f6812296ab9382df1ad3"}
	{
	"name": "Scheduler.__init__",
	"ts": 1742992158571668.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.572000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "a14ffe711c934a2ee253f29d73cae94f"}
	{
	"name": "Scheduler.codegen",
	"ts": 1742992158572023.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.605000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "c9212e0a15939c98062e2f6c6ac8263a"}
	{
	"name": "Scheduler.codegen",
	"ts": 1742992158605492.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.605000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "47f29311967c9fb28c9db06b0047c059"}
	{
	"name": "WrapperCodeGen.generate",
	"ts": 1742992158605794.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.608000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "8dc6dfc423cbc0c1af88943c265bb2a1"}
	{
	"name": "WrapperCodeGen.generate",
	"ts": 1742992158608235.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.608000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/graph.py:1861] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/hi/chi6hm2i2r5fg3p735q2myqwx4uplr5ji3czydmtzsbsjw6fmtl2.py"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "ccae98e057b275846abfbc378763924c"}
	# AOT ID: ['5_backward']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/oz/cozopk2ehguq32bevie5q4mjctwrvknb2efn2zmifydb43kq7u2a.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten.mul, aten.view]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %mul_144 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%tangents_1, 2.0), kwargs = {})
	#   %view_18 : [num_users=2] = call_function[target=torch.ops.aten.reshape.default](args = (%mul_144, [%primals_1, 2048]), kwargs = {})
	triton_poi_fused_mul_view_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_view_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = 2.0
	    tmp2 = tmp0 * tmp1
	    tl.store(out_ptr0 + (x0), tmp2, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/6p/c6pwujwafd7jyea2qiskauahv6a3isypaxtecr7nxxt4nrphxtq6.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_39 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_21, torch.float32), kwargs = {})
	triton_poi_fused__to_copy_1 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[65536], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_1', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 65536
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/2o/c2odmo35jco5222u2qehkh6jj67bnvs7ccevylkmgiqf4prmli3t.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_44 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_25, torch.float32), kwargs = {})
	triton_poi_fused__to_copy_2 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_2', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 262144
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/ut/cutu6nbumwt37rt3w5ivi5f2exxnf3nqjnpdd3meykytx4gvjuio.py
	# Topologically Sorted Source Nodes: [silu], Original ATen: [aten.add, aten.silu, aten.mul, aten.sigmoid, aten.fill, aten.sub]
	# Source node to ATen node mapping:
	#   silu => convert_element_type_11, convert_element_type_12, mul_46, sigmoid
	# Graph fragment:
	#   %add_135 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_21, %view_23), kwargs = {})
	#   %convert_element_type_11 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_39, torch.float32), kwargs = {})
	#   %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_11,), kwargs = {})
	#   %mul_46 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_11, %sigmoid), kwargs = {})
	#   %convert_element_type_12 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_46, torch.float16), kwargs = {})
	#   %mul_146 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_135, %convert_element_type_12), kwargs = {})
	#   %mul_147 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_135, %add_85), kwargs = {})
	#   %mul_148 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_146, 2.0), kwargs = {})
	#   %sigmoid_1 : [num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%add_39,), kwargs = {})
	#   %full_default : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([1, %primals_1, 8192], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
	#   %sub_44 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default, %sigmoid_1), kwargs = {})
	#   %mul_150 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_39, %sub_44), kwargs = {})
	#   %add_137 : [num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_150, 1), kwargs = {})
	#   %mul_151 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_1, %add_137), kwargs = {})
	#   %mul_152 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_147, %mul_151), kwargs = {})
	#   %mul_153 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_152, 2.0), kwargs = {})
	triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1048576], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: '*fp16', 5: '*fp16', 6: '*fp16', 7: '*fp16', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, out_ptr2, out_ptr3, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr1 + (x0), None).to(tl.float32)
	    tmp3 = tl.load(in_ptr2 + (x0), None).to(tl.float32)
	    tmp11 = tl.load(in_ptr3 + (x0), None).to(tl.float32)
	    tmp2 = tmp0 + tmp1
	    tmp4 = tmp3.to(tl.float32)
	    tmp5 = tl.sigmoid(tmp4)
	    tmp6 = tmp4 * tmp5
	    tmp7 = tmp6.to(tl.float32)
	    tmp8 = tmp2 * tmp7
	    tmp9 = 2.0
	    tmp10 = tmp8 * tmp9
	    tmp12 = tmp2 * tmp11
	    tmp13 = tl.sigmoid(tmp3)
	    tmp14 = 1.0
	    tmp15 = tmp14 - tmp13
	    tmp16 = tmp3 * tmp15
	    tmp17 = tmp16 + tmp14
	    tmp18 = tmp13 * tmp17
	    tmp19 = tmp12 * tmp18
	    tmp20 = tmp19 * tmp9
	    tl.store(out_ptr0 + (x0), tmp10, None)
	    tl.store(out_ptr1 + (x0), tmp8, None)
	    tl.store(out_ptr2 + (x0), tmp20, None)
	    tl.store(out_ptr3 + (x0), tmp19, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/vd/cvdgoxiwnbzp7azzk6zja6cs44uw5kuroxt6qq5umvp5rqdwji7r.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten.add]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %add_136 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_27, %view_29), kwargs = {})
	#   %add_138 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_136, %view_33), kwargs = {})
	#   %add_139 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_138, %view_35), kwargs = {})
	triton_poi_fused_add_4 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_4', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_out_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp3 = tl.load(in_ptr1 + (x0), None).to(tl.float32)
	    tmp5 = tl.load(in_ptr2 + (x0), None).to(tl.float32)
	    tmp2 = tmp0 + tmp1
	    tmp4 = tmp2 + tmp3
	    tmp6 = tmp4 + tmp5
	    tl.store(in_out_ptr0 + (x0), tmp6, None)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1 = args
	    args.clear()
	    s0 = primals_1
	    assert_size_stride(view_2, (s0, 2048), (2048, 1))
	    assert_size_stride(view_4, (s0, 32), (32, 1))
	    assert_size_stride(add_39, (1, s0, 8192), (8192*s0, 8192, 1))
	    assert_size_stride(view_10, (s0, 32), (32, 1))
	    assert_size_stride(add_85, (1, s0, 8192), (8192*s0, 8192, 1))
	    assert_size_stride(view_14, (s0, 8192), (8192, 1))
	    assert_size_stride(view_16, (s0, 32), (32, 1))
	    assert_size_stride(permute_20, (2048, 32), (32, 1))
	    assert_size_stride(permute_24, (32, 8192), (8192, 1))
	    assert_size_stride(permute_27, (2048, 8192), (8192, 1))
	    assert_size_stride(permute_30, (8192, 32), (32, 1))
	    assert_size_stride(permute_34, (32, 2048), (2048, 1))
	    assert_size_stride(permute_37, (8192, 2048), (2048, 1))
	    assert_size_stride(permute_41, (8192, 32), (32, 1))
	    assert_size_stride(permute_45, (32, 2048), (2048, 1))
	    assert_size_stride(permute_48, (8192, 2048), (2048, 1))
	    assert_size_stride(tangents_1, (1, s0, 2048), (2048*s0, 2048, 1))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mul, aten.view]
	        triton_poi_fused_mul_view_0_xnumel = 2048*s0
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_mul_view_0.run(tangents_1, buf0, triton_poi_fused_mul_view_0_xnumel, grid=grid(triton_poi_fused_mul_view_0_xnumel), stream=stream0)
	        buf1 = empty_strided_cuda((2048, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf0, (2048, s0), (1, 2048), 0), view_16, out=buf1)
	        del view_16
	        buf2 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf0, permute_20, out=buf2)
	        del permute_20
	        buf3 = empty_strided_cuda((2048, 32), (32, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf1, buf3, 65536, grid=grid(65536), stream=stream0)
	        buf4 = empty_strided_cuda((32, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf2, (32, s0), (1, 32), 0), view_14, out=buf4)
	        del view_14
	        buf5 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf2, permute_24, out=buf5)
	        del permute_24
	        buf6 = empty_strided_cuda((32, 8192), (8192, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_2.run(buf4, buf6, 262144, grid=grid(262144), stream=stream0)
	        buf7 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(tangents_1, (s0, 2048), (2048, 1), 0), permute_27, out=buf7)
	        del permute_27
	        del tangents_1
	        buf8 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        buf15 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        buf17 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        buf24 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [silu], Original ATen: [aten.add, aten.silu, aten.mul, aten.sigmoid, aten.fill, aten.sub]
	        triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3_xnumel = 8192*s0
	        triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3.run(buf5, buf7, add_39, add_85, buf8, buf15, buf17, buf24, triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3_xnumel, grid=grid(triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3_xnumel), stream=stream0)
	        del add_39
	        del add_85
	        del buf5
	        del buf7
	        buf9 = reinterpret_tensor(buf4, (8192, 32), (32, 1), 0); del buf4  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf8, (8192, s0), (1, 8192), 0), view_10, out=buf9)
	        del view_10
	        buf10 = buf2; del buf2  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf8, (s0, 8192), (8192, 1), 0), permute_30, out=buf10)
	        del buf8
	        del permute_30
	        buf11 = empty_strided_cuda((8192, 32), (32, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_2.run(buf9, buf11, 262144, grid=grid(262144), stream=stream0)
	        buf12 = reinterpret_tensor(buf1, (32, 2048), (2048, 1), 0); del buf1  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf10, (32, s0), (1, 32), 0), view_2, out=buf12)
	        buf13 = buf0; del buf0  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf10, permute_34, out=buf13)
	        del permute_34
	        buf14 = empty_strided_cuda((32, 2048), (2048, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf12, buf14, 65536, grid=grid(65536), stream=stream0)
	        buf16 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf15, (s0, 8192), (8192, 1), 0), permute_37, out=buf16)
	        del buf15
	        del permute_37
	        buf18 = buf9; del buf9  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf17, (8192, s0), (1, 8192), 0), view_4, out=buf18)
	        del view_4
	        buf19 = buf10; del buf10  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf17, (s0, 8192), (8192, 1), 0), permute_41, out=buf19)
	        del buf17
	        del permute_41
	        buf20 = empty_strided_cuda((8192, 32), (32, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_2.run(buf18, buf20, 262144, grid=grid(262144), stream=stream0)
	        del buf18
	        buf21 = buf12; del buf12  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf19, (32, s0), (1, 32), 0), view_2, out=buf21)
	        del view_2
	        buf22 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf19, permute_45, out=buf22)
	        del buf19
	        del permute_45
	        buf23 = empty_strided_cuda((32, 2048), (2048, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf21, buf23, 65536, grid=grid(65536), stream=stream0)
	        del buf21
	        buf25 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf24, (s0, 8192), (8192, 1), 0), permute_48, out=buf25)
	        del buf24
	        del permute_48
	        buf26 = reinterpret_tensor(buf13, (1, s0, 2048), (2048*s0, 2048, 1), 0); del buf13  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.add]
	        triton_poi_fused_add_4_xnumel = 2048*s0
	        triton_poi_fused_add_4.run(buf26, buf16, buf22, buf25, triton_poi_fused_add_4_xnumel, grid=grid(triton_poi_fused_add_4_xnumel), stream=stream0)
	        del buf16
	        del buf22
	        del buf25
	    return (None, buf26, None, None, None, None, None, None, None, None, None, None, buf23, buf20, None, None, None, None, None, None, None, None, None, None, buf14, buf11, None, None, None, None, None, None, None, None, None, None, buf6, buf3, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    primals_1 = 100
	    view_2 = rand_strided((100, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    view_4 = rand_strided((100, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    add_39 = rand_strided((1, 100, 8192), (819200, 8192, 1), device='cuda:0', dtype=torch.float16)
	    view_10 = rand_strided((100, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    add_85 = rand_strided((1, 100, 8192), (819200, 8192, 1), device='cuda:0', dtype=torch.float16)
	    view_14 = rand_strided((100, 8192), (8192, 1), device='cuda:0', dtype=torch.float16)
	    view_16 = rand_strided((100, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_20 = rand_strided((2048, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_24 = rand_strided((32, 8192), (8192, 1), device='cuda:0', dtype=torch.float16)
	    permute_27 = rand_strided((2048, 8192), (8192, 1), device='cuda:0', dtype=torch.float16)
	    permute_30 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_34 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    permute_37 = rand_strided((8192, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    permute_41 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_45 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    permute_48 = rand_strided((8192, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    tangents_1 = rand_strided((1, 100, 2048), (204800, 2048, 1), device='cuda:0', dtype=torch.float16)
	    fn = lambda: call([primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:29:18.950000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "f867d238ee60e37ca7fc51fbd4db586e"}
	{
	"name": "code_gen",
	"ts": 1742992158950886.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.951000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "1062781e930e93cfc39ecd1d8c1e1bfd"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1742992158951236.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.965000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "4afaad60d49f9eb990d8c4e838bab56a"}
	{
	"name": "fx_graph_cache_miss",
	"ts": 1742992158431900.8,
	"args": {
	"key": "fuwpd53kuljrtdqmdkesfbb743gwbdes3cogyfbosttj4mjzbydy",
	"components": [
	"[cjhczcrgi3pzc6cive3k3kqwasc5syujhgv54rquueioi4qb6w2] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1):\n    mul_144 = torch.ops.aten.mul.Tensor(tangents_1, 2.0)\n    view_18 = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None\n    permute_18 = torch.ops.aten.permute.default(view_18, [1, 0])\n    mm_9 = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None\n    permute_19 = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None\n    mm_10 = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None\n    view_19 = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None\n    permute_21 = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None\n    convert_element_type_39 = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None\n    view_20 = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None\n    permute_22 = torch.ops.aten.permute.default(view_20, [1, 0])\n    mm_11 = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None\n    permute_23 = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None\n    mm_12 = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None\n    view_21 = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None\n    permute_25 = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None\n    convert_element_type_44 = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None\n    view_22 = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None\n    mm_13 = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None\n    view_23 = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None\n    convert_element_type_default_2 = torch.ops.prims.convert_element_type.default(view_23, torch.float16);  view_23 = None\n    add_135 = torch.ops.aten.add.Tensor(view_21, convert_element_type_default_2);  view_21 = convert_element_type_default_2 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    mul_146 = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None\n    mul_147 = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None\n    mul_148 = torch.ops.aten.mul.Tensor(mul_146, 2.0)\n    view_24 = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None\n    permute_28 = torch.ops.aten.permute.default(view_24, [1, 0])\n    mm_14 = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None\n    permute_29 = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None\n    mm_15 = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None\n    view_25 = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None\n    permute_31 = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None\n    convert_element_type_54 = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None\n    view_26 = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None\n    permute_32 = torch.ops.aten.permute.default(view_26, [1, 0])\n    mm_16 = torch.ops.aten.mm.default(permute_32, view_2);  permute_32 = None\n    permute_33 = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None\n    mm_17 = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None\n    view_27 = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None\n    permute_35 = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None\n    convert_element_type_59 = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None\n    view_28 = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None\n    mm_18 = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None\n    view_29 = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None\n    convert_element_type_default_1 = torch.ops.prims.convert_element_type.default(view_29, torch.float16);  view_29 = None\n    add_136 = torch.ops.aten.add.Tensor(view_27, convert_element_type_default_1);  view_27 = convert_element_type_default_1 = None\n    sigmoid_1 = torch.ops.aten.sigmoid.default(add_39)\n    full_default = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)\n    sub_44 = torch.ops.aten.sub.Tensor(full_default, sigmoid_1);  full_default = None\n    mul_150 = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None\n    add_137 = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None\n    mul_151 = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None\n    mul_152 = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None\n    mul_153 = torch.ops.aten.mul.Tensor(mul_152, 2.0)\n    view_30 = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None\n    permute_39 = torch.ops.aten.permute.default(view_30, [1, 0])\n    mm_19 = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None\n    permute_40 = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None\n    mm_20 = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None\n    view_31 = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None\n    permute_42 = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None\n    convert_element_type_69 = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None\n    view_32 = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None\n    permute_43 = torch.ops.aten.permute.default(view_32, [1, 0])\n    mm_21 = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None\n    permute_44 = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None\n    mm_22 = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None\n    view_33 = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None\n    add_138 = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None\n    permute_46 = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None\n    convert_element_type_74 = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None\n    view_34 = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None\n    mm_23 = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None\n    view_35 = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None\n    convert_element_type_default = torch.ops.prims.convert_element_type.default(view_35, torch.float16);  view_35 = None\n    add_139 = torch.ops.aten.add.Tensor(add_138, convert_element_type_default);  add_138 = convert_element_type_default = None\n    return (None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[6pt52nx6wf2o3fnlscvbb6mpkzglnsxgtp26liujc4wx2tvtpm7] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4096*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[3]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[5]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[4ofuwkiqpjt5dr73pqxsydaoru4cj2w3m4sn674mztswxzkzyv5] example_inputs[6]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[kjiu5nyliqwsnl7icaeauzcjyk3boczmm4vcgqglpeyga4ezc26] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[buqdocmqej3phjqjaio73y24jqcnui6yzgcbitztcq67svbntcd] example_inputs[9]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hxsqnh7shhh7eup67u4ogv42dqcuywp53tt5tp7sdfgeqxuk32j] example_inputs[10]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[11]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[12]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[13]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[14]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[15]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[16]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[17]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_backward]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[n2y367w3cvigrpqrkfggnd4ki5vge46t7f3bihnrmz2hx2qkg6o] fx_kwargs[static_input_idxs]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]",
	"[giofkincgu73hppmew27gpl3zgvnfyyigbbqjyi5t7e6a5ldenw] fx_kwargs[user_visible_outputs]: {'add_139': None, 'convert_element_type_74': None, 'convert_element_type_69': None, 'convert_element_type_59': None, 'convert_element_type_54': None, 'convert_element_type_44': None, 'convert_element_type_39': None}",
	"[vhi4lnshnjmzxgwsuiowu552sqjv64oariyhqqha4hh3x7qlxkw] inputs_to_check[0]: 17",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_taken_ns": 532666105,
	"cache_state": "miss"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:29:18.965000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "c372a1dab9f286d8e0f27b9f385bfc48"}
	{"key": "fuwpd53kuljrtdqmdkesfbb743gwbdes3cogyfbosttj4mjzbydy", "components": ["[cjhczcrgi3pzc6cive3k3kqwasc5syujhgv54rquueioi4qb6w2] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1):\n    mul_144 = torch.ops.aten.mul.Tensor(tangents_1, 2.0)\n    view_18 = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None\n    permute_18 = torch.ops.aten.permute.default(view_18, [1, 0])\n    mm_9 = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None\n    permute_19 = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None\n    mm_10 = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None\n    view_19 = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None\n    permute_21 = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None\n    convert_element_type_39 = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None\n    view_20 = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None\n    permute_22 = torch.ops.aten.permute.default(view_20, [1, 0])\n    mm_11 = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None\n    permute_23 = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None\n    mm_12 = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None\n    view_21 = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None\n    permute_25 = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None\n    convert_element_type_44 = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None\n    view_22 = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None\n    mm_13 = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None\n    view_23 = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None\n    convert_element_type_default_2 = torch.ops.prims.convert_element_type.default(view_23, torch.float16);  view_23 = None\n    add_135 = torch.ops.aten.add.Tensor(view_21, convert_element_type_default_2);  view_21 = convert_element_type_default_2 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    mul_146 = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None\n    mul_147 = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None\n    mul_148 = torch.ops.aten.mul.Tensor(mul_146, 2.0)\n    view_24 = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None\n    permute_28 = torch.ops.aten.permute.default(view_24, [1, 0])\n    mm_14 = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None\n    permute_29 = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None\n    mm_15 = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None\n    view_25 = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None\n    permute_31 = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None\n    convert_element_type_54 = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None\n    view_26 = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None\n    permute_32 = torch.ops.aten.permute.default(view_26, [1, 0])\n    mm_16 = torch.ops.aten.mm.default(permute_32, view_2);  permute_32 = None\n    permute_33 = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None\n    mm_17 = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None\n    view_27 = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None\n    permute_35 = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None\n    convert_element_type_59 = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None\n    view_28 = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None\n    mm_18 = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None\n    view_29 = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None\n    convert_element_type_default_1 = torch.ops.prims.convert_element_type.default(view_29, torch.float16);  view_29 = None\n    add_136 = torch.ops.aten.add.Tensor(view_27, convert_element_type_default_1);  view_27 = convert_element_type_default_1 = None\n    sigmoid_1 = torch.ops.aten.sigmoid.default(add_39)\n    full_default = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)\n    sub_44 = torch.ops.aten.sub.Tensor(full_default, sigmoid_1);  full_default = None\n    mul_150 = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None\n    add_137 = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None\n    mul_151 = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None\n    mul_152 = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None\n    mul_153 = torch.ops.aten.mul.Tensor(mul_152, 2.0)\n    view_30 = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None\n    permute_39 = torch.ops.aten.permute.default(view_30, [1, 0])\n    mm_19 = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None\n    permute_40 = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None\n    mm_20 = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None\n    view_31 = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None\n    permute_42 = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None\n    convert_element_type_69 = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None\n    view_32 = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None\n    permute_43 = torch.ops.aten.permute.default(view_32, [1, 0])\n    mm_21 = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None\n    permute_44 = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None\n    mm_22 = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None\n    view_33 = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None\n    add_138 = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None\n    permute_46 = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None\n    convert_element_type_74 = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None\n    view_34 = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None\n    mm_23 = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None\n    view_35 = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None\n    convert_element_type_default = torch.ops.prims.convert_element_type.default(view_35, torch.float16);  view_35 = None\n    add_139 = torch.ops.aten.add.Tensor(add_138, convert_element_type_default);  add_138 = convert_element_type_default = None\n    return (None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[6pt52nx6wf2o3fnlscvbb6mpkzglnsxgtp26liujc4wx2tvtpm7] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4096*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[3]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[5]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[4ofuwkiqpjt5dr73pqxsydaoru4cj2w3m4sn674mztswxzkzyv5] example_inputs[6]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[kjiu5nyliqwsnl7icaeauzcjyk3boczmm4vcgqglpeyga4ezc26] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[buqdocmqej3phjqjaio73y24jqcnui6yzgcbitztcq67svbntcd] example_inputs[9]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hxsqnh7shhh7eup67u4ogv42dqcuywp53tt5tp7sdfgeqxuk32j] example_inputs[10]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[11]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[12]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[13]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[14]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[15]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[16]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[17]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_backward]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[n2y367w3cvigrpqrkfggnd4ki5vge46t7f3bihnrmz2hx2qkg6o] fx_kwargs[static_input_idxs]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]", "[giofkincgu73hppmew27gpl3zgvnfyyigbbqjyi5t7e6a5ldenw] fx_kwargs[user_visible_outputs]: {'add_139': None, 'convert_element_type_74': None, 'convert_element_type_69': None, 'convert_element_type_59': None, 'convert_element_type_54': None, 'convert_element_type_44': None, 'convert_element_type_39': None}", "[vhi4lnshnjmzxgwsuiowu552sqjv64oariyhqqha4hh3x7qlxkw] inputs_to_check[0]: 17", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_taken_ns": 532666105, "cache_state": "miss"}
V0326 23:29:18.966000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "806af068a0f99783be19d1a3c3348b1c"}
	{
	"name": "inductor_compile",
	"ts": 1742992158966588.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.967000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "2f83d9b22fcfe674a93b500b212cee05"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992158967009.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.967000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"bwd_compilation_metrics": {"compile_id": "3/0", "inductor_compile_time_s": 0.5462415218353271, "code_gen_time_s": 0.457385778427124, "fail_type": null, "fail_reason": null}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:18.967000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "71198e25f4438e94bbc91bbc0ab350b3"}
	{
	"name": "compile_fx.<locals>.bw_compiler",
	"ts": 1742992158967616.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.969000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "ca335f15c421d6d7ce670be582e5ab25"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992158969189.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.969000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "52971eba5e80cc37c89d8eec97919092"}
	{
	"name": "backend_compile",
	"ts": 1742992158969634.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:18.969000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "3bc07963dbc396411dd6a81ec789360a"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992158969878.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.014000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "fa3b69e2a5d507684806f349214cc790"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['x'], accessed_by=DictGetItemGuardAccessor(x)
	| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=True, size=[1, None, 2048], stride=[None, 2048, 1])
	| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False         
	| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['x'], L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight'], L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight'], L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight'], L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight'], L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight'], L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight'])
	| +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)
	| | +- TYPE_MATCH: ___check_type_id(L['self'], 105190333749776)                
	| | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | +- DICT_LENGTH: len(L['self']._modules) == 4                                
	| | | | +- GuardManager: source=L['self']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor(gate_proj)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj'], 105190356359888)
	| | | | | +- GuardManager: source=L['self']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].scaling, accessed_by=DictGetItemGuardAccessor(scaling)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj'].scaling) == 1           
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].scaling['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj'].scaling['default'] == 2.0   
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules) == 7          
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'], accessed_by=DictGetItemGuardAccessor(base_layer)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer'], 105190296992528)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['base_layer'].__dict__)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._buffers, accessed_by=DictGetItemGuardAccessor(_buffers)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._modules['base_layer']._buffers
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._modules['base_layer']._modules
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['base_layer']._parameters) == 2
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'], 105190296990752)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[8388608, 1], stride=[1, 1])
	| | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state, accessed_by=GetAttrGuardAccessor(quant_state)
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state, 105190295709968)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype == torch.bfloat16
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, 127872448968448)
	| | | | | | | | | | | | | +- LENGTH_CHECK: len(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], 127873236398144)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], 127873236398144)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[262144], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, 105190295709968)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[1024], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, 127873236398144)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, 127873236398144)
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['bias'], 127873236386624)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'].compute_dtype, accessed_by=DictGetItemGuardAccessor(compute_dtype)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj']._modules['base_layer'].compute_dtype == torch.bfloat16
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._modules['base_layer']._backward_hooks
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._modules['base_layer']._backward_pre_hooks
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'].compute_type_is_set, accessed_by=DictGetItemGuardAccessor(compute_type_is_set)
	| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['base_layer'].compute_type_is_set, 127873236421344)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout'], accessed_by=DictGetItemGuardAccessor(lora_dropout)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_dropout'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_dropout']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'], 105190279249888)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'].__dict__)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A'], accessed_by=DictGetItemGuardAccessor(lora_A)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_A'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DictGuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- KeyValueManager pair at index=0
	| | | | | | | | | | | +- KeyManager: GuardManager: source=list(L['self']._modules['gate_proj']._modules['lora_A']._modules.keys())[0]
	| | | | | | | | | | | | +- EQUALS_MATCH: list(L['self']._modules['gate_proj']._modules['lora_A']._modules.keys())[0] == 'default'
	| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_A']._modules['default'], 105190278900864)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['lora_A']._modules['default'].__dict__)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[32, 2048], stride=[2048, 1])
	| | | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['bias'], 127873236386624)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B'], accessed_by=DictGetItemGuardAccessor(lora_B)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_B'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_B']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_B']._modules['default'], 105190278900864)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['lora_B']._modules['default'].__dict__)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[8192, 32], stride=[32, 1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['bias'], 127873236386624)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_embedding_A'], accessed_by=DictGetItemGuardAccessor(lora_embedding_A)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_embedding_B'], accessed_by=DictGetItemGuardAccessor(lora_embedding_B)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_magnitude_vector'], accessed_by=DictGetItemGuardAccessor(lora_magnitude_vector)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].use_dora, accessed_by=DictGetItemGuardAccessor(use_dora)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj'].use_dora) == 1          
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].use_dora['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj'].use_dora['default'], 127873236421344)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._parameters             
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._active_adapter, accessed_by=DictGetItemGuardAccessor(_active_adapter)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._active_adapter, 127873236399136)
	| | | | | | | +- LENGTH_CHECK: len(L['self']._modules['gate_proj']._active_adapter) == 1   
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['up_proj']._active_adapter
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['down_proj']._active_adapter
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._active_adapter[0], accessed_by=ListGetItemGuardAccessor(0)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj']._active_adapter[0] == 'default'
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._backward_hooks         
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].merged_adapters, accessed_by=DictGetItemGuardAccessor(merged_adapters)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj'].merged_adapters, 127873236399136)
	| | | | | | | +- LENGTH_CHECK: not L['self']._modules['gate_proj'].merged_adapters         
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._disable_adapters, accessed_by=DictGetItemGuardAccessor(_disable_adapters)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._disable_adapters, 127873236421344)
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._backward_pre_hooks     
	| | | | +- GuardManager: source=L['self']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor(up_proj)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj'], 105190356359888)
	| | | | | +- GuardManager: source=L['self']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj'].scaling, accessed_by=DictGetItemGuardAccessor(scaling)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj'].scaling) == 1             
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj'].scaling['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['up_proj'].scaling['default'] == 2.0     
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules) == 7            
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'], accessed_by=DictGetItemGuardAccessor(base_layer)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer'], 105190296992528)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['base_layer'].__dict__)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._buffers, accessed_by=DictGetItemGuardAccessor(_buffers)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._modules['base_layer']._buffers
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._modules['base_layer']._modules
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['base_layer']._parameters) == 2
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'], 105190296990752)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[8388608, 1], stride=[1, 1])
	| | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state, accessed_by=GetAttrGuardAccessor(quant_state)
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state, 105190295709968)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype == torch.bfloat16
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, 127872448968448)
	| | | | | | | | | | | | | +- LENGTH_CHECK: len(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], 127873236398144)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], 127873236398144)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[262144], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, 105190295709968)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[1024], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, 127873236398144)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, 127873236398144)
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['bias'], 127873236386624)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'].compute_dtype, accessed_by=DictGetItemGuardAccessor(compute_dtype)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['up_proj']._modules['base_layer'].compute_dtype == torch.bfloat16
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._modules['base_layer']._backward_hooks
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._modules['base_layer']._backward_pre_hooks
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'].compute_type_is_set, accessed_by=DictGetItemGuardAccessor(compute_type_is_set)
	| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['base_layer'].compute_type_is_set, 127873236421344)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout'], accessed_by=DictGetItemGuardAccessor(lora_dropout)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_dropout'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_dropout']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'], 105190279249888)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'].__dict__)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A'], accessed_by=DictGetItemGuardAccessor(lora_A)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_A'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DictGuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- KeyValueManager pair at index=0
	| | | | | | | | | | | +- KeyManager: GuardManager: source=list(L['self']._modules['up_proj']._modules['lora_A']._modules.keys())[0]
	| | | | | | | | | | | | +- EQUALS_MATCH: list(L['self']._modules['up_proj']._modules['lora_A']._modules.keys())[0] == 'default'
	| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_A']._modules['default'], 105190278900864)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['lora_A']._modules['default'].__dict__)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[32, 2048], stride=[2048, 1])
	| | | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['bias'], 127873236386624)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B'], accessed_by=DictGetItemGuardAccessor(lora_B)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_B'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_B']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_B']._modules['default'], 105190278900864)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['lora_B']._modules['default'].__dict__)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[8192, 32], stride=[32, 1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['bias'], 127873236386624)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_embedding_A'], accessed_by=DictGetItemGuardAccessor(lora_embedding_A)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_embedding_B'], accessed_by=DictGetItemGuardAccessor(lora_embedding_B)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_magnitude_vector'], accessed_by=DictGetItemGuardAccessor(lora_magnitude_vector)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj'].use_dora, accessed_by=DictGetItemGuardAccessor(use_dora)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj'].use_dora) == 1            
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj'].use_dora['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj'].use_dora['default'], 127873236421344)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._parameters               
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._backward_hooks           
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj'].merged_adapters, accessed_by=DictGetItemGuardAccessor(merged_adapters)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj'].merged_adapters, 127873236399136)
	| | | | | | | +- LENGTH_CHECK: not L['self']._modules['up_proj'].merged_adapters           
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._disable_adapters, accessed_by=DictGetItemGuardAccessor(_disable_adapters)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._disable_adapters, 127873236421344)
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._backward_pre_hooks       
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._active_adapter, accessed_by=DictGetItemGuardAccessor(_active_adapter)
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['up_proj']._active_adapter
	| | | | +- GuardManager: source=L['self']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor(down_proj)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj'], 105190356359888)
	| | | | | +- GuardManager: source=L['self']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj'].scaling, accessed_by=DictGetItemGuardAccessor(scaling)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj'].scaling) == 1           
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj'].scaling['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['down_proj'].scaling['default'] == 2.0   
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules) == 7          
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'], accessed_by=DictGetItemGuardAccessor(base_layer)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer'], 105190296992528)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['base_layer'].__dict__)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._buffers, accessed_by=DictGetItemGuardAccessor(_buffers)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._modules['base_layer']._buffers
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._modules['base_layer']._modules
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['base_layer']._parameters) == 2
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'], 105190296990752)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[8388608, 1], stride=[1, 1])
	| | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state, accessed_by=GetAttrGuardAccessor(quant_state)
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state, 105190295709968)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype == torch.bfloat16
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, 127872448968448)
	| | | | | | | | | | | | | +- LENGTH_CHECK: len(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], 127873236398144)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], 127873236398144)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[262144], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, 105190295709968)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[1024], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, 127873236398144)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, 127873236398144)
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['bias'], 127873236386624)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'].compute_dtype, accessed_by=DictGetItemGuardAccessor(compute_dtype)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['down_proj']._modules['base_layer'].compute_dtype == torch.bfloat16
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._modules['base_layer']._backward_hooks
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._modules['base_layer']._backward_pre_hooks
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'].compute_type_is_set, accessed_by=DictGetItemGuardAccessor(compute_type_is_set)
	| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['base_layer'].compute_type_is_set, 127873236421344)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout'], accessed_by=DictGetItemGuardAccessor(lora_dropout)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_dropout'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_dropout']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'], 105190279249888)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'].__dict__)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A'], accessed_by=DictGetItemGuardAccessor(lora_A)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_A'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DictGuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- KeyValueManager pair at index=0
	| | | | | | | | | | | +- KeyManager: GuardManager: source=list(L['self']._modules['down_proj']._modules['lora_A']._modules.keys())[0]
	| | | | | | | | | | | | +- EQUALS_MATCH: list(L['self']._modules['down_proj']._modules['lora_A']._modules.keys())[0] == 'default'
	| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_A']._modules['default'], 105190278900864)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['lora_A']._modules['default'].__dict__)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[32, 8192], stride=[8192, 1])
	| | | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['bias'], 127873236386624)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B'], accessed_by=DictGetItemGuardAccessor(lora_B)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_B'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_B']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_B']._modules['default'], 105190278900864)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['lora_B']._modules['default'].__dict__)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[2048, 32], stride=[32, 1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['bias'], 127873236386624)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_embedding_A'], accessed_by=DictGetItemGuardAccessor(lora_embedding_A)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_embedding_B'], accessed_by=DictGetItemGuardAccessor(lora_embedding_B)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_magnitude_vector'], accessed_by=DictGetItemGuardAccessor(lora_magnitude_vector)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj'].use_dora, accessed_by=DictGetItemGuardAccessor(use_dora)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj'].use_dora) == 1          
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj'].use_dora['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj'].use_dora['default'], 127873236421344)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._parameters             
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._backward_hooks         
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj'].merged_adapters, accessed_by=DictGetItemGuardAccessor(merged_adapters)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj'].merged_adapters, 127873236399136)
	| | | | | | | +- LENGTH_CHECK: not L['self']._modules['down_proj'].merged_adapters         
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._disable_adapters, accessed_by=DictGetItemGuardAccessor(_disable_adapters)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._disable_adapters, 127873236421344)
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._backward_pre_hooks     
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._active_adapter, accessed_by=DictGetItemGuardAccessor(_active_adapter)
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['down_proj']._active_adapter
	| | | | +- GuardManager: source=L['self']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor(act_fn)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['act_fn'], 105190279291392)
	| | | | | +- GuardManager: source=L['self']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['act_fn'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor(inplace)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['act_fn'].inplace, 127873236421344)
	| | | +- GuardManager: source=L['self']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | +- DICT_LENGTH: not L['self']._parameters                                   
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['F'], accessed_by=DictGetItemGuardAccessor(F)
	| | | +- ID_MATCH: ___check_obj_id(G['F'], 127867906986112)                    
	| | | +- GuardManager: source=G['F'].dequantize_4bit, accessed_by=GetAttrGuardAccessor(dequantize_4bit)
	| | | | +- GuardManager: source=G['F'].dequantize_4bit.__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | | +- ID_MATCH: ___check_obj_id(G['F'].dequantize_4bit.__code__, 127870597942896)
	| | | | +- GuardManager: source=G['F'].dequantize_4bit, accessed_by=FuncDefaultsGuardAccessor
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[1], accessed_by=GetItemGuardAccessor(1)
	| | | | | | +- ID_MATCH: ___check_obj_id(G['F'].dequantize_4bit.__defaults__[1], 127873236386624)
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[2], accessed_by=GetItemGuardAccessor(2)
	| | | | | | +- ID_MATCH: ___check_obj_id(G['F'].dequantize_4bit.__defaults__[2], 127873236386624)
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[3], accessed_by=GetItemGuardAccessor(3)
	| | | | | | +- EQUALS_MATCH: G['F'].dequantize_4bit.__defaults__[3] == 64                
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[4], accessed_by=GetItemGuardAccessor(4)
	| | | | | | +- EQUALS_MATCH: G['F'].dequantize_4bit.__defaults__[4] == 'fp4'             
	| | +- GuardManager: source=G['prod'], accessed_by=DictGetItemGuardAccessor(prod)
	| | | +- ID_MATCH: ___check_obj_id(G['prod'], 127873216027824)                 
	| | +- GuardManager: source=G['Params4bit'], accessed_by=DictGetItemGuardAccessor(Params4bit)
	| | | +- ID_MATCH: ___check_obj_id(G['Params4bit'], 105190296990752)           
	| | +- GuardManager: source=G['__import_kernels'], accessed_by=DictGetItemGuardAccessor(__import_kernels)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'], 127867504994128)     
	| | | +- GuardManager: source=G['__import_kernels'].DEBUG_FLAG, accessed_by=GetAttrGuardAccessor(DEBUG_FLAG)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'].DEBUG_FLAG, 127873236421344)
	| | | +- GuardManager: source=G['__import_kernels'].fused_dequantize_op, accessed_by=GetAttrGuardAccessor(fused_dequantize_op)
	| | | | +- TYPE_MATCH: ___check_type_id(G['__import_kernels'].fused_dequantize_op, 105190280786160)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'].fused_dequantize_op, 127867502131200)
	| | | | +- GuardManager: source=G['__import_kernels'].fused_dequantize_op._opoverload, accessed_by=GetAttrGuardAccessor(_opoverload)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'].fused_dequantize_op._opoverload, 127867502233600)
	| | | +- GuardManager: source=G['__import_kernels'].torch, accessed_by=GetAttrGuardAccessor(torch)
	| | | | +- OBJECT_ALIASING: G['__import_bitsandbytes_dot_nn_dot_modules'].torch is G['__import_kernels'].torch
	| | +- GuardManager: source=G['fused_dequantize'], accessed_by=DictGetItemGuardAccessor(fused_dequantize)
	| | | +- GuardManager: source=G['fused_dequantize'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize'].__code__, 105190312005664)
	| | +- GuardManager: source=G['ENABLE_ASSERTIONS'], accessed_by=DictGetItemGuardAccessor(ENABLE_ASSERTIONS)
	| | | +- ID_MATCH: ___check_obj_id(G['ENABLE_ASSERTIONS'], 127873236421344)    
	| | +- GuardManager: source=G['get_data_transposed'], accessed_by=DictGetItemGuardAccessor(get_data_transposed)
	| | | +- GuardManager: source=G['get_data_transposed'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | +- ID_MATCH: ___check_obj_id(G['get_data_transposed'].__code__, 105190312005664)
	| | +- GuardManager: source=G['TransposeBMatMul4Bit'], accessed_by=DictGetItemGuardAccessor(TransposeBMatMul4Bit)
	| | | +- ID_MATCH: ___check_obj_id(G['TransposeBMatMul4Bit'], 105190317042032) 
	| | | +- GuardManager: source=G['TransposeBMatMul4Bit'].forward, accessed_by=GetAttrGuardAccessor(forward)
	| | | | +- ID_MATCH: ___check_obj_id(G['TransposeBMatMul4Bit'].forward, 127867500433536)
	| | +- GuardManager: source=G['__builtins_dict___10'], accessed_by=DictGetItemGuardAccessor(__builtins_dict___10)
	| | | +- GuardManager: source=G['__builtins_dict___10']['any'], accessed_by=DictGetItemGuardAccessor(any)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['any'], 127873218839552)
	| | | +- GuardManager: source=G['__builtins_dict___10']['str'], accessed_by=DictGetItemGuardAccessor(str)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['str'], 127873236368832)
	| | | +- GuardManager: source=G['__builtins_dict___10']['bool'], accessed_by=DictGetItemGuardAccessor(bool)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['bool'], 127873236421376)
	| | | +- GuardManager: source=G['__builtins_dict___10']['super'], accessed_by=DictGetItemGuardAccessor(super)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['super'], 127873236378592)
	| | | +- GuardManager: source=G['__builtins_dict___10']['getattr'], accessed_by=DictGetItemGuardAccessor(getattr)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['getattr'], 127873218840592)
	| | | +- GuardManager: source=G['__builtins_dict___10']['isinstance'], accessed_by=DictGetItemGuardAccessor(isinstance)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['isinstance'], 127873218841152)
	| | +- GuardManager: source=G['fix_4bit_weight_quant_state_from_module'], accessed_by=DictGetItemGuardAccessor(fix_4bit_weight_quant_state_from_module)
	| | | +- GuardManager: source=G['fix_4bit_weight_quant_state_from_module'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | +- ID_MATCH: ___check_obj_id(G['fix_4bit_weight_quant_state_from_module'].__code__, 105190296796912)
	| | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'], accessed_by=DictGetItemGuardAccessor(__import_bitsandbytes_dot_nn_dot_modules)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'], 127867905299120)
	| | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch, accessed_by=GetAttrGuardAccessor(torch)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'].torch, 127872889574384)
	| | | | +- OBJECT_ALIASING: G['__import_bitsandbytes_dot_nn_dot_modules'].torch is G['torch']
	| | | | +- OBJECT_ALIASING: G['__import_bitsandbytes_dot_nn_dot_modules'].torch is G['__import_kernels'].torch
	| | | | +- OBJECT_ALIASING: G['__import_bitsandbytes_dot_nn_dot_modules'].torch is G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch
	| | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn, accessed_by=GetAttrGuardAccessor(nn)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn, 127869274618384)
	| | | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn.functional, accessed_by=GetAttrGuardAccessor(functional)
	| | | | | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn.functional, 127867930467264)
	| | | | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn.functional
	| | | | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn.functional.linear, accessed_by=GetAttrGuardAccessor(linear)
	| | | | | | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn.functional.linear, 127872854208768)
	| | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.numel, accessed_by=GetAttrGuardAccessor(numel)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'].torch.numel, 127872890415168)
	| | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.matmul, accessed_by=GetAttrGuardAccessor(matmul)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'].torch.matmul, 127872890441616)
	| | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.float16, accessed_by=GetAttrGuardAccessor(float16)
	| | | | | +- EQUALS_MATCH: G['__import_bitsandbytes_dot_nn_dot_modules'].torch.float16 == torch.float16
	| | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.float32, accessed_by=GetAttrGuardAccessor(float32)
	| | | | | +- EQUALS_MATCH: G['__import_bitsandbytes_dot_nn_dot_modules'].torch.float32 == torch.float32
	| | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.bfloat16, accessed_by=GetAttrGuardAccessor(bfloat16)
	| | | | | +- EQUALS_MATCH: G['__import_bitsandbytes_dot_nn_dot_modules'].torch.bfloat16 == torch.bfloat16
	| | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.is_autocast_enabled, accessed_by=GetAttrGuardAccessor(is_autocast_enabled)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'].torch.is_autocast_enabled, 127872890272512)
	| | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'], accessed_by=DictGetItemGuardAccessor(__import_peft_dot_tuners_dot_lora_dot_bnb)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_lora_dot_bnb'], 127867285689408)
	| | | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch, accessed_by=GetAttrGuardAccessor(torch)
	| | | | +- OBJECT_ALIASING: G['__import_bitsandbytes_dot_nn_dot_modules'].torch is G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch
	| | +- GuardManager: source=G['__import_peft_dot_tuners_dot_tuners_utils'], accessed_by=DictGetItemGuardAccessor(__import_peft_dot_tuners_dot_tuners_utils)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_tuners_utils'], 127867292952720)
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_linear)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'], 127867930467104)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F, accessed_by=GetAttrGuardAccessor(F)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F, 127867930467264)
	| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_torch_dot_nn_dot_modules_dot_activation'].F
	| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn.functional
	| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F.silu, accessed_by=GetAttrGuardAccessor(silu)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F.silu, 127867928434368)
	| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F.linear, accessed_by=GetAttrGuardAccessor(linear)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F.linear, 127872854208768)
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_module)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'], 127869274705968)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].Buffer, accessed_by=GetAttrGuardAccessor(Buffer)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].Buffer, 105190275668768)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].Module, accessed_by=GetAttrGuardAccessor(Module)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].Module, 105190277355504)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].Parameter, accessed_by=GetAttrGuardAccessor(Parameter)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].Parameter, 105190275662160)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_pre_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_pre_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_activation'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_activation)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_activation'], 127867928927008)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_activation'].F, accessed_by=GetAttrGuardAccessor(F)
	| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_torch_dot_nn_dot_modules_dot_activation'].F
	| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)
	| | | +- OBJECT_ALIASING: G['__import_bitsandbytes_dot_nn_dot_modules'].torch is G['torch']
	+- LAMBDA_GUARD: L['x'].stride()[0] == 2048*L['x'].size()[1]                   # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1] == 2048  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0] == 8192  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1] == 2048  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0] == 8192  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1] == 8192  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0] == 2048  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['x'].size()[1] <= 262143                               # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0326 23:29:19.015000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "a1b80a36db0c1cc4fe1201bfd0336237"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992159015118.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.015000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "af17e8ea68e8ec73ee0bbf2af6b61689"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992159015372.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.015000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "3/0", "frame_key": "6", "co_name": "compiled_llama_mlp", "co_filename": "/tmp/ipykernel_324469/318589162.py", "co_firstlineno": 10, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": 362, "shape_env_guard_count": 29, "graph_op_count": 32, "graph_node_count": 77, "graph_input_count": 38, "start_time": 1742992156.9157047, "entire_frame_compile_time_s": 2.0993568897247314, "backend_compile_time_s": 1.5913708209991455, "inductor_compile_time_s": 0.7059316635131836, "code_gen_time_s": 0.5937254428863525, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": ["mylib::fused_dequantize_op"], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:29:19.016000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "4c28670d09242653d2c77f26da4e7c87"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992159016581.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.123000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "156b2d0179602ea459ad270014ae7ac4"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992159123592.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.124000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "8899fca591257fd7abdfd45dc8b7e754"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992159124181.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.211000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "35e05d6e48df805744d513f6e201a5e0"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992159211085.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.211000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "78add353af961a6c2cccd1275e28d0fc"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992159211709.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.307000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "dfdf89ccdffbcbe9d1c6f2a994c0e143"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992159307129.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.308000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "8d3fda1fbf7e6311de95945bfa26b9a4"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992159307990.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.398000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "f91d2b0a0f408f363d1dca056d9d9e54"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992159398531.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.400000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "11c94906ca328731ef0ad638bb0517b7"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992159400004.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.502000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "d0a1e5a62f20e326a28ef9deb6f30298"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992159502153.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.502000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "78f23e0b40ed00b73598fa7d9c466c14"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992159502674.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.848000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "a421a03c82b4e633342e1d13818a6d6d"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992159848227.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:19.848000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "f3029a1ebcf9cc732fd48251ebd1ccb8"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992159848785.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:20.198000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "ab4a3e1d64a6ad7be4daf8e8daffdb5a"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992160198857.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:20.199000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "9383f4d9f668792257f1c0ca871b2dcf"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992160199390.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:20.532000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "154257a3d907c922632cfb2d858e1493"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992160532729.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:20.533000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "77c6fd5a596a875fb865a17caa132698"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992160533168.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:20.852000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "0fddf59c2d60d160ff48167198151310"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992160852259.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:20.922000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "be4d31bac6de3eb1db8c7a9f92dccf26"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992160922293.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.166000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "f6a62fa1183151de9c8288f09d5487f1"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992161166164.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.166000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "ad4a0dde59a570c186a365402ba7bb8f"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992161166799.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.261000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "1277c08ae2310ae0a8cca6ec3aedfed9"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992161261814.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.262000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "8888d116cc00dcefc66bdbdddec89ac1"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992161262377.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.348000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "7472730ffdf07af4a29f66084b90481d"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992161348184.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.348000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "1949e9972d2ec82bf6e1a858e86584e7"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992161348705.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.442000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "c99ca4106b0eae1b605b1f9919be1e41"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992161442755.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.443000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "07a46ab18b935458c9261d74e58ce8df"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992161443130.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.531000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "2d0e0db129c27780d5bf15d879d3f5c4"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992161531215.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.531000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "94ed8f1c349b9aab78ae7a6008332704"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992161531921.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.624000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "has_payload": "81d266d4d48825e77f7c77b617a22a4c"}
	{
	"name": "CachingAutotuner.benchmark_all_configs",
	"ts": 1742992161624249.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.626000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2817] {"artifact": {"name": "recompile_reasons", "encoding": "json"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "d37806bf77ab47f5baa6d814cca4993c"}
	[
	"2/1: tensor 'L['A']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA)",
	"2/0: tensor 'L['A']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA)"
	]
V0326 23:29:21.626000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 307, "name": "apply", "filename": 41}, {"line": 65, "name": "backward", "filename": 40}, {"line": 21, "name": "augmented_dequantize_4bit", "filename": 42}, {"line": 330, "name": "fused_dequantize", "filename": 43}]}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.626000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "38b8bfaac028223f1e50122488597df1"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992161626831.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.627000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "2f5b5997ec6239e4bd71b4c76bf6a893"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992161626831.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.629000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 63, "size": 2097152}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.629000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [2097152, 1], "is_leaf": true, "stride": [1, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6fa67430>", "describer_id": 63}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.630000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [1, 2097152], "is_leaf": true, "is_view": true, "stride": [1, 1], "storage": 0, "base": 1, "creation_meta": "CreationMeta.DEFAULT", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744afd7f4ff0>", "describer_id": 63}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.630000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 63, "id": 0, "source": "L['A']"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.633000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 63, "size": 65536}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.633000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [65536], "is_leaf": true, "stride": [1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c272df0>", "describer_id": 63}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.633000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 63, "id": 3, "source": "L['quant_state'].absmax"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.635000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 63, "size": 1024}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.635000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c272da0>", "describer_id": 63}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.635000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 63, "id": 4, "source": "L['quant_state'].state2.code"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.636000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 63, "size": 1024}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.636000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c272d00>", "describer_id": 63}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.636000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 63, "id": 5, "source": "L['quant_state'].state2.absmax"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.638000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 63, "size": 2}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.638000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c272d50>", "describer_id": 63}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.638000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 63, "id": 6, "source": "L['quant_state'].offset"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.639000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 63, "size": 64}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.639000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c272e40>", "describer_id": 63}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.639000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 63, "id": 7, "source": "L['quant_state'].code"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.644000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_a_": [1, "s1"], "l_quant_state_absmax": ["s2"], "l_quant_state_state2_code": [256], "l_quant_state_state2_absmax": ["s3"], "l_quant_state_offset": [], "l_quant_state_code": [16], "output_ptr": ["s4", 2048]}}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "7cb44ba18984348ae7f41158206ad8aa"}
	class GraphModule(torch.nn.Module):
	    def forward(self, s1: "Sym(s1)", L_A_: "u8[1, s1][1, 1]cuda:0", s2: "Sym(s2)", L_quant_state_absmax: "u8[s2][1]cuda:0", L_quant_state_state2_code: "f32[256][1]cuda:0", s3: "Sym(s3)", L_quant_state_state2_absmax: "f32[s3][1]cuda:0", L_quant_state_offset: "f16[][]cuda:0", L_quant_state_code: "f32[16][1]cuda:0", L_quant_state_shape_0_: "Sym(s4)"):
	        l_a_ = L_A_
	        l_quant_state_absmax = L_quant_state_absmax
	        l_quant_state_state2_code = L_quant_state_state2_code
	        l_quant_state_state2_absmax = L_quant_state_state2_absmax
	        l_quant_state_offset = L_quant_state_offset
	        l_quant_state_code = L_quant_state_code
	        l_quant_state_shape_0_ = L_quant_state_shape_0_
	        
	         # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:332 in fused_dequantize, code: n_elements = torch.numel(A) * 2
	        mul: "Sym(2*s1)" = s1 * 2;  s1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	        output_ptr: "bf16[s4, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_quant_state_absmax, l_quant_state_state2_code, l_quant_state_state2_absmax, l_quant_state_offset, l_a_, l_quant_state_code, mul, 256, 32, 16384, 8192, l_quant_state_shape_0_, 2048, torch.bfloat16);  l_quant_state_absmax = l_quant_state_state2_code = l_quant_state_state2_absmax = l_quant_state_offset = l_a_ = l_quant_state_code = mul = l_quant_state_shape_0_ = None
	        return (output_ptr,)
	        
V0326 23:29:21.645000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "89b2351ffa4eb0131281c8cde12eba39"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992161645002.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.645000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "a139818ec8083a3fcfa4e98bf234c981"}
	{
	"name": "backend_compile",
	"ts": 1742992161645002.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.647000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "f6a66fc618e5e8aea2408129c00774b3"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992161647551.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.661000 324469 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:215] {"aot_forward_graph": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "3d5c1887ea22b2eca1234c87f471283a"}
	class <lambda>(torch.nn.Module):
	    def forward(self, arg0_1: "Sym(s1)", arg1_1: "u8[1, s1][1, 1]cuda:0", arg2_1: "Sym(s2)", arg3_1: "u8[s2][1]cuda:0", arg4_1: "f32[256][1]cuda:0", arg5_1: "Sym(s3)", arg6_1: "f32[s3][1]cuda:0", arg7_1: "f16[][]cuda:0", arg8_1: "f32[16][1]cuda:0", arg9_1: "Sym(s4)"):
	         # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:332 in fused_dequantize, code: n_elements = torch.numel(A) * 2
	        mul: "Sym(2*s1)" = arg0_1 * 2;  arg0_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	        fused_dequantize_op: "bf16[s4, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, arg7_1, arg1_1, arg8_1, mul, 256, 32, 16384, 8192, arg9_1, 2048, torch.bfloat16);  arg3_1 = arg4_1 = arg6_1 = arg7_1 = arg1_1 = arg8_1 = mul = arg9_1 = None
	        return (fused_dequantize_op,)
	        
V0326 23:29:21.661000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "bdd730e8ef223008ba028da09d13fdeb"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992161661840.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.662000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "533003308932210d98b6e449560a1ed6"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992161662621.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.662000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "4548b9f7705079daaa34872ef7f6b3c5"}
	{
	"name": "inductor_compile",
	"ts": 1742992161662621.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.670000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/we/cwejxmelilirrhy2igx7nsmi6g5rwqrfsgnr4fcqgfkbrwlsjorv.py"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "fac5e3747c2228c1dbcfc8fd69f50258"}
	# AOT ID: ['4_inference']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/xa/cxaqzqrucz62vf6o3o6sl2ihhxptbp2wjf7gaoczyt27xbq6aopg.py
	# Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	# Source node to ATen node mapping:
	#   output_ptr => fused_dequantize_op
	# Graph fragment:
	#   %fused_dequantize_op : [num_users=1] = call_function[target=torch.ops.mylib.fused_dequantize_op.default](args = (%arg3_1, %arg4_1, %arg6_1, %arg7_1, %arg1_1, %arg8_1, %mul, 256, 32, 16384, 8192, %arg9_1, 2048, torch.bfloat16), kwargs = {})
	triton_poi_fused_fused_dequantize_op_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {2: 1}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=(2,))]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_fused_dequantize_op_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 1
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    tmp0 = tl.load(in_ptr0 + (0)).to(tl.float32)
	    tmp1 = tl.broadcast_to(tmp0, [XBLOCK])
	    tl.store(out_ptr0 + (tl.full([XBLOCK], 0, tl.int32)), tmp1, None)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1 = args
	    args.clear()
	    s1 = arg0_1
	    s2 = arg2_1
	    s3 = arg5_1
	    s4 = arg9_1
	    assert_size_stride(arg1_1, (1, s1), (1, 1))
	    assert_size_stride(arg3_1, (s2, ), (1, ))
	    assert_size_stride(arg4_1, (256, ), (1, ))
	    assert_size_stride(arg6_1, (s3, ), (1, ))
	    assert_size_stride(arg7_1, (), ())
	    assert_size_stride(arg8_1, (16, ), (1, ))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((), (), torch.float16)
	        # Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_fused_dequantize_op_0.run(arg7_1, buf0, 1, grid=grid(1), stream=stream0)
	        del arg7_1
	        # Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	        buf1 = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, buf0, arg1_1, arg8_1, 2*s1, 256, 32, 16384, 8192, s4, 2048, torch.bfloat16)
	        del arg1_1
	        del arg3_1
	        del arg4_1
	        del arg6_1
	        del arg8_1
	        del buf0
	        buf2 = buf1
	        del buf1
	    return (buf2, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    arg0_1 = 524288
	    arg1_1 = rand_strided((1, 524288), (1, 1), device='cuda:0', dtype=torch.uint8)
	    arg2_1 = 16384
	    arg3_1 = rand_strided((16384, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    arg4_1 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg5_1 = 64
	    arg6_1 = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg7_1 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    arg8_1 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg9_1 = 512
	    fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:29:21.671000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "55933b4ec8c4c57c8a995fd77129e6d9"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742992161671050.8,
	"args": {
	"key": "fodgjswdodzqgx3iq4x2v2ulrxwzzqobix2qlzyjko7fokxzlgfp",
	"components": [
	"[grf5bl6rpigmyiaz6c6m4gfmm52jce23fz2stipebcebees47ma] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1):\n    mul = arg0_1 * 2;  arg0_1 = None\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, arg7_1, arg1_1, arg8_1, mul, 256, 32, 16384, 8192, arg9_1, 2048, torch.bfloat16);  arg3_1 = arg4_1 = arg6_1 = arg7_1 = arg1_1 = arg8_1 = mul = arg9_1 = None\n    return (fused_dequantize_op,)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[tpvnjs264ct5lugh57xhauij37yhdwzbkxdyhhhhj5de2tskcds] example_inputs[0]: ('s1',)",
	"[7vwriczjssi3zjl7yvm6w4lda6j63tvcm76dxaglqafuowfzcmv] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([1, s1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[2]: ('s2',)",
	"[sqttp5i5wnsdcripsqrsj7sbdbvyr6p2v547c3del3vagfumq23] example_inputs[3]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([s2]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[4]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[5]: ('s3',)",
	"[osj4eueidclo5irb65z7qycfmcyzhzy3mt2msh2p7b3yhteohpt] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([s3]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[j23zbqrguggah6nscnzj4y55d5chhoardpgvlji4ilrfwrr6asz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[2ylzn3y4vcv2t7rbq447jvkl6by5qf32ycwks3h2nbcmm6wvmnk] example_inputs[8]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ioa426km4idgc234gbdjtmikotjmsh7hn3tdqahy4f4zia42h53] example_inputs[9]: ('s4',)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []",
	"[jan4rfottd4caig5fc5h3ieb5ephfyhn7rn27jmu2ehbwgssm5j] fx_kwargs[user_visible_outputs]: {'fused_dequantize_op': None}",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1",
	"[kcuxe2zwm3mzv2uk6adm6iskoy35bqfv725twacrdewod2dbl5d] inputs_to_check[1]: 3",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[2]: 4",
	"[agkvbkaha53nbz3aeeuhvxjvvc4glhfjofzkg6g2qjoo2e5otcx] inputs_to_check[3]: 6",
	"[j3s5elu6itwgjafc7rzhy4whrbufl6kfmlufjhh25grt643bk5f] inputs_to_check[4]: 7",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inputs_to_check[5]: 8",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 36553287,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:29:21.671000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "479c7fc7b4aeb8b6d83b60fc11accd35"}
	{"key": "fodgjswdodzqgx3iq4x2v2ulrxwzzqobix2qlzyjko7fokxzlgfp", "components": ["[grf5bl6rpigmyiaz6c6m4gfmm52jce23fz2stipebcebees47ma] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1):\n    mul = arg0_1 * 2;  arg0_1 = None\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, arg7_1, arg1_1, arg8_1, mul, 256, 32, 16384, 8192, arg9_1, 2048, torch.bfloat16);  arg3_1 = arg4_1 = arg6_1 = arg7_1 = arg1_1 = arg8_1 = mul = arg9_1 = None\n    return (fused_dequantize_op,)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[tpvnjs264ct5lugh57xhauij37yhdwzbkxdyhhhhj5de2tskcds] example_inputs[0]: ('s1',)", "[7vwriczjssi3zjl7yvm6w4lda6j63tvcm76dxaglqafuowfzcmv] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([1, s1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[2]: ('s2',)", "[sqttp5i5wnsdcripsqrsj7sbdbvyr6p2v547c3del3vagfumq23] example_inputs[3]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([s2]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[4]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[5]: ('s3',)", "[osj4eueidclo5irb65z7qycfmcyzhzy3mt2msh2p7b3yhteohpt] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([s3]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[j23zbqrguggah6nscnzj4y55d5chhoardpgvlji4ilrfwrr6asz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[2ylzn3y4vcv2t7rbq447jvkl6by5qf32ycwks3h2nbcmm6wvmnk] example_inputs[8]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ioa426km4idgc234gbdjtmikotjmsh7hn3tdqahy4f4zia42h53] example_inputs[9]: ('s4',)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []", "[jan4rfottd4caig5fc5h3ieb5ephfyhn7rn27jmu2ehbwgssm5j] fx_kwargs[user_visible_outputs]: {'fused_dequantize_op': None}", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1", "[kcuxe2zwm3mzv2uk6adm6iskoy35bqfv725twacrdewod2dbl5d] inputs_to_check[1]: 3", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[2]: 4", "[agkvbkaha53nbz3aeeuhvxjvvc4glhfjofzkg6g2qjoo2e5otcx] inputs_to_check[3]: 6", "[j3s5elu6itwgjafc7rzhy4whrbufl6kfmlufjhh25grt643bk5f] inputs_to_check[4]: 7", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inputs_to_check[5]: 8", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 36553287, "cache_state": "hit"}
V0326 23:29:21.671000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "4cbee28ccfa6c9bc8e82e371b4a888de"}
	{
	"name": "inductor_compile",
	"ts": 1742992161671963.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 7,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.672000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "3769900303af22e6d792e2588e04661e"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992161672210.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 7,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.672000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "2272e73ec0d310981999fc7d46f4e145"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992161672606.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 7,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.673000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "5ab0e036f0a602686b7d4f53812589c8"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992161673826.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 7,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.674000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "3c61786271bd40e0c53afec2bf7dd60b"}
	{
	"name": "backend_compile",
	"ts": 1742992161674165.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 7,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.674000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "eae3b07efe2a1d143eff420f1c8922b0"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992161674410.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 7,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.680000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "43fb6808edf3a5f62f00fc2da0696bfd"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['A'], accessed_by=DictGetItemGuardAccessor(A)
	| | +- TENSOR_MATCH: check_tensor(L['A'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.uint8, device=0, requires_grad=False, size=[1, None], stride=[1, 1])
	| | +- NO_HASATTR: hasattr(L['A'], '_dynamo_dynamic_indices') == False         
	| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['A'], L['quant_state'].code, L['quant_state'].absmax, L['quant_state'].offset, L['quant_state'].state2.code, L['quant_state'].state2.absmax)
	| +- GuardManager: source=L['quant_state'], accessed_by=DictGetItemGuardAccessor(quant_state)
	| | +- TYPE_MATCH: ___check_type_id(L['quant_state'], 105190295709968)         
	| | +- GuardManager: source=L['quant_state'].code, accessed_by=GetAttrGuardAccessor(code)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].code, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | +- EQUALS_MATCH: L['quant_state'].dtype == torch.bfloat16                    
	| | +- GuardManager: source=L['quant_state'].shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].shape, 127872448968448)   
	| | | +- LENGTH_CHECK: len(L['quant_state'].shape) == 2                            
	| | | +- GuardManager: source=L['quant_state'].shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].shape[0], 127873236398144)
	| | | +- GuardManager: source=L['quant_state'].shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | +- EQUALS_MATCH: L['quant_state'].shape[1] == 2048                           
	| | +- GuardManager: source=L['quant_state'].absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.uint8, device=0, requires_grad=False, size=[None], stride=[1])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].absmax, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].offset, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].state2, 105190295709968)  
	| | | +- GuardManager: source=L['quant_state'].state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | +- TENSOR_MATCH: check_tensor(L['quant_state'].state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | +- NO_HASATTR: hasattr(L['quant_state'].state2.code, '_dynamo_dynamic_indices') == False
	| | | | +- NO_TENSOR_ALIASING
	| | | +- GuardManager: source=L['quant_state'].state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | +- TENSOR_MATCH: check_tensor(L['quant_state'].state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[None], stride=[1])
	| | | | +- NO_HASATTR: hasattr(L['quant_state'].state2.absmax, '_dynamo_dynamic_indices') == False
	| | | | +- NO_TENSOR_ALIASING
	| | | +- GuardManager: source=L['quant_state'].state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | +- EQUALS_MATCH: L['quant_state'].state2.blocksize == 256                    
	| | +- GuardManager: source=L['quant_state'].blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | +- EQUALS_MATCH: L['quant_state'].blocksize == 64                            
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)
	| | | +- ID_MATCH: ___check_obj_id(G['torch'], 127872889574384)                
	| | | +- GuardManager: source=G['torch'].numel, accessed_by=GetAttrGuardAccessor(numel)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].numel, 127872890415168)          
	| | +- GuardManager: source=G['DEBUG_FLAG'], accessed_by=DictGetItemGuardAccessor(DEBUG_FLAG)
	| | | +- ID_MATCH: ___check_obj_id(G['DEBUG_FLAG'], 127873236421344)           
	| | +- GuardManager: source=G['fused_dequantize_op'], accessed_by=DictGetItemGuardAccessor(fused_dequantize_op)
	| | | +- TYPE_MATCH: ___check_type_id(G['fused_dequantize_op'], 105190280786160) 
	| | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize_op'], 127867502131200)  
	| | | +- GuardManager: source=G['fused_dequantize_op']._opoverload, accessed_by=GetAttrGuardAccessor(_opoverload)
	| | | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize_op']._opoverload, 127867502233600)
	+- LAMBDA_GUARD: Ne(2048*L['quant_state'].shape[0], 0)                         # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: Ne(L['quant_state'].shape[0], 1)                              # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2048*L['quant_state'].shape[0] >= 2                           # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['A'].size()[1]                                         # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['quant_state'].absmax.size()[0]                        # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['quant_state'].state2.absmax.size()[0]                 # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 0 <= L['quant_state'].shape[0]                                # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0326 23:29:21.680000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "b840927dbc7e0ae2f0d9015d6a53a6ed"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992161680886.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 7,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.681000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "120129f815e89b751ec1853364af639a"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992161681137.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 7,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.681000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "2/2", "frame_key": "7", "co_name": "fused_dequantize", "co_filename": "/home/tom/unsloth-challenges/submit/Problem_C/kernels.py", "co_firstlineno": 330, "cache_size": 2, "accumulated_cache_size": 2, "guard_count": 25, "shape_env_guard_count": 6, "graph_op_count": 2, "graph_node_count": 13, "graph_input_count": 10, "start_time": 1742992161.6268256, "entire_frame_compile_time_s": 0.0540163516998291, "backend_compile_time_s": 0.02913355827331543, "inductor_compile_time_s": 0.009302139282226562, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": ["mylib::fused_dequantize_op"], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:29:21.722000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2817] {"artifact": {"name": "recompile_reasons", "encoding": "json"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "486232f21199017e96d85a7a1d776209"}
	[
	"3/0: ___check_obj_id(L['self']._modules['gate_proj']._modules['base_layer'].compute_type_is_set, 127873236421344)"
	]
V0326 23:29:21.723000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 10, "name": "compiled_llama_mlp", "filename": 44}]}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.724000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "efb09c9f18447b8d24becb21de62c652"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992161724068.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.724000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "10c10a82477cb5e844bfdeb0d754f57c"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992161724068.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.736000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 65, "size": 1064960}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.736000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 3, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [1, 260, 2048], "requires_grad": true, "stride": [532480, 2048, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744afd7eeb70>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.737000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 0, "source": "L['x']"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.744000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 65, "size": 8388608}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.744000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [8388608, 1], "is_leaf": true, "stride": [1, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6fa65590>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.744000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 1, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.751000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 65, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.752000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [262144], "is_leaf": true, "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c246760>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.752000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 2, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.753000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 65, "size": 1024}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.753000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c247480>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.753000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 3, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.754000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 65, "size": 4096}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.754000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024], "is_leaf": true, "stride": [1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c360230>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.754000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 4, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.755000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 65, "size": 2}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.755000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c362530>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.755000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 5, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.756000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 6, "describer_id": 65, "size": 64}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.756000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 6, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c247f70>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.757000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 6, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.798000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 7, "describer_id": 65, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.799000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 8, "describer_id": 65, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.799000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 19, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 2048], "is_leaf": true, "stride": [2048, 1], "storage": 8, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744afd8fe030>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.799000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 18, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 2048], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2048, 1], "storage": 7, "grad": 19, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x744b6c23c5a0>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.799000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 18, "source": "L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.804000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s1", "sources": ["L['x'].size()[2]"], "value": "2048", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 897, "name": "call_function", "filename": 48}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 271, "name": "_fn", "filename": 49}, {"line": 4364, "name": "matmul", "filename": 50}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 2013, "name": "_dispatch_impl", "filename": 32}, {"line": 716, "name": "__call__", "filename": 51}, {"line": 273, "name": "_fn", "filename": 49}, {"line": 2100, "name": "meta_mm", "filename": 52}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 518, "name": "forward", "filename": 39}, {"line": 125, "name": "forward", "filename": 53}]}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.807000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 9, "describer_id": 65, "size": 1048576}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.807000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 10, "describer_id": 65, "size": 1048576}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.807000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 24, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [8192, 32], "is_leaf": true, "stride": [32, 1], "storage": 10, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744afd79d810>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.808000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 23, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [8192, 32], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [32, 1], "storage": 9, "grad": 24, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x744b6c245f90>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.808000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 23, "source": "L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.812000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s5", "sources": ["L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0]"], "value": "8192", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2220, "name": "BINARY_OP", "filename": 27}, {"line": 301, "name": "impl", "filename": 27}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 943, "name": "_handle_insert_op_in_graph", "filename": 28}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 1924, "name": "_dispatch_impl", "filename": 32}, {"line": 831, "name": "fast_binary_impl", "filename": 33}, {"line": 785, "name": "infer_size", "filename": 33}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 536, "name": "forward", "filename": 39}]}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.829000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 11, "describer_id": 65, "size": 8388608}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.829000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 28, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [8388608, 1], "is_leaf": true, "stride": [1, 1], "storage": 11, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6fa64f50>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.830000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 28, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.836000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 12, "describer_id": 65, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.836000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 29, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [262144], "is_leaf": true, "stride": [1], "storage": 12, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c2504b0>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.836000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 29, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.837000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 13, "describer_id": 65, "size": 1024}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.837000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 30, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 13, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c35e300>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.838000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 30, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.838000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 14, "describer_id": 65, "size": 4096}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.838000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 31, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024], "is_leaf": true, "stride": [1], "storage": 14, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c35dd60>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.839000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 31, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.839000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 15, "describer_id": 65, "size": 2}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.840000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 32, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 15, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c35d6d0>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.840000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 32, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.840000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 16, "describer_id": 65, "size": 64}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.841000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 33, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 16, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c252620>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.841000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 33, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.856000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s8", "sources": ["L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1]"], "value": "2048", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 1680, "name": "CALL_FUNCTION_EX", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 1024, "name": "call_function", "filename": 54}, {"line": 774, "name": "call_method", "filename": 54}, {"line": 699, "name": "call_apply", "filename": 54}, {"line": 2015, "name": "call_function", "filename": 55}, {"line": 462, "name": "speculate_subgraph", "filename": 55}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 897, "name": "call_function", "filename": 48}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 271, "name": "_fn", "filename": 49}, {"line": 4364, "name": "matmul", "filename": 50}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 2013, "name": "_dispatch_impl", "filename": 32}, {"line": 716, "name": "__call__", "filename": 51}, {"line": 273, "name": "_fn", "filename": 49}, {"line": 2100, "name": "meta_mm", "filename": 52}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 496, "name": "forward", "filename": 39}, {"line": 110, "name": "inner_transpose_forward", "filename": 40}, {"line": 34, "name": "forward", "filename": 40}]}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.877000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 17, "describer_id": 65, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.878000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 18, "describer_id": 65, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.878000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 46, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 2048], "is_leaf": true, "stride": [2048, 1], "storage": 18, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744afd7db660>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.878000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 45, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 2048], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2048, 1], "storage": 17, "grad": 46, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x744b6c247660>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.878000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 45, "source": "L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.881000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 19, "describer_id": 65, "size": 1048576}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.882000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 20, "describer_id": 65, "size": 1048576}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.882000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 51, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [8192, 32], "is_leaf": true, "stride": [32, 1], "storage": 20, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744afd7d8c30>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.882000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 50, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [8192, 32], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [32, 1], "storage": 19, "grad": 51, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x744b6c2459f0>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.882000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 50, "source": "L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.887000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s9", "sources": ["L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0]"], "value": "8192", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2220, "name": "BINARY_OP", "filename": 27}, {"line": 301, "name": "impl", "filename": 27}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 943, "name": "_handle_insert_op_in_graph", "filename": 28}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 1924, "name": "_dispatch_impl", "filename": 32}, {"line": 831, "name": "fast_binary_impl", "filename": 33}, {"line": 785, "name": "infer_size", "filename": 33}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 536, "name": "forward", "filename": 39}]}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.899000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 21, "describer_id": 65, "size": 8388608}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.899000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 55, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [8388608, 1], "is_leaf": true, "stride": [1, 1], "storage": 21, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6fa65270>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.899000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 55, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.905000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 22, "describer_id": 65, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.905000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 56, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [262144], "is_leaf": true, "stride": [1], "storage": 22, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6fa3be30>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.906000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 56, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.906000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 23, "describer_id": 65, "size": 1024}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.906000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 57, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 23, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b78f51db0>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.907000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 57, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.907000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 24, "describer_id": 65, "size": 4096}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.908000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 58, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024], "is_leaf": true, "stride": [1], "storage": 24, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c24d720>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.908000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 58, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.908000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 25, "describer_id": 65, "size": 2}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.908000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 59, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 25, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c24ddb0>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.909000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 59, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.909000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 26, "describer_id": 65, "size": 64}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.909000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 60, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 26, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744b6c390e60>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.910000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 60, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.921000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s12", "sources": ["L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1]"], "value": "8192", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 1680, "name": "CALL_FUNCTION_EX", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 1024, "name": "call_function", "filename": 54}, {"line": 774, "name": "call_method", "filename": 54}, {"line": 699, "name": "call_apply", "filename": 54}, {"line": 2015, "name": "call_function", "filename": 55}, {"line": 462, "name": "speculate_subgraph", "filename": 55}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 897, "name": "call_function", "filename": 48}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 271, "name": "_fn", "filename": 49}, {"line": 4364, "name": "matmul", "filename": 50}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 2013, "name": "_dispatch_impl", "filename": 32}, {"line": 716, "name": "__call__", "filename": 51}, {"line": 273, "name": "_fn", "filename": 49}, {"line": 2100, "name": "meta_mm", "filename": 52}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 496, "name": "forward", "filename": 39}, {"line": 110, "name": "inner_transpose_forward", "filename": 40}, {"line": 34, "name": "forward", "filename": 40}]}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.941000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 27, "describer_id": 65, "size": 1048576}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.941000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 28, "describer_id": 65, "size": 1048576}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.941000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 73, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 8192], "is_leaf": true, "stride": [8192, 1], "storage": 28, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744afd63d6d0>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.942000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 72, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 8192], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [8192, 1], "storage": 27, "grad": 73, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x744b6c27c500>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.942000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 72, "source": "L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.945000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 29, "describer_id": 65, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.945000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 30, "describer_id": 65, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.946000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 78, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [2048, 32], "is_leaf": true, "stride": [32, 1], "storage": 30, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x744afd63de00>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.946000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 77, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [2048, 32], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [32, 1], "storage": 29, "grad": 78, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x744b6faa4190>", "describer_id": 65}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.946000 324469 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 65, "id": 77, "source": "L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.950000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s13", "sources": ["L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0]"], "value": "2048", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2220, "name": "BINARY_OP", "filename": 27}, {"line": 301, "name": "impl", "filename": 27}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 943, "name": "_handle_insert_op_in_graph", "filename": 28}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 1924, "name": "_dispatch_impl", "filename": 32}, {"line": 831, "name": "fast_binary_impl", "filename": 33}, {"line": 785, "name": "infer_size", "filename": 33}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 536, "name": "forward", "filename": 39}]}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.954000 324469 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s4", "sources": ["L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1]"], "value": "2048", "reason": "find", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2987, "name": "RETURN_VALUE", "filename": 27}, {"line": 2972, "name": "_return", "filename": 27}, {"line": 1117, "name": "compile_subgraph", "filename": 56}, {"line": 1317, "name": "compile_and_call_fx_graph", "filename": 56}, {"line": 287, "name": "insert_deferred_runtime_asserts", "filename": 57}, {"line": 280, "name": "match_symbol", "filename": 57}, {"line": 169, "name": "expr", "filename": 36}, {"line": 1532, "name": "wrapper", "filename": 35}, {"line": 4572, "name": "replace", "filename": 35}, {"line": 1532, "name": "wrapper", "filename": 35}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 4885, "name": "_find", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": null}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:21.964000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_x_": [1, "s0", "2048"], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data": [8388608, 1], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax": [262144], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code": [256], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax": [1024], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset": [], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code": [16], "l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_": [32, 2048], "l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_": [8192, 32], "l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data": [8388608, 1], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax": [262144], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code": [256], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax": [1024], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset": [], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code": [16], "l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_": [32, 2048], "l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_": [8192, 32], "l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data": [8388608, 1], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax": [262144], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code": [256], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax": [1024], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset": [], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code": [16], "l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_": [32, 8192], "l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_": [2048, 32], "x": [1, "s0", "2048"], "B": [1, 8388608], "autograd_function_apply": [1, "s0", "8192"], "result": [1, "s0", "8192"], "result_1": [1, "s0", "8192"], "linear": [1, "s0", 32], "linear_1": [1, "s0", 8192], "output": [1, "s0", 8192], "result_2": [1, "s0", 8192], "silu": [1, "s0", 8192], "x_1": [1, "s0", 2048], "B_1": [1, 8388608], "autograd_function_apply_1": [1, "s0", "8192"], "result_3": [1, "s0", "8192"], "result_4": [1, "s0", "8192"], "linear_2": [1, "s0", 32], "linear_3": [1, "s0", 8192], "output_1": [1, "s0", 8192], "result_5": [1, "s0", 8192], "mul_2": [1, "s0", 8192], "x_2": [1, "s0", 8192], "B_2": [1, 8388608], "autograd_function_apply_2": [1, "s0", "2048"], "result_6": [1, "s0", "2048"], "result_7": [1, "s0", "2048"], "linear_4": [1, "s0", 32], "linear_5": [1, "s0", 2048], "output_2": [1, "s0", 2048], "result_8": [1, "s0", 2048]}}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "b14c1f4a858a8cd2ba579afbc0427fea"}
	class GraphModule(torch.nn.Module):
	    def forward(self, s0: "Sym(s0)", L_x_: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data: "u8[8388608, 1][1, 1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s2)", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s3)", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", L_self_modules_gate_proj_modules_lora_A_modules_default_parameters_weight_: "f32[32, 2048][2048, 1]cuda:0", L_self_modules_gate_proj_modules_lora_B_modules_default_parameters_weight_: "f32[8192, 32][32, 1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_some_data: "u8[8388608, 1][1, 1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s6)", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s7)", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", L_self_modules_up_proj_modules_lora_A_modules_default_parameters_weight_: "f32[32, 2048][2048, 1]cuda:0", L_self_modules_up_proj_modules_lora_B_modules_default_parameters_weight_: "f32[8192, 32][32, 1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_some_data: "u8[8388608, 1][1, 1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s10)", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s11)", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(8192)", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(2048)", L_self_modules_down_proj_modules_lora_A_modules_default_parameters_weight_: "f32[32, 8192][8192, 1]cuda:0", L_self_modules_down_proj_modules_lora_B_modules_default_parameters_weight_: "f32[2048, 32][32, 1]cuda:0"):
	        l_x_ = L_x_
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data = L_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_
	        l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_ = L_self_modules_gate_proj_modules_lora_A_modules_default_parameters_weight_
	        l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_ = L_self_modules_gate_proj_modules_lora_B_modules_default_parameters_weight_
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data = L_self_modules_up_proj_modules_base_layer_parameters_weight_some_data
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_
	        l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_ = L_self_modules_up_proj_modules_lora_A_modules_default_parameters_weight_
	        l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_ = L_self_modules_up_proj_modules_lora_B_modules_default_parameters_weight_
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data = L_self_modules_down_proj_modules_base_layer_parameters_weight_some_data
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_
	        l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_ = L_self_modules_down_proj_modules_lora_A_modules_default_parameters_weight_
	        l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_ = L_self_modules_down_proj_modules_lora_B_modules_default_parameters_weight_
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        x: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = l_x_.to(torch.bfloat16)
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        B: "u8[1, 8388608][1, 1]cuda:0" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data.t();  l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        function_ctx = torch.autograd.function.FunctionCtx();  function_ctx = None
	        fwd_body_0 = self.fwd_body_0
	        bwd_body_0 = self.bwd_body_0
	        autograd_function_apply: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.higher_order.autograd_function_apply(fwd_body_0, bwd_body_0, x, B, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, args_tensor_mask = [True, True, False, False, False], non_differentiable_idx = []);  fwd_body_0 = bwd_body_0 = x = B = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	        result: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = autograd_function_apply.to(torch.float16);  autograd_function_apply = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        result_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result.clone();  result = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        linear: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_, None);  l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_ = None
	        linear_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(linear, l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_, None);  linear = l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_ = None
	        output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = linear_1 * 2.0;  linear_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        result_2: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result_1 + output;  result_1 = output = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        silu: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.nn.functional.silu(result_2, inplace = False);  result_2 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        x_1: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = l_x_.to(torch.bfloat16)
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        B_1: "u8[1, 8388608][1, 1]cuda:0" = l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data.t();  l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        function_ctx_1 = torch.autograd.function.FunctionCtx();  function_ctx_1 = None
	        fwd_body_1 = self.fwd_body_1
	        bwd_body_1 = self.bwd_body_1
	        autograd_function_apply_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.higher_order.autograd_function_apply(fwd_body_1, bwd_body_1, x_1, B_1, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, args_tensor_mask = [True, True, False, False, False], non_differentiable_idx = []);  fwd_body_1 = bwd_body_1 = x_1 = B_1 = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	        result_3: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = autograd_function_apply_1.to(torch.float16);  autograd_function_apply_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        result_4: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result_3.clone();  result_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        linear_2: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_, None);  l_x_ = l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_ = None
	        linear_3: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(linear_2, l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_, None);  linear_2 = l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_ = None
	        output_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = linear_3 * 2.0;  linear_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        result_5: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result_4 + output_1;  result_4 = output_1 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_2: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = silu * result_5;  silu = result_5 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        x_2: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = mul_2.to(torch.bfloat16)
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        B_2: "u8[1, 8388608][1, 1]cuda:0" = l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data.t();  l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        function_ctx_2 = torch.autograd.function.FunctionCtx();  function_ctx_2 = None
	        fwd_body_2 = self.fwd_body_2
	        bwd_body_2 = self.bwd_body_2
	        autograd_function_apply_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.higher_order.autograd_function_apply(fwd_body_2, bwd_body_2, x_2, B_2, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, args_tensor_mask = [True, True, False, False, False], non_differentiable_idx = []);  fwd_body_2 = bwd_body_2 = x_2 = B_2 = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	        result_6: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = autograd_function_apply_2.to(torch.float16);  autograd_function_apply_2 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        result_7: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = result_6.clone();  result_6 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        linear_4: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch._C._nn.linear(mul_2, l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_, None);  mul_2 = l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_ = None
	        linear_5: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch._C._nn.linear(linear_4, l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_, None);  linear_4 = l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_ = None
	        output_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = linear_5 * 2.0;  linear_5 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        result_8: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = result_7 + output_2;  result_7 = output_2 = None
	        return (result_8,)
	        
	    class fwd_body_0(torch.nn.Module):
	        def forward(self, ctx, A: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", B: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s3)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s2)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            a = A
	            b = B
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:21 in forward, code: if prod(A.shape) == 0:
	            size = a.size()
	            getitem = size[0];  getitem = None
	            getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
	            getitem_2: "Sym(2048)" = size[2];  getitem_2 = None
	            prod: "Sym(2048*s0)" = math_prod(size);  size = None
	            eq: "Sym(False)" = prod == 0;  prod = eq = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s3/2))" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s2*s3)" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s2*s3/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  rshift = mul = rshift_1 = None
	            
	             # File: /tmp/ipykernel_324469/636066363.py:21 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:34 in forward, code: output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias) # NOTE the transposition
	            to: "bf16[2048, 8192][1, 2048]cuda:0" = t.to(torch.bfloat16);  t = None
	            t_1: "bf16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(a, t_1, None);  a = t_1 = None
	            return (output, [l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_])
	            
	    class bwd_body_0(torch.nn.Module):
	        def forward(self, ctx, grad_output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s3)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s2)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", b: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            # No stacktrace found for following nodes
	            _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s3/2))" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s2*s3)" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize;  l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s2*s3/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset = b = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = rshift = mul = rshift_1 = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	            
	             # File: /tmp/ipykernel_324469/636066363.py:21 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:65 in backward, code: grad_A = torch.matmul(grad_output, F.dequantize_4bit(B, ctx.state).to(grad_output.dtype).t()) # NOTE the transposition
	            to: "f16[2048, 8192][1, 2048]cuda:0" = t.to(torch.float16);  t = None
	            t_1: "f16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            grad_A: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.matmul(grad_output, t_1);  grad_output = t_1 = None
	            
	            # No stacktrace found for following nodes
	            _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None
	            return (grad_A, None)
	            
	    class fwd_body_1(torch.nn.Module):
	        def forward(self, ctx, A: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", B: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s7)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s6)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            a = A
	            b = B
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:21 in forward, code: if prod(A.shape) == 0:
	            size = a.size()
	            getitem = size[0];  getitem = None
	            getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
	            getitem_2 = size[2];  getitem_2 = None
	            prod: "Sym(2048*s0)" = math_prod(size);  size = None
	            eq: "Sym(False)" = prod == 0;  prod = eq = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s7/2))" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s6*s7)" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s6*s7/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  rshift = mul = rshift_1 = None
	            
	             # File: /tmp/ipykernel_324469/636066363.py:21 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:34 in forward, code: output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias) # NOTE the transposition
	            to: "bf16[2048, 8192][1, 2048]cuda:0" = t.to(torch.bfloat16);  t = None
	            t_1: "bf16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(a, t_1, None);  a = t_1 = None
	            return (output, [l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_])
	            
	    class bwd_body_1(torch.nn.Module):
	        def forward(self, ctx, grad_output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s7)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s6)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", b: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            # No stacktrace found for following nodes
	            _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s7/2))" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s6*s7)" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize;  l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s6*s7/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset = b = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = rshift = mul = rshift_1 = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	            
	             # File: /tmp/ipykernel_324469/636066363.py:21 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:65 in backward, code: grad_A = torch.matmul(grad_output, F.dequantize_4bit(B, ctx.state).to(grad_output.dtype).t()) # NOTE the transposition
	            to: "f16[2048, 8192][1, 2048]cuda:0" = t.to(torch.float16);  t = None
	            t_1: "f16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            grad_A: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.matmul(grad_output, t_1);  grad_output = t_1 = None
	            
	            # No stacktrace found for following nodes
	            _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None
	            return (grad_A, None)
	            
	    class fwd_body_2(torch.nn.Module):
	        def forward(self, ctx, A: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", B: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s11)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s10)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(2048)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(8192)"):
	            a = A
	            b = B
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:21 in forward, code: if prod(A.shape) == 0:
	            size = a.size()
	            getitem = size[0];  getitem = None
	            getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
	            getitem_2 = size[2];  getitem_2 = None
	            prod: "Sym(8192*s0)" = math_prod(size);  size = None
	            eq: "Sym(False)" = prod == 0;  prod = eq = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s11/2))" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s10*s11)" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s10*s11/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  rshift = mul = rshift_1 = None
	            
	             # File: /tmp/ipykernel_324469/636066363.py:21 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[8192, 2048][1, 8192]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:34 in forward, code: output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias) # NOTE the transposition
	            to: "bf16[8192, 2048][1, 8192]cuda:0" = t.to(torch.bfloat16);  t = None
	            t_1: "bf16[2048, 8192][8192, 1]cuda:0" = to.t();  to = None
	            output: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch._C._nn.linear(a, t_1, None);  a = t_1 = None
	            return (output, [l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_])
	            
	    class bwd_body_2(torch.nn.Module):
	        def forward(self, ctx, grad_output: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s11)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s10)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", b: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(2048)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(8192)"):
	            # No stacktrace found for following nodes
	            _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s11/2))" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s10*s11)" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize;  l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s10*s11/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset = b = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = rshift = mul = rshift_1 = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	            
	             # File: /tmp/ipykernel_324469/636066363.py:21 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[8192, 2048][1, 8192]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_324469/2489781049.py:65 in backward, code: grad_A = torch.matmul(grad_output, F.dequantize_4bit(B, ctx.state).to(grad_output.dtype).t()) # NOTE the transposition
	            to: "f16[8192, 2048][1, 8192]cuda:0" = t.to(torch.float16);  t = None
	            t_1: "f16[2048, 8192][8192, 1]cuda:0" = to.t();  to = None
	            grad_A: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.matmul(grad_output, t_1);  grad_output = t_1 = None
	            
	            # No stacktrace found for following nodes
	            _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None
	            return (grad_A, None)
	            
V0326 23:29:21.964000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "aefe1dcc458c810805e95b770eae963c"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992161964819.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.965000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "f34fd2da4d324fc27afa493d1a35b88c"}
	{
	"name": "backend_compile",
	"ts": 1742992161964819.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:21.978000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "f298a54feecab274f033bc1e79a3b549"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992161978593.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.224000 324469 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "249aa606ddadedc59d4b20c6a60fdc51"}
	class joint_helper(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "Sym(s0)"; primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"; primals_3: "u8[8388608, 1][1, 1]cuda:0"; primals_4: "Sym(s2)"; primals_5: "Sym(s3)"; primals_6: "u8[262144][1]cuda:0"; primals_7: "f32[256][1]cuda:0"; primals_8: "f32[1024][1]cuda:0"; primals_9: "f16[][]cuda:0"; primals_10: "f32[16][1]cuda:0"; primals_11: "Sym(2048)"; primals_12: "Sym(8192)"; primals_13: "f32[32, 2048][2048, 1]cuda:0"; primals_14: "f32[8192, 32][32, 1]cuda:0"; primals_15: "u8[8388608, 1][1, 1]cuda:0"; primals_16: "Sym(s6)"; primals_17: "Sym(s7)"; primals_18: "u8[262144][1]cuda:0"; primals_19: "f32[256][1]cuda:0"; primals_20: "f32[1024][1]cuda:0"; primals_21: "f16[][]cuda:0"; primals_22: "f32[16][1]cuda:0"; primals_23: "Sym(2048)"; primals_24: "Sym(8192)"; primals_25: "f32[32, 2048][2048, 1]cuda:0"; primals_26: "f32[8192, 32][32, 1]cuda:0"; primals_27: "u8[8388608, 1][1, 1]cuda:0"; primals_28: "Sym(s10)"; primals_29: "Sym(s11)"; primals_30: "u8[262144][1]cuda:0"; primals_31: "f32[256][1]cuda:0"; primals_32: "f32[1024][1]cuda:0"; primals_33: "f16[][]cuda:0"; primals_34: "f32[16][1]cuda:0"; primals_35: "Sym(8192)"; primals_36: "Sym(2048)"; primals_37: "f32[32, 8192][8192, 1]cuda:0"; primals_38: "f32[2048, 32][32, 1]cuda:0"; tangents_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.bfloat16)
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift: "Sym(floor(s3/2))" = primals_5 >> 1
	        mul_4: "Sym(s2*s3)" = primals_4 * primals_5;  primals_5 = None
	        rshift_1: "Sym(floor(s2*s3/2))" = mul_4 >> 1
	        fused_dequantize_op: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16)
	        permute_1: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None
	        permute_2: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0]);  permute_1 = None
	        convert_element_type_1: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None
	        convert_element_type_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type, torch.float16);  convert_element_type = None
	        permute_3: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None
	        view: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2, [primals_1, 2048]);  convert_element_type_2 = None
	        mm: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view, permute_3);  view = permute_3 = None
	        view_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        clone: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.clone.default(view_1);  view_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_5: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None
	        permute_4: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None
	        view_2: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(primals_2, [primals_1, 2048])
	        mm_1: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_2, permute_4)
	        view_3: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None
	        convert_element_type_8: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None
	        permute_5: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None
	        view_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None
	        mm_2: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_4, permute_5)
	        view_5: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None
	        mul_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(clone, mul_39);  clone = mul_39 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        convert_element_type_11: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_13: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.bfloat16)
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_6: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_2: "Sym(floor(s7/2))" = primals_17 >> 1
	        mul_54: "Sym(s6*s7)" = primals_16 * primals_17;  primals_17 = None
	        rshift_3: "Sym(floor(s6*s7/2))" = mul_54 >> 1
	        fused_dequantize_op_1: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16)
	        permute_7: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None
	        permute_8: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0]);  permute_7 = None
	        convert_element_type_14: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None
	        convert_element_type_15: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_13, torch.float16);  convert_element_type_13 = None
	        permute_9: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None
	        view_6: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_15, [primals_1, 2048]);  convert_element_type_15 = None
	        mm_3: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_6, permute_9);  view_6 = permute_9 = None
	        view_7: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        clone_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.clone.default(view_7);  view_7 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_18: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None
	        permute_10: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None
	        view_8: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None
	        mm_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_8, permute_10)
	        view_9: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None
	        convert_element_type_21: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None
	        permute_11: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None
	        view_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None
	        mm_5: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_10, permute_11)
	        view_11: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None
	        mul_87: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_85: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(clone_1, mul_87);  clone_1 = mul_87 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_94: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85)
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_24: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_94, torch.bfloat16)
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_12: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_4: "Sym(floor(s11/2))" = primals_29 >> 1
	        mul_102: "Sym(s10*s11)" = primals_28 * primals_29;  primals_29 = None
	        rshift_5: "Sym(floor(s10*s11/2))" = mul_102 >> 1
	        fused_dequantize_op_2: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16)
	        permute_13: "bf16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None
	        permute_14: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_13, [1, 0]);  permute_13 = None
	        convert_element_type_25: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None
	        convert_element_type_26: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_24, torch.float16);  convert_element_type_24 = None
	        permute_15: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None
	        view_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_26, [primals_1, 8192]);  convert_element_type_26 = None
	        mm_6: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None
	        view_13: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        clone_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.clone.default(view_13);  view_13 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_29: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None
	        permute_16: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None
	        view_14: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None
	        mm_7: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_14, permute_16)
	        view_15: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None
	        convert_element_type_32: "f16[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None
	        permute_17: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None
	        view_16: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None
	        mm_8: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_16, permute_17)
	        view_17: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None
	        mul_137: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_131: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(clone_2, mul_137);  clone_2 = mul_137 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_144: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, 2.0)
	        view_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None
	        permute_18: "f16[2048, s0][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_18, [1, 0])
	        mm_9: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None
	        permute_19: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None
	        permute_20: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None
	        mm_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None
	        view_19: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None
	        permute_21: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None
	        convert_element_type_39: "f32[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None
	        view_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None
	        permute_22: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_20, [1, 0])
	        mm_11: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None
	        permute_23: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None
	        permute_24: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None
	        mm_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None
	        view_21: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None
	        permute_25: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None
	        convert_element_type_44: "f32[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        fused_dequantize_op_3: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None
	        permute_26: "bf16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_3, [1, 0]);  fused_dequantize_op_3 = None
	        convert_element_type_45: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.prims.convert_element_type.default(permute_26, torch.float16);  permute_26 = None
	        permute_27: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None
	        view_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None
	        mm_13: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None
	        view_23: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None
	        convert_element_type_48: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_23, torch.bfloat16);  view_23 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_49: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_48, torch.float16);  convert_element_type_48 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_135: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_21, convert_element_type_49);  view_21 = convert_element_type_49 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_146: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None
	        mul_147: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_148: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_146, 2.0)
	        view_24: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None
	        permute_28: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_24, [1, 0])
	        mm_14: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None
	        permute_29: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None
	        permute_30: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None
	        mm_15: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None
	        view_25: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None
	        permute_31: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None
	        convert_element_type_54: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None
	        view_26: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None
	        permute_32: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_26, [1, 0])
	        mm_16: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_32, view_8);  permute_32 = view_8 = None
	        permute_33: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None
	        permute_34: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None
	        mm_17: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None
	        view_27: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None
	        permute_35: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None
	        convert_element_type_59: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        fused_dequantize_op_4: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None
	        permute_36: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_4, [1, 0]);  fused_dequantize_op_4 = None
	        convert_element_type_60: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_36, torch.float16);  permute_36 = None
	        permute_37: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None
	        view_28: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None
	        mm_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None
	        view_29: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None
	        convert_element_type_63: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_29, torch.bfloat16);  view_29 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_64: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_63, torch.float16);  convert_element_type_63 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_136: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_27, convert_element_type_64);  view_27 = convert_element_type_64 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        sigmoid_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(add_39)
	        full: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_44: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(full, sigmoid_1);  full = None
	        mul_150: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None
	        add_137: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None
	        mul_151: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None
	        mul_152: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_153: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_152, 2.0)
	        view_30: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None
	        permute_39: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_30, [1, 0])
	        mm_19: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None
	        permute_40: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None
	        permute_41: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None
	        mm_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None
	        view_31: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None
	        permute_42: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None
	        convert_element_type_69: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None
	        view_32: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None
	        permute_43: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_32, [1, 0])
	        mm_21: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None
	        permute_44: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None
	        permute_45: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None
	        mm_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None
	        view_33: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        add_138: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_46: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None
	        convert_element_type_74: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        fused_dequantize_op_5: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None
	        permute_47: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_5, [1, 0]);  fused_dequantize_op_5 = None
	        convert_element_type_75: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_47, torch.float16);  permute_47 = None
	        permute_48: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None
	        view_34: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None
	        mm_23: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None
	        view_35: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None
	        convert_element_type_78: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_35, torch.bfloat16);  view_35 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_79: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_78, torch.float16);  convert_element_type_78 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_139: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_138, convert_element_type_79);  add_138 = convert_element_type_79 = None
	        return pytree.tree_unflatten([add_131, None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39], self._out_spec)
	        
V0326 23:29:22.288000 324469 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "da0cd3eb0c08434120e89e55970e93b0"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "Sym(s0)", primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", primals_3: "u8[8388608, 1][1, 1]cuda:0", primals_4: "Sym(s2)", primals_5: "Sym(s3)", primals_6: "u8[262144][1]cuda:0", primals_7: "f32[256][1]cuda:0", primals_8: "f32[1024][1]cuda:0", primals_9: "f16[][]cuda:0", primals_10: "f32[16][1]cuda:0", primals_11: "Sym(2048)", primals_12: "Sym(8192)", primals_13: "f32[32, 2048][2048, 1]cuda:0", primals_14: "f32[8192, 32][32, 1]cuda:0", primals_15: "u8[8388608, 1][1, 1]cuda:0", primals_16: "Sym(s6)", primals_17: "Sym(s7)", primals_18: "u8[262144][1]cuda:0", primals_19: "f32[256][1]cuda:0", primals_20: "f32[1024][1]cuda:0", primals_21: "f16[][]cuda:0", primals_22: "f32[16][1]cuda:0", primals_23: "Sym(2048)", primals_24: "Sym(8192)", primals_25: "f32[32, 2048][2048, 1]cuda:0", primals_26: "f32[8192, 32][32, 1]cuda:0", primals_27: "u8[8388608, 1][1, 1]cuda:0", primals_28: "Sym(s10)", primals_29: "Sym(s11)", primals_30: "u8[262144][1]cuda:0", primals_31: "f32[256][1]cuda:0", primals_32: "f32[1024][1]cuda:0", primals_33: "f16[][]cuda:0", primals_34: "f32[16][1]cuda:0", primals_35: "Sym(8192)", primals_36: "Sym(2048)", primals_37: "f32[32, 8192][8192, 1]cuda:0", primals_38: "f32[2048, 32][32, 1]cuda:0"):
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift: "Sym(floor(s3/2))" = primals_5 >> 1
	        mul_4: "Sym(s2*s3)" = primals_4 * primals_5;  primals_5 = None
	        rshift_1: "Sym(floor(s2*s3/2))" = mul_4 >> 1
	        fused_dequantize_op: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None
	        permute_1: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None
	        permute_2: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0])
	        convert_element_type_1: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None
	        convert_element_type_default_5: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.float16)
	        permute_3: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None
	        view: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_default_5, [primals_1, 2048]);  convert_element_type_default_5 = None
	        mm: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view, permute_3);  permute_3 = None
	        view_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_5: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None
	        permute_4: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None
	        view_2: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None
	        mm_1: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_2, permute_4)
	        view_3: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None
	        convert_element_type_8: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None
	        permute_5: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None
	        view_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None
	        mm_2: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_4, permute_5)
	        view_5: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None
	        mul_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1, mul_39);  view_1 = mul_39 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        convert_element_type_11: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_6: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_2: "Sym(floor(s7/2))" = primals_17 >> 1
	        mul_54: "Sym(s6*s7)" = primals_16 * primals_17;  primals_17 = None
	        rshift_3: "Sym(floor(s6*s7/2))" = mul_54 >> 1
	        fused_dequantize_op_1: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None
	        permute_7: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None
	        permute_8: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0])
	        convert_element_type_14: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None
	        permute_9: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None
	        mm_3: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view, permute_9);  view = permute_9 = None
	        view_7: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_18: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None
	        permute_10: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None
	        mm_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_2, permute_10)
	        view_9: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None
	        convert_element_type_21: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None
	        permute_11: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None
	        view_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None
	        mm_5: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_10, permute_11)
	        view_11: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None
	        mul_87: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_85: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_7, mul_87);  view_7 = mul_87 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_94: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85);  convert_element_type_12 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_12: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_4: "Sym(floor(s11/2))" = primals_29 >> 1
	        mul_102: "Sym(s10*s11)" = primals_28 * primals_29;  primals_29 = None
	        rshift_5: "Sym(floor(s10*s11/2))" = mul_102 >> 1
	        fused_dequantize_op_2: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None
	        permute_13: "bf16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None
	        permute_14: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_13, [1, 0])
	        convert_element_type_25: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None
	        convert_element_type_default_3: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_94, torch.float16)
	        permute_15: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None
	        view_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_default_3, [primals_1, 8192]);  convert_element_type_default_3 = None
	        mm_6: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None
	        view_13: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_29: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None
	        permute_16: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None
	        view_14: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None
	        mm_7: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_14, permute_16)
	        view_15: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None
	        convert_element_type_32: "f16[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None
	        permute_17: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None
	        view_16: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None
	        mm_8: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_16, permute_17)
	        view_17: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None
	        mul_137: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_131: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_13, mul_137);  view_13 = mul_137 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_20: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None
	        permute_24: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_45: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.prims.convert_element_type.default(permute_13, torch.float16);  permute_13 = None
	        permute_27: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_30: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None
	        permute_34: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_60: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_7, torch.float16);  permute_7 = None
	        permute_37: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_41: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None
	        permute_45: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_75: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_1, torch.float16);  permute_1 = None
	        permute_48: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None
	        return (add_131, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, primals_1)
	        
V0326 23:29:22.291000 324469 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "2959bc24a322398834c0f5283239895e"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "Sym(s0)", view_2: "f16[s0, 2048][2048, 1]cuda:0", view_4: "f16[s0, 32][32, 1]cuda:0", add_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", view_10: "f16[s0, 32][32, 1]cuda:0", add_85: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", view_14: "f16[s0, 8192][8192, 1]cuda:0", view_16: "f16[s0, 32][32, 1]cuda:0", permute_20: "f16[2048, 32][32, 1]cuda:0", permute_24: "f16[32, 8192][8192, 1]cuda:0", permute_27: "f16[2048, 8192][8192, 1]cuda:0", permute_30: "f16[8192, 32][32, 1]cuda:0", permute_34: "f16[32, 2048][2048, 1]cuda:0", permute_37: "f16[8192, 2048][2048, 1]cuda:0", permute_41: "f16[8192, 32][32, 1]cuda:0", permute_45: "f16[32, 2048][2048, 1]cuda:0", permute_48: "f16[8192, 2048][2048, 1]cuda:0", tangents_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"):
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_144: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, 2.0)
	        view_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None
	        permute_18: "f16[2048, s0][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_18, [1, 0])
	        mm_9: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None
	        permute_19: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None
	        mm_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None
	        view_19: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None
	        permute_21: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None
	        convert_element_type_39: "f32[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None
	        view_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None
	        permute_22: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_20, [1, 0])
	        mm_11: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None
	        permute_23: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None
	        mm_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None
	        view_21: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None
	        permute_25: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None
	        convert_element_type_44: "f32[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None
	        mm_13: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None
	        view_23: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_default_2: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_23, torch.float16);  view_23 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_135: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_21, convert_element_type_default_2);  view_21 = convert_element_type_default_2 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        convert_element_type_11: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        mul_146: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None
	        mul_147: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_148: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_146, 2.0)
	        view_24: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None
	        permute_28: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_24, [1, 0])
	        mm_14: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None
	        permute_29: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None
	        mm_15: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None
	        view_25: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None
	        permute_31: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None
	        convert_element_type_54: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None
	        view_26: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None
	        permute_32: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_26, [1, 0])
	        mm_16: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_32, view_2);  permute_32 = None
	        permute_33: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None
	        mm_17: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None
	        view_27: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None
	        permute_35: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None
	        convert_element_type_59: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_28: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None
	        mm_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None
	        view_29: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_default_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_29, torch.float16);  view_29 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_136: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_27, convert_element_type_default_1);  view_27 = convert_element_type_default_1 = None
	        
	         # File: /tmp/ipykernel_324469/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        sigmoid_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(add_39)
	        full_default: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_44: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_default, sigmoid_1);  full_default = None
	        mul_150: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None
	        add_137: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None
	        mul_151: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None
	        mul_152: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_153: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_152, 2.0)
	        view_30: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None
	        permute_39: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_30, [1, 0])
	        mm_19: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None
	        permute_40: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None
	        mm_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None
	        view_31: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None
	        permute_42: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None
	        convert_element_type_69: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None
	        view_32: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None
	        permute_43: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_32, [1, 0])
	        mm_21: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None
	        permute_44: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None
	        mm_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None
	        view_33: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        add_138: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_46: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None
	        convert_element_type_74: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_34: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None
	        mm_23: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None
	        view_35: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_default: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_35, torch.float16);  view_35 = None
	        
	         # File: /tmp/ipykernel_324469/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_139: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_138, convert_element_type_default);  add_138 = convert_element_type_default = None
	        return (None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39)
	        
V0326 23:29:22.292000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "6448c509ee5b500a7c62a0860f56201a"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992162291892.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.292000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "6f7e06cfa438b5f7e3bed3251a2aabd6"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992162292361.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.292000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "a747dda63d6eada39723b114fbf74e4e"}
	{
	"name": "inductor_compile",
	"ts": 1742992162292361.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.309000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/6t/c6ti5avby2m7pt3ut73u7cv6pskab7oqrj32flivbrhi5omzahyd.py"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "2cfff416bd40de843c972c9ab081a285"}
	# AOT ID: ['5_forward']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/tc/ctc3wd5ie7s2fjcr2f4u2qxws22nb2vss5um5266to6nceu4xt3o.py
	# Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [mylib.fused_dequantize_op]
	# Source node to ATen node mapping:
	#   autograd_function_apply => fused_dequantize_op
	# Graph fragment:
	#   %fused_dequantize_op : [num_users=1] = call_function[target=torch.ops.mylib.fused_dequantize_op.default](args = (%primals_6, %primals_7, %primals_8, %primals_9, %permute, %primals_10, 16777216, %primals_4, %rshift, %mul_4, %rshift_1, 8192, 2048, torch.bfloat16), kwargs = {})
	triton_poi_fused_fused_dequantize_op_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {2: 1}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=(2,))]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_fused_dequantize_op_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 1
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    tmp0 = tl.load(in_ptr0 + (0)).to(tl.float32)
	    tmp1 = tl.broadcast_to(tmp0, [XBLOCK])
	    tl.store(out_ptr0 + (tl.full([XBLOCK], 0, tl.int32)), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/4i/c4ighga7fod2ed36hdz67v2ofqmyk7mlh2kc24zdiinrkgsdjog7.py
	# Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	#   autograd_function_apply => convert_element_type_1
	# Graph fragment:
	#   %convert_element_type_1 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_2, torch.float16), kwargs = {})
	triton_poi_fused__to_copy_1 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[16777216], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*bf16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_1', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 16777216
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/dm/cdm5ebpcdjlaqnrcavlevwhnlhakhmbh235waumdxi2dlz4oqdc3.py
	# Topologically Sorted Source Nodes: [linear], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	#   linear => convert_element_type_5, permute_4
	# Graph fragment:
	#   %convert_element_type_5 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_13, torch.float16), kwargs = {})
	#   %permute_4 : [num_users=2] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_5, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_2 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[65536], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp32', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_2', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 65536
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/4v/c4vy7sd4hdenvrautochni2fe3ce3tkssjud74ei4t45g5jo4bxq.py
	# Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	#   linear_1 => convert_element_type_8, permute_5
	# Graph fragment:
	#   %convert_element_type_8 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_14, torch.float16), kwargs = {})
	#   %permute_5 : [num_users=2] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_8, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_3 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp32', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_3', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 262144
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/5b/c5bxmvhk7qwah3r5zprbeosehsg4smud74ljqbertg7zky4hs4tq.py
	# Topologically Sorted Source Nodes: [output, result_2, silu, output_1, result_5, mul_2], Original ATen: [aten.mul, aten.add, aten.silu]
	# Source node to ATen node mapping:
	#   mul_2 => mul_94
	#   output => mul_39
	#   output_1 => mul_87
	#   result_2 => add_39
	#   result_5 => add_85
	#   silu => convert_element_type_11, convert_element_type_12, mul_46, sigmoid
	# Graph fragment:
	#   %mul_39 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_5, 2.0), kwargs = {})
	#   %add_39 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_1, %mul_39), kwargs = {})
	#   %convert_element_type_11 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_39, torch.float32), kwargs = {})
	#   %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_11,), kwargs = {})
	#   %mul_46 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_11, %sigmoid), kwargs = {})
	#   %convert_element_type_12 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_46, torch.float16), kwargs = {})
	#   %mul_87 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_11, 2.0), kwargs = {})
	#   %add_85 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_7, %mul_87), kwargs = {})
	#   %mul_94 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_12, %add_85), kwargs = {})
	triton_poi_fused_add_mul_silu_4 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1048576], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: '*fp16', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mul_silu_4', 'mutated_arg_names': ['in_out_ptr0', 'in_out_ptr1'], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_out_ptr0, in_out_ptr1, in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_out_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp5 = tl.load(in_out_ptr1 + (x0), None).to(tl.float32)
	    tmp6 = tl.load(in_ptr1 + (x0), None).to(tl.float32)
	    tmp2 = 2.0
	    tmp3 = tmp1 * tmp2
	    tmp4 = tmp0 + tmp3
	    tmp7 = tmp6 * tmp2
	    tmp8 = tmp5 + tmp7
	    tmp9 = tmp4.to(tl.float32)
	    tmp10 = tl.sigmoid(tmp9)
	    tmp11 = tmp9 * tmp10
	    tmp12 = tmp11.to(tl.float32)
	    tmp13 = tmp12 * tmp8
	    tl.store(in_out_ptr0 + (x0), tmp4, None)
	    tl.store(in_out_ptr1 + (x0), tmp8, None)
	    tl.store(out_ptr0 + (x0), tmp13, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/3b/c3bktvkn3n6hu4v7tiwk7rpos2wp4lajxxzrdgoicggbyxk5d3cb.py
	# Topologically Sorted Source Nodes: [output_2, result_8], Original ATen: [aten.mul, aten.add]
	# Source node to ATen node mapping:
	#   output_2 => mul_137
	#   result_8 => add_131
	# Graph fragment:
	#   %mul_137 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_17, 2.0), kwargs = {})
	#   %add_131 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_13, %mul_137), kwargs = {})
	triton_poi_fused_add_mul_5 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mul_5', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_out_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp2 = 2.0
	    tmp3 = tmp1 * tmp2
	    tmp4 = tmp0 + tmp3
	    tl.store(in_out_ptr0 + (x0), tmp4, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/j7/cj776okodjzzoh7cfozhudvywfzftzc6cabab2mmcdjjege6gbpi.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_45 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_13, torch.float16), kwargs = {})
	triton_poi_fused__to_copy_6 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[8192, 2048], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*bf16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_6', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 8192
	    xnumel = 2048
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (8192*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (2048*y0)), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/mi/cmiz6hpfs7acuhgi57mdp7dpphsyd43zzboziig5wum6ghgma4e2.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_45 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_13, torch.float16), kwargs = {})
	#   %permute_27 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_45, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_7 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[2048, 8192], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_7', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 2048
	    xnumel = 8192
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (2048*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (8192*y0)), tmp0, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/vd/cvds2jaf5sirynr2qnt7rpujbamyslbkrjowetcdyb6ep4yhq4gq.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_60 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_7, torch.float16), kwargs = {})
	triton_poi_fused__to_copy_8 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[2048, 8192], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*bf16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_8', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 2048
	    xnumel = 8192
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (2048*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (8192*y0)), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/ai/caikh3jmq5xv3uwakclrkwfn3zmg36elfcn5skhch7uvaivem3uj.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_60 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_7, torch.float16), kwargs = {})
	#   %permute_37 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_60, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_9 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[8192, 2048], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_9', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 8192
	    xnumel = 2048
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (8192*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (2048*y0)), tmp0, None)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38 = args
	    args.clear()
	    s0 = primals_1
	    s2 = primals_4
	    s3 = primals_5
	    s6 = primals_16
	    s7 = primals_17
	    s10 = primals_28
	    s11 = primals_29
	    assert_size_stride(primals_2, (1, s0, 2048), (2048*s0, 2048, 1))
	    assert_size_stride(primals_3, (8388608, 1), (1, 1))
	    assert_size_stride(primals_6, (262144, ), (1, ))
	    assert_size_stride(primals_7, (256, ), (1, ))
	    assert_size_stride(primals_8, (1024, ), (1, ))
	    assert_size_stride(primals_9, (), ())
	    assert_size_stride(primals_10, (16, ), (1, ))
	    assert_size_stride(primals_13, (32, 2048), (2048, 1))
	    assert_size_stride(primals_14, (8192, 32), (32, 1))
	    assert_size_stride(primals_15, (8388608, 1), (1, 1))
	    assert_size_stride(primals_18, (262144, ), (1, ))
	    assert_size_stride(primals_19, (256, ), (1, ))
	    assert_size_stride(primals_20, (1024, ), (1, ))
	    assert_size_stride(primals_21, (), ())
	    assert_size_stride(primals_22, (16, ), (1, ))
	    assert_size_stride(primals_25, (32, 2048), (2048, 1))
	    assert_size_stride(primals_26, (8192, 32), (32, 1))
	    assert_size_stride(primals_27, (8388608, 1), (1, 1))
	    assert_size_stride(primals_30, (262144, ), (1, ))
	    assert_size_stride(primals_31, (256, ), (1, ))
	    assert_size_stride(primals_32, (1024, ), (1, ))
	    assert_size_stride(primals_33, (), ())
	    assert_size_stride(primals_34, (16, ), (1, ))
	    assert_size_stride(primals_37, (32, 8192), (8192, 1))
	    assert_size_stride(primals_38, (2048, 32), (32, 1))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((), (), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [mylib.fused_dequantize_op]
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_fused_dequantize_op_0.run(primals_9, buf0, 1, grid=grid(1), stream=stream0)
	        del primals_9
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [mylib.fused_dequantize_op]
	        buf1 = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, buf0, reinterpret_tensor(primals_3, (1, 8388608), (1, 1), 0), primals_10, 16777216, s2, math.floor((1/2)*s3), s2*s3, math.floor((1/2)*s2*s3), 8192, 2048, torch.bfloat16)
	        del primals_10
	        del primals_3
	        del primals_6
	        del primals_7
	        del primals_8
	        buf2 = buf1
	        del buf1
	        buf3 = empty_strided_cuda((8192, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf2, buf3, 16777216, grid=grid(16777216), stream=stream0)
	        buf4 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), reinterpret_tensor(buf3, (2048, 8192), (1, 2048), 0), out=buf4)
	        buf5 = empty_strided_cuda((2048, 32), (1, 2048), torch.float16)
	        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_2.run(primals_13, buf5, 65536, grid=grid(65536), stream=stream0)
	        del primals_13
	        buf6 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), buf5, out=buf6)
	        buf7 = empty_strided_cuda((32, 8192), (1, 32), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_3.run(primals_14, buf7, 262144, grid=grid(262144), stream=stream0)
	        del primals_14
	        buf8 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten.mm]
	        extern_kernels.mm(buf6, buf7, out=buf8)
	        buf10 = buf0; del buf0  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [mylib.fused_dequantize_op]
	        triton_poi_fused_fused_dequantize_op_0.run(primals_21, buf10, 1, grid=grid(1), stream=stream0)
	        del primals_21
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [mylib.fused_dequantize_op]
	        buf11 = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, buf10, reinterpret_tensor(primals_15, (1, 8388608), (1, 1), 0), primals_22, 16777216, s6, math.floor((1/2)*s7), s6*s7, math.floor((1/2)*s6*s7), 8192, 2048, torch.bfloat16)
	        del primals_15
	        del primals_18
	        del primals_19
	        del primals_20
	        del primals_22
	        buf12 = buf11
	        del buf11
	        buf13 = buf3; del buf3  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf12, buf13, 16777216, grid=grid(16777216), stream=stream0)
	        buf14 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), reinterpret_tensor(buf13, (2048, 8192), (1, 2048), 0), out=buf14)
	        buf15 = empty_strided_cuda((2048, 32), (1, 2048), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_2], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_2.run(primals_25, buf15, 65536, grid=grid(65536), stream=stream0)
	        del primals_25
	        buf16 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_2], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), buf15, out=buf16)
	        buf17 = empty_strided_cuda((32, 8192), (1, 32), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_3], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_3.run(primals_26, buf17, 262144, grid=grid(262144), stream=stream0)
	        del primals_26
	        buf18 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_3], Original ATen: [aten.mm]
	        extern_kernels.mm(buf16, buf17, out=buf18)
	        buf9 = reinterpret_tensor(buf4, (1, s0, 8192), (8192*s0, 8192, 1), 0); del buf4  # reuse
	        buf19 = reinterpret_tensor(buf14, (1, s0, 8192), (8192*s0, 8192, 1), 0); del buf14  # reuse
	        buf23 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [output, result_2, silu, output_1, result_5, mul_2], Original ATen: [aten.mul, aten.add, aten.silu]
	        triton_poi_fused_add_mul_silu_4_xnumel = 8192*s0
	        triton_poi_fused_add_mul_silu_4.run(buf9, buf19, buf8, buf18, buf23, triton_poi_fused_add_mul_silu_4_xnumel, grid=grid(triton_poi_fused_add_mul_silu_4_xnumel), stream=stream0)
	        del buf18
	        del buf8
	        buf20 = buf10; del buf10  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [mylib.fused_dequantize_op]
	        triton_poi_fused_fused_dequantize_op_0.run(primals_33, buf20, 1, grid=grid(1), stream=stream0)
	        del primals_33
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [mylib.fused_dequantize_op]
	        buf21 = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, buf20, reinterpret_tensor(primals_27, (1, 8388608), (1, 1), 0), primals_34, 16777216, s10, math.floor((1/2)*s11), s10*s11, math.floor((1/2)*s10*s11), 2048, 8192, torch.bfloat16)
	        del buf20
	        del primals_27
	        del primals_30
	        del primals_31
	        del primals_32
	        del primals_34
	        buf22 = buf21
	        del buf21
	        buf24 = reinterpret_tensor(buf13, (2048, 8192), (8192, 1), 0); del buf13  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf22, buf24, 16777216, grid=grid(16777216), stream=stream0)
	        buf25 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf23, (s0, 8192), (8192, 1), 0), reinterpret_tensor(buf24, (8192, 2048), (1, 8192), 0), out=buf25)
	        buf26 = empty_strided_cuda((8192, 32), (1, 8192), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_3.run(primals_37, buf26, 262144, grid=grid(262144), stream=stream0)
	        del primals_37
	        buf27 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf23, (s0, 8192), (8192, 1), 0), buf26, out=buf27)
	        buf28 = empty_strided_cuda((32, 2048), (1, 32), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_5], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_2.run(primals_38, buf28, 65536, grid=grid(65536), stream=stream0)
	        del primals_38
	        buf29 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_5], Original ATen: [aten.mm]
	        extern_kernels.mm(buf27, buf28, out=buf29)
	        buf30 = reinterpret_tensor(buf25, (1, s0, 2048), (2048*s0, 2048, 1), 0); del buf25  # reuse
	        # Topologically Sorted Source Nodes: [output_2, result_8], Original ATen: [aten.mul, aten.add]
	        triton_poi_fused_add_mul_5_xnumel = 2048*s0
	        triton_poi_fused_add_mul_5.run(buf30, buf29, triton_poi_fused_add_mul_5_xnumel, grid=grid(triton_poi_fused_add_mul_5_xnumel), stream=stream0)
	        del buf29
	        buf31 = reinterpret_tensor(buf24, (8192, 2048), (2048, 1), 0); del buf24  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_6.run(buf22, buf31, 8192, 2048, grid=grid(8192, 2048), stream=stream0)
	        del buf22
	        buf32 = empty_strided_cuda((2048, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_7.run(buf31, buf32, 2048, 8192, grid=grid(2048, 8192), stream=stream0)
	        buf33 = reinterpret_tensor(buf31, (2048, 8192), (8192, 1), 0); del buf31  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_8.run(buf12, buf33, 2048, 8192, grid=grid(2048, 8192), stream=stream0)
	        del buf12
	        buf34 = empty_strided_cuda((8192, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_9.run(buf33, buf34, 8192, 2048, grid=grid(8192, 2048), stream=stream0)
	        buf35 = buf33; del buf33  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_8.run(buf2, buf35, 2048, 8192, grid=grid(2048, 8192), stream=stream0)
	        del buf2
	        buf36 = empty_strided_cuda((8192, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_9.run(buf35, buf36, 8192, 2048, grid=grid(8192, 2048), stream=stream0)
	        del buf35
	    return (buf30, reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), buf6, buf9, buf16, buf19, reinterpret_tensor(buf23, (s0, 8192), (8192, 1), 0), buf27, reinterpret_tensor(buf28, (2048, 32), (32, 1), 0), reinterpret_tensor(buf26, (32, 8192), (8192, 1), 0), buf32, reinterpret_tensor(buf17, (8192, 32), (32, 1), 0), reinterpret_tensor(buf15, (32, 2048), (2048, 1), 0), buf34, reinterpret_tensor(buf7, (8192, 32), (32, 1), 0), reinterpret_tensor(buf5, (32, 2048), (2048, 1), 0), buf36, s0, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    primals_1 = 100
	    primals_2 = rand_strided((1, 100, 2048), (204800, 2048, 1), device='cuda:0', dtype=torch.float16)
	    primals_3 = rand_strided((8388608, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    primals_4 = 256
	    primals_5 = 64
	    primals_6 = rand_strided((262144, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    primals_7 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_8 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_9 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    primals_10 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_11 = 2048
	    primals_12 = 8192
	    primals_13 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)
	    primals_14 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float32)
	    primals_15 = rand_strided((8388608, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    primals_16 = 256
	    primals_17 = 64
	    primals_18 = rand_strided((262144, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    primals_19 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_20 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_21 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    primals_22 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_23 = 2048
	    primals_24 = 8192
	    primals_25 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)
	    primals_26 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float32)
	    primals_27 = rand_strided((8388608, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    primals_28 = 256
	    primals_29 = 64
	    primals_30 = rand_strided((262144, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    primals_31 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_32 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_33 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    primals_34 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_35 = 8192
	    primals_36 = 2048
	    primals_37 = rand_strided((32, 8192), (8192, 1), device='cuda:0', dtype=torch.float32)
	    primals_38 = rand_strided((2048, 32), (32, 1), device='cuda:0', dtype=torch.float32)
	    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:29:22.309000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "39d3e54bc6c23c191ea40e027ceac738"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742992162309447.8,
	"args": {
	"key": "fibzwaj5j2pxd4mgs6jpctih67rusxg2lyckoxqaw3hr5wi42ks3",
	"components": [
	"[iu3kgzgo5x6udk6uhcxvnilg7lheh3rln2fhyjpg5qgapvm76o4] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38):\n    permute = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None\n    rshift = primals_5 >> 1\n    mul_4 = primals_4 * primals_5;  primals_5 = None\n    rshift_1 = mul_4 >> 1\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None\n    permute_1 = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None\n    permute_2 = torch.ops.aten.permute.default(permute_1, [1, 0])\n    convert_element_type_1 = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None\n    convert_element_type_default_5 = torch.ops.prims.convert_element_type.default(primals_2, torch.float16)\n    permute_3 = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None\n    view = torch.ops.aten.view.default(convert_element_type_default_5, [primals_1, 2048]);  convert_element_type_default_5 = None\n    mm = torch.ops.aten.mm.default(view, permute_3);  permute_3 = None\n    view_1 = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None\n    convert_element_type_5 = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None\n    permute_4 = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None\n    view_2 = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None\n    mm_1 = torch.ops.aten.mm.default(view_2, permute_4)\n    view_3 = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None\n    convert_element_type_8 = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None\n    permute_5 = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None\n    view_4 = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None\n    mm_2 = torch.ops.aten.mm.default(view_4, permute_5)\n    view_5 = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None\n    mul_39 = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None\n    add_39 = torch.ops.aten.add.Tensor(view_1, mul_39);  view_1 = mul_39 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    permute_6 = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None\n    rshift_2 = primals_17 >> 1\n    mul_54 = primals_16 * primals_17;  primals_17 = None\n    rshift_3 = mul_54 >> 1\n    fused_dequantize_op_1 = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None\n    permute_7 = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None\n    permute_8 = torch.ops.aten.permute.default(permute_7, [1, 0])\n    convert_element_type_14 = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None\n    permute_9 = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None\n    mm_3 = torch.ops.aten.mm.default(view, permute_9);  view = permute_9 = None\n    view_7 = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None\n    convert_element_type_18 = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None\n    permute_10 = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None\n    mm_4 = torch.ops.aten.mm.default(view_2, permute_10)\n    view_9 = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None\n    convert_element_type_21 = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None\n    permute_11 = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None\n    view_10 = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None\n    mm_5 = torch.ops.aten.mm.default(view_10, permute_11)\n    view_11 = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None\n    mul_87 = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None\n    add_85 = torch.ops.aten.add.Tensor(view_7, mul_87);  view_7 = mul_87 = None\n    mul_94 = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85);  convert_element_type_12 = None\n    permute_12 = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None\n    rshift_4 = primals_29 >> 1\n    mul_102 = primals_28 * primals_29;  primals_29 = None\n    rshift_5 = mul_102 >> 1\n    fused_dequantize_op_2 = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None\n    permute_13 = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None\n    permute_14 = torch.ops.aten.permute.default(permute_13, [1, 0])\n    convert_element_type_25 = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None\n    convert_element_type_default_3 = torch.ops.prims.convert_element_type.default(mul_94, torch.float16)\n    permute_15 = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None\n    view_12 = torch.ops.aten.view.default(convert_element_type_default_3, [primals_1, 8192]);  convert_element_type_default_3 = None\n    mm_6 = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None\n    view_13 = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None\n    convert_element_type_29 = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None\n    permute_16 = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None\n    view_14 = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None\n    mm_7 = torch.ops.aten.mm.default(view_14, permute_16)\n    view_15 = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None\n    convert_element_type_32 = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None\n    permute_17 = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None\n    view_16 = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None\n    mm_8 = torch.ops.aten.mm.default(view_16, permute_17)\n    view_17 = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None\n    mul_137 = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None\n    add_131 = torch.ops.aten.add.Tensor(view_13, mul_137);  view_13 = mul_137 = None\n    permute_20 = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None\n    permute_24 = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None\n    convert_element_type_45 = torch.ops.prims.convert_element_type.default(permute_13, torch.float16);  permute_13 = None\n    permute_27 = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None\n    permute_30 = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None\n    permute_34 = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None\n    convert_element_type_60 = torch.ops.prims.convert_element_type.default(permute_7, torch.float16);  permute_7 = None\n    permute_37 = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None\n    permute_41 = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None\n    permute_45 = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None\n    convert_element_type_75 = torch.ops.prims.convert_element_type.default(permute_1, torch.float16);  permute_1 = None\n    permute_48 = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None\n    return (add_131, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, primals_1)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[2]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[3]: ('s2',)",
	"[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[4]: ('s3',)",
	"[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[5]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[7]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[9]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[10]: ('2048',)",
	"[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[11]: ('8192',)",
	"[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[12]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[13]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[14]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[bp5m6l3n24hn6zbxwmg6wfbnn5g5tujtrpje3hczu5umrrf2qjx] example_inputs[15]: ('s6',)",
	"[i3xrsw5sisbji2ivhzqy3l7zuqzhmvjpsyt77nanlhezk7f6tur] example_inputs[16]: ('s7',)",
	"[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[17]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[18]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[19]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[20]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[21]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[22]: ('2048',)",
	"[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[23]: ('8192',)",
	"[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[24]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[25]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[26]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hj7nkho4e2pgg3rxjgt2xo2ofzqotfhatuhlt6bxcifwqbvipzw] example_inputs[27]: ('s10',)",
	"[kosoqok7aznz5iahzbmrdsrqfq3jkju4dvvd7cfij3xrvzxbmb4] example_inputs[28]: ('s11',)",
	"[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[29]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[30]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[31]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[32]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[33]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[34]: ('8192',)",
	"[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[35]: ('2048',)",
	"[zs5vjd7lduxixuzsgnizmtcmiocu75spefi5z3zyshqbfmva6ge] example_inputs[36]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[chru3fsd44dmnozqmsiynvahv26l3bk7j747je77m6edbjsd2kq] example_inputs[37]: TensorMetadata(dtype=torch.float32, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[34jl2fvzyx6odxtpy45nz66wulvmgs5bvsvvhx2zwehki6ym5kw] fx_kwargs[static_input_idxs]: [2, 5, 6, 7, 8, 9, 12, 13, 14, 17, 18, 19, 20, 21, 24, 25, 26, 29, 30, 31, 32, 33, 36, 37]",
	"[2ruvd6wcktb7xtp52gxtldu3ygrqbnwbiluwcx3yagimk7erlhx] fx_kwargs[user_visible_outputs]: {'add_131': None}",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 688660460,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:29:22.309000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "37a16f91963db61f5f565392fb5de95d"}
	{"key": "fibzwaj5j2pxd4mgs6jpctih67rusxg2lyckoxqaw3hr5wi42ks3", "components": ["[iu3kgzgo5x6udk6uhcxvnilg7lheh3rln2fhyjpg5qgapvm76o4] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38):\n    permute = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None\n    rshift = primals_5 >> 1\n    mul_4 = primals_4 * primals_5;  primals_5 = None\n    rshift_1 = mul_4 >> 1\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None\n    permute_1 = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None\n    permute_2 = torch.ops.aten.permute.default(permute_1, [1, 0])\n    convert_element_type_1 = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None\n    convert_element_type_default_5 = torch.ops.prims.convert_element_type.default(primals_2, torch.float16)\n    permute_3 = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None\n    view = torch.ops.aten.view.default(convert_element_type_default_5, [primals_1, 2048]);  convert_element_type_default_5 = None\n    mm = torch.ops.aten.mm.default(view, permute_3);  permute_3 = None\n    view_1 = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None\n    convert_element_type_5 = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None\n    permute_4 = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None\n    view_2 = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None\n    mm_1 = torch.ops.aten.mm.default(view_2, permute_4)\n    view_3 = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None\n    convert_element_type_8 = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None\n    permute_5 = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None\n    view_4 = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None\n    mm_2 = torch.ops.aten.mm.default(view_4, permute_5)\n    view_5 = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None\n    mul_39 = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None\n    add_39 = torch.ops.aten.add.Tensor(view_1, mul_39);  view_1 = mul_39 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    permute_6 = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None\n    rshift_2 = primals_17 >> 1\n    mul_54 = primals_16 * primals_17;  primals_17 = None\n    rshift_3 = mul_54 >> 1\n    fused_dequantize_op_1 = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None\n    permute_7 = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None\n    permute_8 = torch.ops.aten.permute.default(permute_7, [1, 0])\n    convert_element_type_14 = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None\n    permute_9 = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None\n    mm_3 = torch.ops.aten.mm.default(view, permute_9);  view = permute_9 = None\n    view_7 = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None\n    convert_element_type_18 = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None\n    permute_10 = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None\n    mm_4 = torch.ops.aten.mm.default(view_2, permute_10)\n    view_9 = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None\n    convert_element_type_21 = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None\n    permute_11 = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None\n    view_10 = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None\n    mm_5 = torch.ops.aten.mm.default(view_10, permute_11)\n    view_11 = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None\n    mul_87 = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None\n    add_85 = torch.ops.aten.add.Tensor(view_7, mul_87);  view_7 = mul_87 = None\n    mul_94 = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85);  convert_element_type_12 = None\n    permute_12 = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None\n    rshift_4 = primals_29 >> 1\n    mul_102 = primals_28 * primals_29;  primals_29 = None\n    rshift_5 = mul_102 >> 1\n    fused_dequantize_op_2 = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None\n    permute_13 = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None\n    permute_14 = torch.ops.aten.permute.default(permute_13, [1, 0])\n    convert_element_type_25 = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None\n    convert_element_type_default_3 = torch.ops.prims.convert_element_type.default(mul_94, torch.float16)\n    permute_15 = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None\n    view_12 = torch.ops.aten.view.default(convert_element_type_default_3, [primals_1, 8192]);  convert_element_type_default_3 = None\n    mm_6 = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None\n    view_13 = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None\n    convert_element_type_29 = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None\n    permute_16 = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None\n    view_14 = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None\n    mm_7 = torch.ops.aten.mm.default(view_14, permute_16)\n    view_15 = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None\n    convert_element_type_32 = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None\n    permute_17 = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None\n    view_16 = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None\n    mm_8 = torch.ops.aten.mm.default(view_16, permute_17)\n    view_17 = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None\n    mul_137 = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None\n    add_131 = torch.ops.aten.add.Tensor(view_13, mul_137);  view_13 = mul_137 = None\n    permute_20 = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None\n    permute_24 = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None\n    convert_element_type_45 = torch.ops.prims.convert_element_type.default(permute_13, torch.float16);  permute_13 = None\n    permute_27 = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None\n    permute_30 = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None\n    permute_34 = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None\n    convert_element_type_60 = torch.ops.prims.convert_element_type.default(permute_7, torch.float16);  permute_7 = None\n    permute_37 = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None\n    permute_41 = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None\n    permute_45 = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None\n    convert_element_type_75 = torch.ops.prims.convert_element_type.default(permute_1, torch.float16);  permute_1 = None\n    permute_48 = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None\n    return (add_131, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, primals_1)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[2]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[3]: ('s2',)", "[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[4]: ('s3',)", "[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[5]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[7]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[9]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[10]: ('2048',)", "[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[11]: ('8192',)", "[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[12]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[13]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[14]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[bp5m6l3n24hn6zbxwmg6wfbnn5g5tujtrpje3hczu5umrrf2qjx] example_inputs[15]: ('s6',)", "[i3xrsw5sisbji2ivhzqy3l7zuqzhmvjpsyt77nanlhezk7f6tur] example_inputs[16]: ('s7',)", "[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[17]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[18]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[19]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[20]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[21]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[22]: ('2048',)", "[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[23]: ('8192',)", "[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[24]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[25]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[26]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hj7nkho4e2pgg3rxjgt2xo2ofzqotfhatuhlt6bxcifwqbvipzw] example_inputs[27]: ('s10',)", "[kosoqok7aznz5iahzbmrdsrqfq3jkju4dvvd7cfij3xrvzxbmb4] example_inputs[28]: ('s11',)", "[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[29]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[30]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[31]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[32]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[33]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[34]: ('8192',)", "[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[35]: ('2048',)", "[zs5vjd7lduxixuzsgnizmtcmiocu75spefi5z3zyshqbfmva6ge] example_inputs[36]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[chru3fsd44dmnozqmsiynvahv26l3bk7j747je77m6edbjsd2kq] example_inputs[37]: TensorMetadata(dtype=torch.float32, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[34jl2fvzyx6odxtpy45nz66wulvmgs5bvsvvhx2zwehki6ym5kw] fx_kwargs[static_input_idxs]: [2, 5, 6, 7, 8, 9, 12, 13, 14, 17, 18, 19, 20, 21, 24, 25, 26, 29, 30, 31, 32, 33, 36, 37]", "[2ruvd6wcktb7xtp52gxtldu3ygrqbnwbiluwcx3yagimk7erlhx] fx_kwargs[user_visible_outputs]: {'add_131': None}", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 688660460, "cache_state": "hit"}
V0326 23:29:22.312000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "4a128a3cceb4c26ad702d7ba2fbb4ff9"}
	{
	"name": "inductor_compile",
	"ts": 1742992162311988.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 8,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.312000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "6ff8cad7ecef547eb1d6a4cc53aaa948"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992162312423.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 8,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.312000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "0236d1332024db8822fe8b60ebb74a9a"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742992162312825.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 8,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.313000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "fadf11ddd0b06d2d667b02feb7416533"}
	{
	"name": "compile_fx.<locals>.bw_compiler",
	"ts": 1742992162313558.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.313000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "cae5c671e087f042f97bc1674fb083a4"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992162313950.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.314000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "097c73b2a18f09fed8f475d383b7f945"}
	{
	"name": "inductor_compile",
	"ts": 1742992162313950.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.324000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/hi/chi6hm2i2r5fg3p735q2myqwx4uplr5ji3czydmtzsbsjw6fmtl2.py"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "ccae98e057b275846abfbc378763924c"}
	# AOT ID: ['5_backward']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/oz/cozopk2ehguq32bevie5q4mjctwrvknb2efn2zmifydb43kq7u2a.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten.mul, aten.view]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %mul_144 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%tangents_1, 2.0), kwargs = {})
	#   %view_18 : [num_users=2] = call_function[target=torch.ops.aten.reshape.default](args = (%mul_144, [%primals_1, 2048]), kwargs = {})
	triton_poi_fused_mul_view_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_view_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = 2.0
	    tmp2 = tmp0 * tmp1
	    tl.store(out_ptr0 + (x0), tmp2, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/6p/c6pwujwafd7jyea2qiskauahv6a3isypaxtecr7nxxt4nrphxtq6.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_39 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_21, torch.float32), kwargs = {})
	triton_poi_fused__to_copy_1 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[65536], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_1', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 65536
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/2o/c2odmo35jco5222u2qehkh6jj67bnvs7ccevylkmgiqf4prmli3t.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_44 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_25, torch.float32), kwargs = {})
	triton_poi_fused__to_copy_2 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_2', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 262144
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/ut/cutu6nbumwt37rt3w5ivi5f2exxnf3nqjnpdd3meykytx4gvjuio.py
	# Topologically Sorted Source Nodes: [silu], Original ATen: [aten.add, aten.silu, aten.mul, aten.sigmoid, aten.fill, aten.sub]
	# Source node to ATen node mapping:
	#   silu => convert_element_type_11, convert_element_type_12, mul_46, sigmoid
	# Graph fragment:
	#   %add_135 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_21, %view_23), kwargs = {})
	#   %convert_element_type_11 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_39, torch.float32), kwargs = {})
	#   %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_11,), kwargs = {})
	#   %mul_46 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_11, %sigmoid), kwargs = {})
	#   %convert_element_type_12 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_46, torch.float16), kwargs = {})
	#   %mul_146 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_135, %convert_element_type_12), kwargs = {})
	#   %mul_147 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_135, %add_85), kwargs = {})
	#   %mul_148 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_146, 2.0), kwargs = {})
	#   %sigmoid_1 : [num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%add_39,), kwargs = {})
	#   %full_default : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([1, %primals_1, 8192], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
	#   %sub_44 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default, %sigmoid_1), kwargs = {})
	#   %mul_150 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_39, %sub_44), kwargs = {})
	#   %add_137 : [num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_150, 1), kwargs = {})
	#   %mul_151 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_1, %add_137), kwargs = {})
	#   %mul_152 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_147, %mul_151), kwargs = {})
	#   %mul_153 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_152, 2.0), kwargs = {})
	triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1048576], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: '*fp16', 5: '*fp16', 6: '*fp16', 7: '*fp16', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, out_ptr2, out_ptr3, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr1 + (x0), None).to(tl.float32)
	    tmp3 = tl.load(in_ptr2 + (x0), None).to(tl.float32)
	    tmp11 = tl.load(in_ptr3 + (x0), None).to(tl.float32)
	    tmp2 = tmp0 + tmp1
	    tmp4 = tmp3.to(tl.float32)
	    tmp5 = tl.sigmoid(tmp4)
	    tmp6 = tmp4 * tmp5
	    tmp7 = tmp6.to(tl.float32)
	    tmp8 = tmp2 * tmp7
	    tmp9 = 2.0
	    tmp10 = tmp8 * tmp9
	    tmp12 = tmp2 * tmp11
	    tmp13 = tl.sigmoid(tmp3)
	    tmp14 = 1.0
	    tmp15 = tmp14 - tmp13
	    tmp16 = tmp3 * tmp15
	    tmp17 = tmp16 + tmp14
	    tmp18 = tmp13 * tmp17
	    tmp19 = tmp12 * tmp18
	    tmp20 = tmp19 * tmp9
	    tl.store(out_ptr0 + (x0), tmp10, None)
	    tl.store(out_ptr1 + (x0), tmp8, None)
	    tl.store(out_ptr2 + (x0), tmp20, None)
	    tl.store(out_ptr3 + (x0), tmp19, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/vd/cvdgoxiwnbzp7azzk6zja6cs44uw5kuroxt6qq5umvp5rqdwji7r.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten.add]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %add_136 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_27, %view_29), kwargs = {})
	#   %add_138 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_136, %view_33), kwargs = {})
	#   %add_139 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_138, %view_35), kwargs = {})
	triton_poi_fused_add_4 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_4', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_out_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp3 = tl.load(in_ptr1 + (x0), None).to(tl.float32)
	    tmp5 = tl.load(in_ptr2 + (x0), None).to(tl.float32)
	    tmp2 = tmp0 + tmp1
	    tmp4 = tmp2 + tmp3
	    tmp6 = tmp4 + tmp5
	    tl.store(in_out_ptr0 + (x0), tmp6, None)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1 = args
	    args.clear()
	    s0 = primals_1
	    assert_size_stride(view_2, (s0, 2048), (2048, 1))
	    assert_size_stride(view_4, (s0, 32), (32, 1))
	    assert_size_stride(add_39, (1, s0, 8192), (8192*s0, 8192, 1))
	    assert_size_stride(view_10, (s0, 32), (32, 1))
	    assert_size_stride(add_85, (1, s0, 8192), (8192*s0, 8192, 1))
	    assert_size_stride(view_14, (s0, 8192), (8192, 1))
	    assert_size_stride(view_16, (s0, 32), (32, 1))
	    assert_size_stride(permute_20, (2048, 32), (32, 1))
	    assert_size_stride(permute_24, (32, 8192), (8192, 1))
	    assert_size_stride(permute_27, (2048, 8192), (8192, 1))
	    assert_size_stride(permute_30, (8192, 32), (32, 1))
	    assert_size_stride(permute_34, (32, 2048), (2048, 1))
	    assert_size_stride(permute_37, (8192, 2048), (2048, 1))
	    assert_size_stride(permute_41, (8192, 32), (32, 1))
	    assert_size_stride(permute_45, (32, 2048), (2048, 1))
	    assert_size_stride(permute_48, (8192, 2048), (2048, 1))
	    assert_size_stride(tangents_1, (1, s0, 2048), (2048*s0, 2048, 1))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mul, aten.view]
	        triton_poi_fused_mul_view_0_xnumel = 2048*s0
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_mul_view_0.run(tangents_1, buf0, triton_poi_fused_mul_view_0_xnumel, grid=grid(triton_poi_fused_mul_view_0_xnumel), stream=stream0)
	        buf1 = empty_strided_cuda((2048, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf0, (2048, s0), (1, 2048), 0), view_16, out=buf1)
	        del view_16
	        buf2 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf0, permute_20, out=buf2)
	        del permute_20
	        buf3 = empty_strided_cuda((2048, 32), (32, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf1, buf3, 65536, grid=grid(65536), stream=stream0)
	        buf4 = empty_strided_cuda((32, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf2, (32, s0), (1, 32), 0), view_14, out=buf4)
	        del view_14
	        buf5 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf2, permute_24, out=buf5)
	        del permute_24
	        buf6 = empty_strided_cuda((32, 8192), (8192, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_2.run(buf4, buf6, 262144, grid=grid(262144), stream=stream0)
	        buf7 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(tangents_1, (s0, 2048), (2048, 1), 0), permute_27, out=buf7)
	        del permute_27
	        del tangents_1
	        buf8 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        buf15 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        buf17 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        buf24 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [silu], Original ATen: [aten.add, aten.silu, aten.mul, aten.sigmoid, aten.fill, aten.sub]
	        triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3_xnumel = 8192*s0
	        triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3.run(buf5, buf7, add_39, add_85, buf8, buf15, buf17, buf24, triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3_xnumel, grid=grid(triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3_xnumel), stream=stream0)
	        del add_39
	        del add_85
	        del buf5
	        del buf7
	        buf9 = reinterpret_tensor(buf4, (8192, 32), (32, 1), 0); del buf4  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf8, (8192, s0), (1, 8192), 0), view_10, out=buf9)
	        del view_10
	        buf10 = buf2; del buf2  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf8, (s0, 8192), (8192, 1), 0), permute_30, out=buf10)
	        del buf8
	        del permute_30
	        buf11 = empty_strided_cuda((8192, 32), (32, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_2.run(buf9, buf11, 262144, grid=grid(262144), stream=stream0)
	        buf12 = reinterpret_tensor(buf1, (32, 2048), (2048, 1), 0); del buf1  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf10, (32, s0), (1, 32), 0), view_2, out=buf12)
	        buf13 = buf0; del buf0  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf10, permute_34, out=buf13)
	        del permute_34
	        buf14 = empty_strided_cuda((32, 2048), (2048, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf12, buf14, 65536, grid=grid(65536), stream=stream0)
	        buf16 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf15, (s0, 8192), (8192, 1), 0), permute_37, out=buf16)
	        del buf15
	        del permute_37
	        buf18 = buf9; del buf9  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf17, (8192, s0), (1, 8192), 0), view_4, out=buf18)
	        del view_4
	        buf19 = buf10; del buf10  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf17, (s0, 8192), (8192, 1), 0), permute_41, out=buf19)
	        del buf17
	        del permute_41
	        buf20 = empty_strided_cuda((8192, 32), (32, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_2.run(buf18, buf20, 262144, grid=grid(262144), stream=stream0)
	        del buf18
	        buf21 = buf12; del buf12  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf19, (32, s0), (1, 32), 0), view_2, out=buf21)
	        del view_2
	        buf22 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf19, permute_45, out=buf22)
	        del buf19
	        del permute_45
	        buf23 = empty_strided_cuda((32, 2048), (2048, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf21, buf23, 65536, grid=grid(65536), stream=stream0)
	        del buf21
	        buf25 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf24, (s0, 8192), (8192, 1), 0), permute_48, out=buf25)
	        del buf24
	        del permute_48
	        buf26 = reinterpret_tensor(buf13, (1, s0, 2048), (2048*s0, 2048, 1), 0); del buf13  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.add]
	        triton_poi_fused_add_4_xnumel = 2048*s0
	        triton_poi_fused_add_4.run(buf26, buf16, buf22, buf25, triton_poi_fused_add_4_xnumel, grid=grid(triton_poi_fused_add_4_xnumel), stream=stream0)
	        del buf16
	        del buf22
	        del buf25
	    return (None, buf26, None, None, None, None, None, None, None, None, None, None, buf23, buf20, None, None, None, None, None, None, None, None, None, None, buf14, buf11, None, None, None, None, None, None, None, None, None, None, buf6, buf3, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    primals_1 = 100
	    view_2 = rand_strided((100, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    view_4 = rand_strided((100, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    add_39 = rand_strided((1, 100, 8192), (819200, 8192, 1), device='cuda:0', dtype=torch.float16)
	    view_10 = rand_strided((100, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    add_85 = rand_strided((1, 100, 8192), (819200, 8192, 1), device='cuda:0', dtype=torch.float16)
	    view_14 = rand_strided((100, 8192), (8192, 1), device='cuda:0', dtype=torch.float16)
	    view_16 = rand_strided((100, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_20 = rand_strided((2048, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_24 = rand_strided((32, 8192), (8192, 1), device='cuda:0', dtype=torch.float16)
	    permute_27 = rand_strided((2048, 8192), (8192, 1), device='cuda:0', dtype=torch.float16)
	    permute_30 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_34 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    permute_37 = rand_strided((8192, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    permute_41 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_45 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    permute_48 = rand_strided((8192, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    tangents_1 = rand_strided((1, 100, 2048), (204800, 2048, 1), device='cuda:0', dtype=torch.float16)
	    fn = lambda: call([primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:29:22.325000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "25f5b7d949c219548a04baf795c16ba5"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742992162325069.8,
	"args": {
	"key": "fuwpd53kuljrtdqmdkesfbb743gwbdes3cogyfbosttj4mjzbydy",
	"components": [
	"[cjhczcrgi3pzc6cive3k3kqwasc5syujhgv54rquueioi4qb6w2] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1):\n    mul_144 = torch.ops.aten.mul.Tensor(tangents_1, 2.0)\n    view_18 = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None\n    permute_18 = torch.ops.aten.permute.default(view_18, [1, 0])\n    mm_9 = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None\n    permute_19 = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None\n    mm_10 = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None\n    view_19 = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None\n    permute_21 = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None\n    convert_element_type_39 = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None\n    view_20 = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None\n    permute_22 = torch.ops.aten.permute.default(view_20, [1, 0])\n    mm_11 = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None\n    permute_23 = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None\n    mm_12 = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None\n    view_21 = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None\n    permute_25 = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None\n    convert_element_type_44 = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None\n    view_22 = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None\n    mm_13 = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None\n    view_23 = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None\n    convert_element_type_default_2 = torch.ops.prims.convert_element_type.default(view_23, torch.float16);  view_23 = None\n    add_135 = torch.ops.aten.add.Tensor(view_21, convert_element_type_default_2);  view_21 = convert_element_type_default_2 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    mul_146 = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None\n    mul_147 = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None\n    mul_148 = torch.ops.aten.mul.Tensor(mul_146, 2.0)\n    view_24 = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None\n    permute_28 = torch.ops.aten.permute.default(view_24, [1, 0])\n    mm_14 = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None\n    permute_29 = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None\n    mm_15 = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None\n    view_25 = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None\n    permute_31 = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None\n    convert_element_type_54 = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None\n    view_26 = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None\n    permute_32 = torch.ops.aten.permute.default(view_26, [1, 0])\n    mm_16 = torch.ops.aten.mm.default(permute_32, view_2);  permute_32 = None\n    permute_33 = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None\n    mm_17 = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None\n    view_27 = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None\n    permute_35 = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None\n    convert_element_type_59 = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None\n    view_28 = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None\n    mm_18 = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None\n    view_29 = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None\n    convert_element_type_default_1 = torch.ops.prims.convert_element_type.default(view_29, torch.float16);  view_29 = None\n    add_136 = torch.ops.aten.add.Tensor(view_27, convert_element_type_default_1);  view_27 = convert_element_type_default_1 = None\n    sigmoid_1 = torch.ops.aten.sigmoid.default(add_39)\n    full_default = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)\n    sub_44 = torch.ops.aten.sub.Tensor(full_default, sigmoid_1);  full_default = None\n    mul_150 = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None\n    add_137 = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None\n    mul_151 = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None\n    mul_152 = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None\n    mul_153 = torch.ops.aten.mul.Tensor(mul_152, 2.0)\n    view_30 = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None\n    permute_39 = torch.ops.aten.permute.default(view_30, [1, 0])\n    mm_19 = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None\n    permute_40 = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None\n    mm_20 = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None\n    view_31 = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None\n    permute_42 = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None\n    convert_element_type_69 = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None\n    view_32 = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None\n    permute_43 = torch.ops.aten.permute.default(view_32, [1, 0])\n    mm_21 = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None\n    permute_44 = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None\n    mm_22 = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None\n    view_33 = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None\n    add_138 = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None\n    permute_46 = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None\n    convert_element_type_74 = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None\n    view_34 = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None\n    mm_23 = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None\n    view_35 = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None\n    convert_element_type_default = torch.ops.prims.convert_element_type.default(view_35, torch.float16);  view_35 = None\n    add_139 = torch.ops.aten.add.Tensor(add_138, convert_element_type_default);  add_138 = convert_element_type_default = None\n    return (None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[6pt52nx6wf2o3fnlscvbb6mpkzglnsxgtp26liujc4wx2tvtpm7] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4096*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[3]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[5]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[4ofuwkiqpjt5dr73pqxsydaoru4cj2w3m4sn674mztswxzkzyv5] example_inputs[6]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[kjiu5nyliqwsnl7icaeauzcjyk3boczmm4vcgqglpeyga4ezc26] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[buqdocmqej3phjqjaio73y24jqcnui6yzgcbitztcq67svbntcd] example_inputs[9]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hxsqnh7shhh7eup67u4ogv42dqcuywp53tt5tp7sdfgeqxuk32j] example_inputs[10]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[11]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[12]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[13]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[14]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[15]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[16]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[17]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_backward]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[n2y367w3cvigrpqrkfggnd4ki5vge46t7f3bihnrmz2hx2qkg6o] fx_kwargs[static_input_idxs]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]",
	"[giofkincgu73hppmew27gpl3zgvnfyyigbbqjyi5t7e6a5ldenw] fx_kwargs[user_visible_outputs]: {'add_139': None, 'convert_element_type_74': None, 'convert_element_type_69': None, 'convert_element_type_59': None, 'convert_element_type_54': None, 'convert_element_type_44': None, 'convert_element_type_39': None}",
	"[vhi4lnshnjmzxgwsuiowu552sqjv64oariyhqqha4hh3x7qlxkw] inputs_to_check[0]: 17",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 532666105,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:29:22.325000 324469 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "db354f4ac35fe5c12607d9e5788c27e9"}
	{"key": "fuwpd53kuljrtdqmdkesfbb743gwbdes3cogyfbosttj4mjzbydy", "components": ["[cjhczcrgi3pzc6cive3k3kqwasc5syujhgv54rquueioi4qb6w2] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1):\n    mul_144 = torch.ops.aten.mul.Tensor(tangents_1, 2.0)\n    view_18 = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None\n    permute_18 = torch.ops.aten.permute.default(view_18, [1, 0])\n    mm_9 = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None\n    permute_19 = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None\n    mm_10 = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None\n    view_19 = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None\n    permute_21 = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None\n    convert_element_type_39 = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None\n    view_20 = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None\n    permute_22 = torch.ops.aten.permute.default(view_20, [1, 0])\n    mm_11 = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None\n    permute_23 = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None\n    mm_12 = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None\n    view_21 = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None\n    permute_25 = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None\n    convert_element_type_44 = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None\n    view_22 = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None\n    mm_13 = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None\n    view_23 = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None\n    convert_element_type_default_2 = torch.ops.prims.convert_element_type.default(view_23, torch.float16);  view_23 = None\n    add_135 = torch.ops.aten.add.Tensor(view_21, convert_element_type_default_2);  view_21 = convert_element_type_default_2 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    mul_146 = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None\n    mul_147 = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None\n    mul_148 = torch.ops.aten.mul.Tensor(mul_146, 2.0)\n    view_24 = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None\n    permute_28 = torch.ops.aten.permute.default(view_24, [1, 0])\n    mm_14 = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None\n    permute_29 = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None\n    mm_15 = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None\n    view_25 = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None\n    permute_31 = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None\n    convert_element_type_54 = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None\n    view_26 = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None\n    permute_32 = torch.ops.aten.permute.default(view_26, [1, 0])\n    mm_16 = torch.ops.aten.mm.default(permute_32, view_2);  permute_32 = None\n    permute_33 = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None\n    mm_17 = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None\n    view_27 = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None\n    permute_35 = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None\n    convert_element_type_59 = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None\n    view_28 = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None\n    mm_18 = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None\n    view_29 = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None\n    convert_element_type_default_1 = torch.ops.prims.convert_element_type.default(view_29, torch.float16);  view_29 = None\n    add_136 = torch.ops.aten.add.Tensor(view_27, convert_element_type_default_1);  view_27 = convert_element_type_default_1 = None\n    sigmoid_1 = torch.ops.aten.sigmoid.default(add_39)\n    full_default = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)\n    sub_44 = torch.ops.aten.sub.Tensor(full_default, sigmoid_1);  full_default = None\n    mul_150 = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None\n    add_137 = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None\n    mul_151 = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None\n    mul_152 = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None\n    mul_153 = torch.ops.aten.mul.Tensor(mul_152, 2.0)\n    view_30 = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None\n    permute_39 = torch.ops.aten.permute.default(view_30, [1, 0])\n    mm_19 = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None\n    permute_40 = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None\n    mm_20 = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None\n    view_31 = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None\n    permute_42 = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None\n    convert_element_type_69 = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None\n    view_32 = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None\n    permute_43 = torch.ops.aten.permute.default(view_32, [1, 0])\n    mm_21 = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None\n    permute_44 = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None\n    mm_22 = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None\n    view_33 = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None\n    add_138 = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None\n    permute_46 = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None\n    convert_element_type_74 = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None\n    view_34 = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None\n    mm_23 = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None\n    view_35 = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None\n    convert_element_type_default = torch.ops.prims.convert_element_type.default(view_35, torch.float16);  view_35 = None\n    add_139 = torch.ops.aten.add.Tensor(add_138, convert_element_type_default);  add_138 = convert_element_type_default = None\n    return (None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[6pt52nx6wf2o3fnlscvbb6mpkzglnsxgtp26liujc4wx2tvtpm7] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4096*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[3]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[5]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[4ofuwkiqpjt5dr73pqxsydaoru4cj2w3m4sn674mztswxzkzyv5] example_inputs[6]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[kjiu5nyliqwsnl7icaeauzcjyk3boczmm4vcgqglpeyga4ezc26] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[buqdocmqej3phjqjaio73y24jqcnui6yzgcbitztcq67svbntcd] example_inputs[9]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hxsqnh7shhh7eup67u4ogv42dqcuywp53tt5tp7sdfgeqxuk32j] example_inputs[10]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[11]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[12]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[13]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[14]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[15]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[16]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[17]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_backward]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[n2y367w3cvigrpqrkfggnd4ki5vge46t7f3bihnrmz2hx2qkg6o] fx_kwargs[static_input_idxs]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]", "[giofkincgu73hppmew27gpl3zgvnfyyigbbqjyi5t7e6a5ldenw] fx_kwargs[user_visible_outputs]: {'add_139': None, 'convert_element_type_74': None, 'convert_element_type_69': None, 'convert_element_type_59': None, 'convert_element_type_54': None, 'convert_element_type_44': None, 'convert_element_type_39': None}", "[vhi4lnshnjmzxgwsuiowu552sqjv64oariyhqqha4hh3x7qlxkw] inputs_to_check[0]: 17", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 532666105, "cache_state": "hit"}
V0326 23:29:22.326000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "8e1d362bf52d3dbeda98e261b3406f3d"}
	{
	"name": "inductor_compile",
	"ts": 1742992162326455.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.326000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "dcfe549c3e588ec8215b7e2c0ba1d2c4"}
	{
	"name": "compile_fx_inner",
	"ts": 1742992162326811.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.327000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"bwd_compilation_metrics": {"compile_id": "3/1", "inductor_compile_time_s": 0.012452125549316406, "code_gen_time_s": null, "fail_type": null, "fail_reason": null}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:29:22.327000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "cb73b5fda70512d2c33218d168f956b9"}
	{
	"name": "compile_fx.<locals>.bw_compiler",
	"ts": 1742992162327398.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.329000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "705193f3a9696100b6fe0db8801afe64"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742992162329120.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.329000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "dbbae8ca9b709bb798e79b52645a9b5d"}
	{
	"name": "backend_compile",
	"ts": 1742992162329586.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.329000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "00a53526667673c20a7a0fa1859bc834"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742992162329834.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.372000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "86a98e36a779c74c7d1208b12bb95eab"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['x'], accessed_by=DictGetItemGuardAccessor(x)
	| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=True, size=[1, None, 2048], stride=[None, 2048, 1])
	| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False         
	| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['x'], L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight'], L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight'], L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight'], L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight'], L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight'], L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight'])
	| +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)
	| | +- TYPE_MATCH: ___check_type_id(L['self'], 105190333749776)                
	| | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | +- DICT_LENGTH: len(L['self']._modules) == 4                                
	| | | | +- GuardManager: source=L['self']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor(gate_proj)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj'], 105190356359888)
	| | | | | +- GuardManager: source=L['self']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].scaling, accessed_by=DictGetItemGuardAccessor(scaling)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj'].scaling) == 1           
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].scaling['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj'].scaling['default'] == 2.0   
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules) == 7          
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'], accessed_by=DictGetItemGuardAccessor(base_layer)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer'], 105190296992528)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['base_layer'].__dict__)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['base_layer']._parameters) == 2
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'], 105190296990752)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[8388608, 1], stride=[1, 1])
	| | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state, accessed_by=GetAttrGuardAccessor(quant_state)
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state, 105190295709968)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype == torch.bfloat16
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, 127872448968448)
	| | | | | | | | | | | | | +- LENGTH_CHECK: len(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], 127873236398144)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], 127873236398144)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[262144], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, 105190295709968)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[1024], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, 127873236398144)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, 127873236398144)
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['bias'], 127873236386624)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'].compute_dtype, accessed_by=DictGetItemGuardAccessor(compute_dtype)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj']._modules['base_layer'].compute_dtype == torch.bfloat16
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._modules['base_layer']._backward_hooks
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._modules['base_layer']._backward_pre_hooks
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'].compute_type_is_set, accessed_by=DictGetItemGuardAccessor(compute_type_is_set)
	| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['base_layer'].compute_type_is_set, 127873236421792)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout'], accessed_by=DictGetItemGuardAccessor(lora_dropout)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_dropout'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_dropout']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'], 105190279249888)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'].__dict__)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A'], accessed_by=DictGetItemGuardAccessor(lora_A)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_A'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DictGuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- KeyValueManager pair at index=0
	| | | | | | | | | | | +- KeyManager: GuardManager: source=list(L['self']._modules['gate_proj']._modules['lora_A']._modules.keys())[0]
	| | | | | | | | | | | | +- EQUALS_MATCH: list(L['self']._modules['gate_proj']._modules['lora_A']._modules.keys())[0] == 'default'
	| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_A']._modules['default'], 105190278900864)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['lora_A']._modules['default'].__dict__)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[32, 2048], stride=[2048, 1])
	| | | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['bias'], 127873236386624)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B'], accessed_by=DictGetItemGuardAccessor(lora_B)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_B'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_B']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_B']._modules['default'], 105190278900864)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['lora_B']._modules['default'].__dict__)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[8192, 32], stride=[32, 1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['bias'], 127873236386624)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_embedding_A'], accessed_by=DictGetItemGuardAccessor(lora_embedding_A)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_embedding_B'], accessed_by=DictGetItemGuardAccessor(lora_embedding_B)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_magnitude_vector'], accessed_by=DictGetItemGuardAccessor(lora_magnitude_vector)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].use_dora, accessed_by=DictGetItemGuardAccessor(use_dora)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj'].use_dora) == 1          
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].use_dora['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj'].use_dora['default'], 127873236421344)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._parameters             
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._active_adapter, accessed_by=DictGetItemGuardAccessor(_active_adapter)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._active_adapter, 127873236399136)
	| | | | | | | +- LENGTH_CHECK: len(L['self']._modules['gate_proj']._active_adapter) == 1   
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['up_proj']._active_adapter
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['down_proj']._active_adapter
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._active_adapter[0], accessed_by=ListGetItemGuardAccessor(0)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj']._active_adapter[0] == 'default'
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._backward_hooks         
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].merged_adapters, accessed_by=DictGetItemGuardAccessor(merged_adapters)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj'].merged_adapters, 127873236399136)
	| | | | | | | +- LENGTH_CHECK: not L['self']._modules['gate_proj'].merged_adapters         
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._disable_adapters, accessed_by=DictGetItemGuardAccessor(_disable_adapters)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._disable_adapters, 127873236421344)
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._backward_pre_hooks     
	| | | | +- GuardManager: source=L['self']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor(up_proj)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj'], 105190356359888)
	| | | | | +- GuardManager: source=L['self']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj'].scaling, accessed_by=DictGetItemGuardAccessor(scaling)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj'].scaling) == 1             
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj'].scaling['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['up_proj'].scaling['default'] == 2.0     
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules) == 7            
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'], accessed_by=DictGetItemGuardAccessor(base_layer)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer'], 105190296992528)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['base_layer'].__dict__)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['base_layer']._parameters) == 2
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'], 105190296990752)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[8388608, 1], stride=[1, 1])
	| | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state, accessed_by=GetAttrGuardAccessor(quant_state)
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state, 105190295709968)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype == torch.bfloat16
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, 127872448968448)
	| | | | | | | | | | | | | +- LENGTH_CHECK: len(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], 127873236398144)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], 127873236398144)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[262144], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, 105190295709968)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[1024], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, 127873236398144)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, 127873236398144)
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['bias'], 127873236386624)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'].compute_dtype, accessed_by=DictGetItemGuardAccessor(compute_dtype)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['up_proj']._modules['base_layer'].compute_dtype == torch.bfloat16
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._modules['base_layer']._backward_hooks
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._modules['base_layer']._backward_pre_hooks
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'].compute_type_is_set, accessed_by=DictGetItemGuardAccessor(compute_type_is_set)
	| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['base_layer'].compute_type_is_set, 127873236421792)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout'], accessed_by=DictGetItemGuardAccessor(lora_dropout)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_dropout'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_dropout']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'], 105190279249888)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'].__dict__)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A'], accessed_by=DictGetItemGuardAccessor(lora_A)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_A'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DictGuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- KeyValueManager pair at index=0
	| | | | | | | | | | | +- KeyManager: GuardManager: source=list(L['self']._modules['up_proj']._modules['lora_A']._modules.keys())[0]
	| | | | | | | | | | | | +- EQUALS_MATCH: list(L['self']._modules['up_proj']._modules['lora_A']._modules.keys())[0] == 'default'
	| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_A']._modules['default'], 105190278900864)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['lora_A']._modules['default'].__dict__)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[32, 2048], stride=[2048, 1])
	| | | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['bias'], 127873236386624)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B'], accessed_by=DictGetItemGuardAccessor(lora_B)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_B'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_B']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_B']._modules['default'], 105190278900864)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['lora_B']._modules['default'].__dict__)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[8192, 32], stride=[32, 1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['bias'], 127873236386624)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_embedding_A'], accessed_by=DictGetItemGuardAccessor(lora_embedding_A)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_embedding_B'], accessed_by=DictGetItemGuardAccessor(lora_embedding_B)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_magnitude_vector'], accessed_by=DictGetItemGuardAccessor(lora_magnitude_vector)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj'].use_dora, accessed_by=DictGetItemGuardAccessor(use_dora)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj'].use_dora) == 1            
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj'].use_dora['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj'].use_dora['default'], 127873236421344)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._parameters               
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._backward_hooks           
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj'].merged_adapters, accessed_by=DictGetItemGuardAccessor(merged_adapters)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj'].merged_adapters, 127873236399136)
	| | | | | | | +- LENGTH_CHECK: not L['self']._modules['up_proj'].merged_adapters           
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._disable_adapters, accessed_by=DictGetItemGuardAccessor(_disable_adapters)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._disable_adapters, 127873236421344)
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._backward_pre_hooks       
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._active_adapter, accessed_by=DictGetItemGuardAccessor(_active_adapter)
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['up_proj']._active_adapter
	| | | | +- GuardManager: source=L['self']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor(down_proj)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj'], 105190356359888)
	| | | | | +- GuardManager: source=L['self']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj'].scaling, accessed_by=DictGetItemGuardAccessor(scaling)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj'].scaling) == 1           
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj'].scaling['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['down_proj'].scaling['default'] == 2.0   
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules) == 7          
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'], accessed_by=DictGetItemGuardAccessor(base_layer)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer'], 105190296992528)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['base_layer'].__dict__)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['base_layer']._parameters) == 2
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'], 105190296990752)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[8388608, 1], stride=[1, 1])
	| | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state, accessed_by=GetAttrGuardAccessor(quant_state)
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state, 105190295709968)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype == torch.bfloat16
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, 127872448968448)
	| | | | | | | | | | | | | +- LENGTH_CHECK: len(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], 127873236398144)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], 127873236398144)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[262144], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, 105190295709968)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[1024], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, 127873236398144)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, 127873236398144)
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['bias'], 127873236386624)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'].compute_dtype, accessed_by=DictGetItemGuardAccessor(compute_dtype)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['down_proj']._modules['base_layer'].compute_dtype == torch.bfloat16
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._modules['base_layer']._backward_hooks
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._modules['base_layer']._backward_pre_hooks
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'].compute_type_is_set, accessed_by=DictGetItemGuardAccessor(compute_type_is_set)
	| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['base_layer'].compute_type_is_set, 127873236421792)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout'], accessed_by=DictGetItemGuardAccessor(lora_dropout)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_dropout'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_dropout']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'], 105190279249888)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'].__dict__)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A'], accessed_by=DictGetItemGuardAccessor(lora_A)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_A'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DictGuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- KeyValueManager pair at index=0
	| | | | | | | | | | | +- KeyManager: GuardManager: source=list(L['self']._modules['down_proj']._modules['lora_A']._modules.keys())[0]
	| | | | | | | | | | | | +- EQUALS_MATCH: list(L['self']._modules['down_proj']._modules['lora_A']._modules.keys())[0] == 'default'
	| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_A']._modules['default'], 105190278900864)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['lora_A']._modules['default'].__dict__)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[32, 8192], stride=[8192, 1])
	| | | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['bias'], 127873236386624)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B'], accessed_by=DictGetItemGuardAccessor(lora_B)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_B'], 105190278640688)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_B']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_B']._modules['default'], 105190278900864)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['lora_B']._modules['default'].__dict__)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[2048, 32], stride=[32, 1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['bias'], 127873236386624)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_embedding_A'], accessed_by=DictGetItemGuardAccessor(lora_embedding_A)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_embedding_B'], accessed_by=DictGetItemGuardAccessor(lora_embedding_B)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_magnitude_vector'], accessed_by=DictGetItemGuardAccessor(lora_magnitude_vector)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj'].use_dora, accessed_by=DictGetItemGuardAccessor(use_dora)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj'].use_dora) == 1          
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj'].use_dora['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj'].use_dora['default'], 127873236421344)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._parameters             
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._backward_hooks         
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj'].merged_adapters, accessed_by=DictGetItemGuardAccessor(merged_adapters)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj'].merged_adapters, 127873236399136)
	| | | | | | | +- LENGTH_CHECK: not L['self']._modules['down_proj'].merged_adapters         
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._disable_adapters, accessed_by=DictGetItemGuardAccessor(_disable_adapters)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._disable_adapters, 127873236421344)
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._backward_pre_hooks     
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._active_adapter, accessed_by=DictGetItemGuardAccessor(_active_adapter)
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['down_proj']._active_adapter
	| | | | +- GuardManager: source=L['self']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor(act_fn)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['act_fn'], 105190279291392)
	| | | | | +- GuardManager: source=L['self']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['act_fn'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor(inplace)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['act_fn'].inplace, 127873236421344)
	| | | +- GuardManager: source=L['self']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | +- DICT_LENGTH: not L['self']._parameters                                   
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['F'], accessed_by=DictGetItemGuardAccessor(F)
	| | | +- ID_MATCH: ___check_obj_id(G['F'], 127867906986112)                    
	| | | +- GuardManager: source=G['F'].dequantize_4bit, accessed_by=GetAttrGuardAccessor(dequantize_4bit)
	| | | | +- GuardManager: source=G['F'].dequantize_4bit.__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | | +- ID_MATCH: ___check_obj_id(G['F'].dequantize_4bit.__code__, 127870597942896)
	| | | | +- GuardManager: source=G['F'].dequantize_4bit, accessed_by=FuncDefaultsGuardAccessor
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[1], accessed_by=GetItemGuardAccessor(1)
	| | | | | | +- ID_MATCH: ___check_obj_id(G['F'].dequantize_4bit.__defaults__[1], 127873236386624)
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[2], accessed_by=GetItemGuardAccessor(2)
	| | | | | | +- ID_MATCH: ___check_obj_id(G['F'].dequantize_4bit.__defaults__[2], 127873236386624)
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[3], accessed_by=GetItemGuardAccessor(3)
	| | | | | | +- EQUALS_MATCH: G['F'].dequantize_4bit.__defaults__[3] == 64                
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[4], accessed_by=GetItemGuardAccessor(4)
	| | | | | | +- EQUALS_MATCH: G['F'].dequantize_4bit.__defaults__[4] == 'fp4'             
	| | +- GuardManager: source=G['prod'], accessed_by=DictGetItemGuardAccessor(prod)
	| | | +- ID_MATCH: ___check_obj_id(G['prod'], 127873216027824)                 
	| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)
	| | | +- ID_MATCH: ___check_obj_id(G['torch'], 127872889574384)                
	| | | +- OBJECT_ALIASING: G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch is G['torch']
	| | | +- OBJECT_ALIASING: G['torch'] is G['__import_kernels'].torch                   
	| | | +- GuardManager: source=G['torch'].nn, accessed_by=GetAttrGuardAccessor(nn)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].nn, 127869274618384)             
	| | | | +- GuardManager: source=G['torch'].nn.functional, accessed_by=GetAttrGuardAccessor(functional)
	| | | | | +- ID_MATCH: ___check_obj_id(G['torch'].nn.functional, 127867930467264)  
	| | | | | +- GuardManager: source=G['torch'].nn.functional.linear, accessed_by=GetAttrGuardAccessor(linear)
	| | | | | | +- ID_MATCH: ___check_obj_id(G['torch'].nn.functional.linear, 127872854208768)
	| | | +- GuardManager: source=G['torch'].numel, accessed_by=GetAttrGuardAccessor(numel)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].numel, 127872890415168)          
	| | | +- GuardManager: source=G['torch'].matmul, accessed_by=GetAttrGuardAccessor(matmul)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].matmul, 127872890441616)         
	| | +- GuardManager: source=G['Params4bit'], accessed_by=DictGetItemGuardAccessor(Params4bit)
	| | | +- ID_MATCH: ___check_obj_id(G['Params4bit'], 105190296990752)           
	| | +- GuardManager: source=G['__import_kernels'], accessed_by=DictGetItemGuardAccessor(__import_kernels)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'], 127867504994128)     
	| | | +- GuardManager: source=G['__import_kernels'].DEBUG_FLAG, accessed_by=GetAttrGuardAccessor(DEBUG_FLAG)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'].DEBUG_FLAG, 127873236421344)
	| | | +- GuardManager: source=G['__import_kernels'].fused_dequantize_op, accessed_by=GetAttrGuardAccessor(fused_dequantize_op)
	| | | | +- TYPE_MATCH: ___check_type_id(G['__import_kernels'].fused_dequantize_op, 105190280786160)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'].fused_dequantize_op, 127867502131200)
	| | | | +- GuardManager: source=G['__import_kernels'].fused_dequantize_op._opoverload, accessed_by=GetAttrGuardAccessor(_opoverload)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'].fused_dequantize_op._opoverload, 127867502233600)
	| | | +- GuardManager: source=G['__import_kernels'].torch, accessed_by=GetAttrGuardAccessor(torch)
	| | | | +- OBJECT_ALIASING: G['torch'] is G['__import_kernels'].torch                   
	| | | | +- OBJECT_ALIASING: G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch is G['__import_kernels'].torch
	| | +- GuardManager: source=G['fused_dequantize'], accessed_by=DictGetItemGuardAccessor(fused_dequantize)
	| | | +- GuardManager: source=G['fused_dequantize'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize'].__code__, 105190312005664)
	| | +- GuardManager: source=G['ENABLE_ASSERTIONS'], accessed_by=DictGetItemGuardAccessor(ENABLE_ASSERTIONS)
	| | | +- ID_MATCH: ___check_obj_id(G['ENABLE_ASSERTIONS'], 127873236421344)    
	| | +- GuardManager: source=G['get_data_transposed'], accessed_by=DictGetItemGuardAccessor(get_data_transposed)
	| | | +- GuardManager: source=G['get_data_transposed'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | +- ID_MATCH: ___check_obj_id(G['get_data_transposed'].__code__, 105190312005664)
	| | +- GuardManager: source=G['TransposeBMatMul4Bit'], accessed_by=DictGetItemGuardAccessor(TransposeBMatMul4Bit)
	| | | +- ID_MATCH: ___check_obj_id(G['TransposeBMatMul4Bit'], 105190317042032) 
	| | | +- GuardManager: source=G['TransposeBMatMul4Bit'].forward, accessed_by=GetAttrGuardAccessor(forward)
	| | | | +- ID_MATCH: ___check_obj_id(G['TransposeBMatMul4Bit'].forward, 127867500433536)
	| | +- GuardManager: source=G['__builtins_dict___14'], accessed_by=DictGetItemGuardAccessor(__builtins_dict___14)
	| | | +- GuardManager: source=G['__builtins_dict___14']['any'], accessed_by=DictGetItemGuardAccessor(any)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___14']['any'], 127873218839552)
	| | | +- GuardManager: source=G['__builtins_dict___14']['str'], accessed_by=DictGetItemGuardAccessor(str)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___14']['str'], 127873236368832)
	| | | +- GuardManager: source=G['__builtins_dict___14']['bool'], accessed_by=DictGetItemGuardAccessor(bool)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___14']['bool'], 127873236421376)
	| | | +- GuardManager: source=G['__builtins_dict___14']['getattr'], accessed_by=DictGetItemGuardAccessor(getattr)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___14']['getattr'], 127873218840592)
	| | | +- GuardManager: source=G['__builtins_dict___14']['isinstance'], accessed_by=DictGetItemGuardAccessor(isinstance)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___14']['isinstance'], 127873218841152)
	| | +- GuardManager: source=G['fix_4bit_weight_quant_state_from_module'], accessed_by=DictGetItemGuardAccessor(fix_4bit_weight_quant_state_from_module)
	| | | +- GuardManager: source=G['fix_4bit_weight_quant_state_from_module'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | +- ID_MATCH: ___check_obj_id(G['fix_4bit_weight_quant_state_from_module'].__code__, 105190296796912)
	| | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'], accessed_by=DictGetItemGuardAccessor(__import_bitsandbytes_dot_nn_dot_modules)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'], 127867905299120)
	| | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'], accessed_by=DictGetItemGuardAccessor(__import_peft_dot_tuners_dot_lora_dot_bnb)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_lora_dot_bnb'], 127867285689408)
	| | | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch, accessed_by=GetAttrGuardAccessor(torch)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch, 127872889574384)
	| | | | +- OBJECT_ALIASING: G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch is G['torch']
	| | | | +- OBJECT_ALIASING: G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch is G['__import_kernels'].torch
	| | | | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.nn, accessed_by=GetAttrGuardAccessor(nn)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.nn, 127869274618384)
	| | | | | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.nn.functional, accessed_by=GetAttrGuardAccessor(functional)
	| | | | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.nn.functional
	| | | | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.numel, accessed_by=GetAttrGuardAccessor(numel)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.numel, 127872890415168)
	| | | | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.matmul, accessed_by=GetAttrGuardAccessor(matmul)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.matmul, 127872890441616)
	| | | | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.is_autocast_enabled, accessed_by=GetAttrGuardAccessor(is_autocast_enabled)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.is_autocast_enabled, 127872890272512)
	| | +- GuardManager: source=G['__import_peft_dot_tuners_dot_tuners_utils'], accessed_by=DictGetItemGuardAccessor(__import_peft_dot_tuners_dot_tuners_utils)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_tuners_utils'], 127867292952720)
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_linear)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'], 127867930467104)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F, accessed_by=GetAttrGuardAccessor(F)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F, 127867930467264)
	| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_torch_dot_nn_dot_modules_dot_activation'].F
	| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.nn.functional
	| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F.silu, accessed_by=GetAttrGuardAccessor(silu)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F.silu, 127867928434368)
	| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F.linear, accessed_by=GetAttrGuardAccessor(linear)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F.linear, 127872854208768)
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_module)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'], 127869274705968)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_pre_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_pre_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_activation'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_activation)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_activation'], 127867928927008)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_activation'].F, accessed_by=GetAttrGuardAccessor(F)
	| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_torch_dot_nn_dot_modules_dot_activation'].F
	+- LAMBDA_GUARD: L['x'].stride()[0] == 2048*L['x'].size()[1]                   # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1] == 2048  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0] == 8192  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1] == 2048  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0] == 8192  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1] == 8192  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0] == 2048  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['x'].size()[1] <= 262143                               # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0326 23:29:22.373000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "4676b78019a8dcd677aac365dddafb3a"}
	{
	"name": "entire_frame_compile",
	"ts": 1742992162373149.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.373000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "11f729b5d416e05047b814565c128865"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742992162373412.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:29:22.373000 324469 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "3/1", "frame_key": "8", "co_name": "compiled_llama_mlp", "co_filename": "/tmp/ipykernel_324469/318589162.py", "co_firstlineno": 10, "cache_size": 1, "accumulated_cache_size": 1, "guard_count": 342, "shape_env_guard_count": 29, "graph_op_count": 32, "graph_node_count": 77, "graph_input_count": 38, "start_time": 1742992161.724059, "entire_frame_compile_time_s": 0.6490166187286377, "backend_compile_time_s": 0.364727258682251, "inductor_compile_time_s": 0.01957535743713379, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": ["mylib::fused_dequantize_op"], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
