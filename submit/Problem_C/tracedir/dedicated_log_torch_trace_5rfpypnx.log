V0326 23:52:04.064000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", 0]}
V0326 23:52:04.065000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["<frozen runpy>", 1]}
V0326 23:52:04.065000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/ipykernel_launcher.py", 2]}
V0326 23:52:04.065000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/traitlets/config/application.py", 3]}
V0326 23:52:04.066000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py", 4]}
V0326 23:52:04.066000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py", 5]}
V0326 23:52:04.066000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/usr/lib/python3.12/asyncio/base_events.py", 6]}
V0326 23:52:04.066000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/usr/lib/python3.12/asyncio/events.py", 7]}
V0326 23:52:04.066000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py", 8]}
V0326 23:52:04.066000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py", 9]}
V0326 23:52:04.067000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py", 10]}
V0326 23:52:04.067000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py", 11]}
V0326 23:52:04.067000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py", 12]}
V0326 23:52:04.067000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/tmp/ipykernel_331427/999499299.py", 13]}
V0326 23:52:04.067000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/transformers/trainer.py", 14]}
V0326 23:52:04.068000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py", 15]}
V0326 23:52:04.068000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", 16]}
V0326 23:52:04.068000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py", 17]}
V0326 23:52:04.068000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py", 18]}
V0326 23:52:04.068000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/peft_model.py", 19]}
V0326 23:52:04.068000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py", 20]}
V0326 23:52:04.069000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py", 21]}
V0326 23:52:04.069000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", 22]}
V0326 23:52:04.069000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/tmp/ipykernel_331427/3685874839.py", 23]}
V0326 23:52:04.069000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 333, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 1, "name": "compiled_rms_layernorm", "filename": 23}]}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:04.069000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "453195593ceee32710bdaf6e931131d8"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993524069866.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:04.070000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "40c773262c005d92ea80e1f03e73e8d2"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993524069866.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:04.083000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 0, "size": 409600}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:04.083000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 3, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [1, 100, 2048], "is_leaf": true, "requires_grad": true, "stride": [204800, 2048, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ff9f025a800>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:04.084000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 0, "source": "L['hidden_states']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:04.119000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 0, "size": 4096}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:04.120000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 1, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [2048], "is_leaf": true, "is_parameter": true, "stride": [1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7ffa30956990>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:04.120000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 2, "source": "L['self']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:04.126000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", 24]}
V0326 23:52:04.126000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_utils_internal.py", 25]}
V0326 23:52:04.126000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", 26]}
V0326 23:52:04.126000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", 27]}
V0326 23:52:04.127000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/builtin.py", 28]}
V0326 23:52:04.127000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/builder.py", 29]}
V0326 23:52:04.127000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/utils.py", 30]}
V0326 23:52:04.127000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/utils/_stats.py", 31]}
V0326 23:52:04.127000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", 32]}
V0326 23:52:04.127000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_subclasses/fake_impls.py", 33]}
V0326 23:52:04.128000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/__init__.py", 34]}
V0326 23:52:04.128000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py", 35]}
V0326 23:52:04.128000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/fx/experimental/sym_node.py", 36]}
V0326 23:52:04.128000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/fx/experimental/recording.py", 37]}
V0326 23:52:04.128000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_logging/_internal.py", 38]}
V0326 23:52:04.129000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s1", "sources": ["L['hidden_states'].size()[2]"], "value": "2048", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 333, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2220, "name": "BINARY_OP", "filename": 27}, {"line": 301, "name": "impl", "filename": 27}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 711, "name": "<lambda>", "filename": 28}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 943, "name": "_handle_insert_op_in_graph", "filename": 28}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 1924, "name": "_dispatch_impl", "filename": 32}, {"line": 831, "name": "fast_binary_impl", "filename": 33}, {"line": 785, "name": "infer_size", "filename": 33}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 7, "name": "compiled_rms_layernorm", "filename": 23}]}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:04.135000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_hidden_states_": [1, "s0", "2048"], "l_self_parameters_weight_": [2048], "hidden_states": [1, "s0", "2048"], "pow_1": [1, "s0", "2048"], "variance": [1, "s0", 1], "add": [1, "s0", 1], "rsqrt": [1, "s0", 1], "hidden_states_1": [1, "s0", "2048"], "to_1": [1, "s0", "2048"], "mul_1": [1, "s0", 2048]}}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "d3ee9c302c9ce00e48e6eca1c53a2776"}
	class GraphModule(torch.nn.Module):
	    def forward(self, s0: "Sym(s0)", L_hidden_states_: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", L_self_parameters_weight_: "f16[2048][1]cuda:0"):
	        l_hidden_states_ = L_hidden_states_
	        l_self_parameters_weight_ = L_self_parameters_weight_
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:4 in compiled_rms_layernorm, code: hidden_states = hidden_states.to(torch.float32)
	        hidden_states: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = l_hidden_states_.to(torch.float32);  l_hidden_states_ = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:5 in compiled_rms_layernorm, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
	        pow_1: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = hidden_states.pow(2)
	        variance: "f32[1, s0, 1][s0, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:6 in compiled_rms_layernorm, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
	        add: "f32[1, s0, 1][s0, 1, 1]cuda:0" = variance + 1e-05;  variance = None
	        rsqrt: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
	        hidden_states_1: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:7 in compiled_rms_layernorm, code: return self.weight * hidden_states.to(input_dtype)
	        to_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = hidden_states_1.to(torch.float16);  hidden_states_1 = None
	        mul_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = l_self_parameters_weight_ * to_1;  l_self_parameters_weight_ = to_1 = None
	        return (mul_1,)
	        
V0326 23:52:04.135000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "e9d488f96373a0d64064af83a77832ad"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993524135740.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:04.136000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "3e1219a60b423bbfae1bbbf07d5da7d9"}
	{
	"name": "backend_compile",
	"ts": 1742993524135740.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:04.549000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "768bd8a5f95ab164b8a643d67074954d"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993524548910.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:04.599000 331427 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "6448ec3baef605600b7af0e432477381"}
	class joint_helper(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "Sym(s0)"; primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"; primals_3: "f16[2048][1]cuda:0"; tangents_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /tmp/ipykernel_331427/3685874839.py:4 in compiled_rms_layernorm, code: hidden_states = hidden_states.to(torch.float32)
	        convert_element_type: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.float32);  primals_2 = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:5 in compiled_rms_layernorm, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
	        pow_1: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 2)
	        mean: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:6 in compiled_rms_layernorm, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
	        add_9: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
	        rsqrt: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_9);  add_9 = None
	        alias: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt)
	        alias_1: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.alias.default(alias);  alias = None
	        mul_15: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type, rsqrt)
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:7 in compiled_rms_layernorm, code: return self.weight * hidden_states.to(input_dtype)
	        convert_element_type_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_15, torch.float16);  mul_15 = None
	        mul_22: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_3, convert_element_type_1);  convert_element_type_1 = None
	        mul_26: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, primals_3);  tangents_1 = primals_3 = None
	        convert_element_type_2: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_26, torch.float32);  mul_26 = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:6 in compiled_rms_layernorm, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
	        mul_27: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2, convert_element_type)
	        mul_28: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2, rsqrt);  convert_element_type_2 = rsqrt = None
	        sum_1: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_27, [2], True, dtype = torch.float32);  mul_27 = None
	        alias_2: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
	        alias_3: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
	        pow_2: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_3, 3);  alias_3 = None
	        mul_29: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_1, -0.5);  sum_1 = None
	        mul_30: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, pow_2);  mul_29 = pow_2 = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:5 in compiled_rms_layernorm, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
	        expand: "f32[1, s0, 2048][s0, 1, 0]cuda:0" = torch.ops.aten.expand.default(mul_30, [1, primals_1, 2048]);  mul_30 = primals_1 = None
	        div: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.div.Scalar(expand, 2048);  expand = None
	        pow_3: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 1.0);  convert_element_type = None
	        mul_31: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_3, 2.0);  pow_3 = None
	        mul_32: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div, mul_31);  div = mul_31 = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:5 in compiled_rms_layernorm, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
	        add_28: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_28, mul_32);  mul_28 = mul_32 = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:4 in compiled_rms_layernorm, code: hidden_states = hidden_states.to(torch.float32)
	        convert_element_type_3: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_28, torch.float16);  add_28 = None
	        return pytree.tree_unflatten([mul_22, None, convert_element_type_3, None], self._out_spec)
	        
V0326 23:52:04.723000 331427 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "bb76c734c1452a8c9a0de35ac8e8dc59"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "Sym(s0)", primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", primals_3: "f16[2048][1]cuda:0"):
	         # File: /tmp/ipykernel_331427/3685874839.py:4 in compiled_rms_layernorm, code: hidden_states = hidden_states.to(torch.float32)
	        convert_element_type: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.float32)
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:5 in compiled_rms_layernorm, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
	        pow_1: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 2)
	        mean: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:6 in compiled_rms_layernorm, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
	        add_9: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
	        rsqrt: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_9);  add_9 = None
	        mul_15: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type, rsqrt);  convert_element_type = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:7 in compiled_rms_layernorm, code: return self.weight * hidden_states.to(input_dtype)
	        convert_element_type_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_15, torch.float16);  mul_15 = None
	        mul_22: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_3, convert_element_type_1);  convert_element_type_1 = None
	        return (mul_22, primals_2, primals_3, rsqrt, primals_1)
	        
V0326 23:52:04.724000 331427 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "45c5889084317b4c576a3c3db273c0d4"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "Sym(s0)", primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", primals_3: "f16[2048][1]cuda:0", rsqrt: "f32[1, s0, 1][s0, 1, 1]cuda:0", tangents_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"):
	         # File: /tmp/ipykernel_331427/3685874839.py:7 in compiled_rms_layernorm, code: return self.weight * hidden_states.to(input_dtype)
	        mul_26: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, primals_3);  tangents_1 = primals_3 = None
	        convert_element_type_2: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_26, torch.float32);  mul_26 = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:4 in compiled_rms_layernorm, code: hidden_states = hidden_states.to(torch.float32)
	        convert_element_type: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.float32);  primals_2 = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:6 in compiled_rms_layernorm, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
	        mul_27: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2, convert_element_type)
	        mul_28: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2, rsqrt);  convert_element_type_2 = None
	        sum_1: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_27, [2], True, dtype = torch.float32);  mul_27 = None
	        pow_2: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt, 3);  rsqrt = None
	        mul_29: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_1, -0.5);  sum_1 = None
	        mul_30: "f32[1, s0, 1][s0, 1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, pow_2);  mul_29 = pow_2 = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:5 in compiled_rms_layernorm, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
	        expand: "f32[1, s0, 2048][s0, 1, 0]cuda:0" = torch.ops.aten.expand.default(mul_30, [1, primals_1, 2048]);  mul_30 = primals_1 = None
	        div: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.div.Scalar(expand, 2048);  expand = None
	        pow_3: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 1.0);  convert_element_type = None
	        mul_31: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_3, 2.0);  pow_3 = None
	        mul_32: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div, mul_31);  div = mul_31 = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:5 in compiled_rms_layernorm, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
	        add_28: "f32[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_28, mul_32);  mul_28 = mul_32 = None
	        
	         # File: /tmp/ipykernel_331427/3685874839.py:4 in compiled_rms_layernorm, code: hidden_states = hidden_states.to(torch.float32)
	        convert_element_type_3: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_28, torch.float16);  add_28 = None
	        return (None, convert_element_type_3, None)
	        
V0326 23:52:04.725000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "b76447ea93780e3f3b4b125b77f4e450"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993524725272.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:04.725000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "7d73f64c40ddbf16870e25d41e0633be"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993524725867.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:04.726000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "751f2c14eee2b957727b3f7a8e077271"}
	{
	"name": "inductor_compile",
	"ts": 1742993524725867.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.033000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/ol/colxjf5anh2mdbnjhp6dmderb4e35tzxhwlbgmfqa7pp47rrypuj.py"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "b98c3b54eb76476c8d4e0e65aacd5726"}
	# AOT ID: ['0_forward']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/cf/ccfxfxxhz45i7ck6up4hcseaddtu7awjgch7xkmk7ogkvm3vpyve.py
	# Topologically Sorted Source Nodes: [hidden_states, pow_1, variance, add, rsqrt, hidden_states_1, to_1, mul_1], Original ATen: [aten._to_copy, aten.pow, aten.mean, aten.add, aten.rsqrt, aten.mul]
	# Source node to ATen node mapping:
	#   add => add_9
	#   hidden_states => convert_element_type
	#   hidden_states_1 => mul_15
	#   mul_1 => mul_22
	#   pow_1 => pow_1
	#   rsqrt => rsqrt
	#   to_1 => convert_element_type_1
	#   variance => mean
	# Graph fragment:
	#   %convert_element_type : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_2, torch.float32), kwargs = {})
	#   %pow_1 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%convert_element_type, 2), kwargs = {})
	#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_1, [-1], True), kwargs = {})
	#   %add_9 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean, 1e-05), kwargs = {})
	#   %rsqrt : [num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_9,), kwargs = {})
	#   %mul_15 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type, %rsqrt), kwargs = {})
	#   %convert_element_type_1 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_15, torch.float16), kwargs = {})
	#   %mul_22 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_3, %convert_element_type_1), kwargs = {})
	triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.reduction(
	    size_hints=[128, 2048],
	    reduction_hint=ReductionHint.INNER,
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp32', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_0', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
	)
	@triton.jit
	def triton_(in_out_ptr0, in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
	    rnumel = 2048
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = xindex < xnumel
	    rbase = tl.arange(0, RBLOCK)[None, :]
	    x0 = xindex
	    _tmp4 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
	    for roffset in range(0, rnumel, RBLOCK):
	        rindex = roffset + rbase
	        rmask = rindex < rnumel
	        r1 = rindex
	        tmp0 = tl.load(in_ptr0 + (r1 + (2048*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
	        tmp1 = tmp0.to(tl.float32)
	        tmp2 = tmp1 * tmp1
	        tmp3 = tl.broadcast_to(tmp2, [XBLOCK, RBLOCK])
	        tmp5 = _tmp4 + tmp3
	        _tmp4 = tl.where(rmask & xmask, tmp5, _tmp4)
	    tmp4 = tl.sum(_tmp4, 1)[:, None]
	    tmp6 = 2048.0
	    tmp7 = tmp4 / tmp6
	    tmp8 = 1e-05
	    tmp9 = tmp7 + tmp8
	    tmp10 = libdevice.rsqrt(tmp9)
	    tl.debug_barrier()
	    tl.store(in_out_ptr0 + (x0), tmp10, xmask)
	    for roffset in range(0, rnumel, RBLOCK):
	        rindex = roffset + rbase
	        rmask = rindex < rnumel
	        r1 = rindex
	        tmp11 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
	        tmp12 = tl.load(in_ptr0 + (r1 + (2048*x0)), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
	        tmp13 = tmp12.to(tl.float32)
	        tmp14 = tmp13 * tmp10
	        tmp15 = tmp14.to(tl.float32)
	        tmp16 = tmp11 * tmp15
	        tl.store(out_ptr0 + (r1 + (2048*x0)), tmp16, rmask & xmask)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    primals_1, primals_2, primals_3 = args
	    args.clear()
	    s0 = primals_1
	    assert_size_stride(primals_2, (1, s0, 2048), (2048*s0, 2048, 1))
	    assert_size_stride(primals_3, (2048, ), (1, ))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((1, s0, 1), (s0, 1, s0), torch.float32)
	        buf1 = reinterpret_tensor(buf0, (1, s0, 1), (s0, 1, 1), 0); del buf0  # reuse
	        buf2 = empty_strided_cuda((1, s0, 2048), (2048*s0, 2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [hidden_states, pow_1, variance, add, rsqrt, hidden_states_1, to_1, mul_1], Original ATen: [aten._to_copy, aten.pow, aten.mean, aten.add, aten.rsqrt, aten.mul]
	        stream0 = get_raw_stream(0)
	        triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_0.run(buf1, primals_2, primals_3, buf2, s0, 2048, grid=grid(s0), stream=stream0)
	    return (buf2, primals_2, primals_3, buf1, s0, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    primals_1 = 100
	    primals_2 = rand_strided((1, 100, 2048), (204800, 2048, 1), device='cuda:0', dtype=torch.float16)
	    primals_3 = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
	    fn = lambda: call([primals_1, primals_2, primals_3])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:52:05.034000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "3d80ad597739ba48bcf53929145a8082"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742993525034133.8,
	"args": {
	"key": "fd3xqug2n3niasr2o2256x5c2rwhs6jkrqjoda67njvfakanm7qg",
	"components": [
	"[ljcxhldnyyhoosb2awv34zperzctwcujy2e2ntokpenfwbzyniy] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3):\n    convert_element_type = torch.ops.prims.convert_element_type.default(primals_2, torch.float32)\n    pow_1 = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 2)\n    mean = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None\n    add_9 = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None\n    rsqrt = torch.ops.aten.rsqrt.default(add_9);  add_9 = None\n    mul_15 = torch.ops.aten.mul.Tensor(convert_element_type, rsqrt);  convert_element_type = None\n    convert_element_type_1 = torch.ops.prims.convert_element_type.default(mul_15, torch.float16);  mul_15 = None\n    mul_22 = torch.ops.aten.mul.Tensor(primals_3, convert_element_type_1);  convert_element_type_1 = None\n    return (mul_22, primals_2, primals_3, rsqrt, primals_1)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[y2xmti2d5nfvoydvw5fwmyz3r65x7l5lz4kteznxv3qzpai5dku] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[4vufduoeo5swmf7jicwhszdzxvtgpacvh5sgv4h7fniqr6z35sv] fx_kwargs[static_input_idxs]: [2]",
	"[vj2utvrrmkqiulu3xnz7mk3wbv3zlgeqljk6t5ixxgrruq43k7b] fx_kwargs[user_visible_outputs]: {'mul_22': None}",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 528910043,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:52:05.034000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "0fd04df68080b5e4e96a1698930c3328"}
	{"key": "fd3xqug2n3niasr2o2256x5c2rwhs6jkrqjoda67njvfakanm7qg", "components": ["[ljcxhldnyyhoosb2awv34zperzctwcujy2e2ntokpenfwbzyniy] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3):\n    convert_element_type = torch.ops.prims.convert_element_type.default(primals_2, torch.float32)\n    pow_1 = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 2)\n    mean = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None\n    add_9 = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None\n    rsqrt = torch.ops.aten.rsqrt.default(add_9);  add_9 = None\n    mul_15 = torch.ops.aten.mul.Tensor(convert_element_type, rsqrt);  convert_element_type = None\n    convert_element_type_1 = torch.ops.prims.convert_element_type.default(mul_15, torch.float16);  mul_15 = None\n    mul_22 = torch.ops.aten.mul.Tensor(primals_3, convert_element_type_1);  convert_element_type_1 = None\n    return (mul_22, primals_2, primals_3, rsqrt, primals_1)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[y2xmti2d5nfvoydvw5fwmyz3r65x7l5lz4kteznxv3qzpai5dku] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[4vufduoeo5swmf7jicwhszdzxvtgpacvh5sgv4h7fniqr6z35sv] fx_kwargs[static_input_idxs]: [2]", "[vj2utvrrmkqiulu3xnz7mk3wbv3zlgeqljk6t5ixxgrruq43k7b] fx_kwargs[user_visible_outputs]: {'mul_22': None}", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 528910043, "cache_state": "hit"}
V0326 23:52:05.036000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "aaa731f4a7989c134a8b3f400dc92003"}
	{
	"name": "inductor_compile",
	"ts": 1742993525036187.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 1,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.036000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "8118774d081d5d3a4f6e32dd1c5ca0b1"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993525036769.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 1,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.037000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "6eaebca015f5bb62b5c17c8a7179649e"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993525037359.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 1,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.038000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "d7ba17b1ef6fc6d8af7cb443cfe8984e"}
	{
	"name": "compile_fx.<locals>.bw_compiler",
	"ts": 1742993525037974.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.038000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "8ba1d599b697662c38c5748fa1b3ca07"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993525038390.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.038000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "9faea503468d2d65815893324091125f"}
	{
	"name": "inductor_compile",
	"ts": 1742993525038390.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.049000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/2j/c2j2yc6bujankaeuksegulsdlhmfbtuxl7di5ye75akihhdxc7g2.py"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "f21d95a51e415dc2d04dcbd06043e91d"}
	# AOT ID: ['0_backward']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/ig/cigvf5bf5wgns5w5s6rzi5dmp46jajb43gyhameg45qklmsz7xdm.py
	# Topologically Sorted Source Nodes: [hidden_states], Original ATen: [aten.mul, aten._to_copy, aten.sum, aten.div, aten.pow, aten.add]
	# Source node to ATen node mapping:
	#   hidden_states => convert_element_type
	# Graph fragment:
	#   %mul_26 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%tangents_1, %primals_3), kwargs = {})
	#   %convert_element_type_2 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_26, torch.float32), kwargs = {})
	#   %convert_element_type : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_2, torch.float32), kwargs = {})
	#   %mul_27 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_2, %convert_element_type), kwargs = {})
	#   %mul_28 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_2, %rsqrt), kwargs = {})
	#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_27, [2], True), kwargs = {dtype: torch.float32})
	#   %div : [num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand, 2048), kwargs = {})
	#   %pow_3 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%convert_element_type, 1.0), kwargs = {})
	#   %mul_31 : [num_users=1] = call_function[target=torch.ops.aten.mul.Scalar](args = (%pow_3, 2.0), kwargs = {})
	#   %mul_32 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div, %mul_31), kwargs = {})
	#   %add_28 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_28, %mul_32), kwargs = {})
	#   %convert_element_type_3 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_28, torch.float16), kwargs = {})
	triton_red_fused__to_copy_add_div_mul_pow_sum_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.reduction(
	    size_hints=[128, 2048],
	    reduction_hint=ReductionHint.INNER,
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp32', 4: '*fp16', 5: 'i32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused__to_copy_add_div_mul_pow_sum_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 1, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
	)
	@triton.jit
	def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
	    rnumel = 2048
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = xindex < xnumel
	    rbase = tl.arange(0, RBLOCK)[None, :]
	    x0 = xindex
	    _tmp8 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
	    for roffset in range(0, rnumel, RBLOCK):
	        rindex = roffset + rbase
	        rmask = rindex < rnumel
	        r1 = rindex
	        tmp0 = tl.load(in_ptr0 + (r1 + (2048*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
	        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
	        tmp4 = tl.load(in_ptr2 + (r1 + (2048*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
	        tmp2 = tmp0 * tmp1
	        tmp3 = tmp2.to(tl.float32)
	        tmp5 = tmp4.to(tl.float32)
	        tmp6 = tmp3 * tmp5
	        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])
	        tmp9 = _tmp8 + tmp7
	        _tmp8 = tl.where(rmask & xmask, tmp9, _tmp8)
	    tmp8 = tl.sum(_tmp8, 1)[:, None]
	    tmp14 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
	    for roffset in range(0, rnumel, RBLOCK):
	        rindex = roffset + rbase
	        rmask = rindex < rnumel
	        r1 = rindex
	        tmp10 = tl.load(in_ptr0 + (r1 + (2048*x0)), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
	        tmp11 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
	        tmp23 = tl.load(in_ptr2 + (r1 + (2048*x0)), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
	        tmp12 = tmp10 * tmp11
	        tmp13 = tmp12.to(tl.float32)
	        tmp15 = tmp13 * tmp14
	        tmp16 = -0.5
	        tmp17 = tmp8 * tmp16
	        tmp18 = tmp14 * tmp14
	        tmp19 = tmp18 * tmp14
	        tmp20 = tmp17 * tmp19
	        tmp21 = 0.00048828125
	        tmp22 = tmp20 * tmp21
	        tmp24 = tmp23.to(tl.float32)
	        tmp25 = 2.0
	        tmp26 = tmp24 * tmp25
	        tmp27 = tmp22 * tmp26
	        tmp28 = tmp15 + tmp27
	        tmp29 = tmp28.to(tl.float32)
	        tl.store(out_ptr1 + (r1 + (2048*x0)), tmp29, rmask & xmask)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    primals_1, primals_2, primals_3, rsqrt, tangents_1 = args
	    args.clear()
	    s0 = primals_1
	    assert_size_stride(primals_2, (1, s0, 2048), (2048*s0, 2048, 1))
	    assert_size_stride(primals_3, (2048, ), (1, ))
	    assert_size_stride(rsqrt, (1, s0, 1), (s0, 1, 1))
	    assert_size_stride(tangents_1, (1, s0, 2048), (2048*s0, 2048, 1))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf1 = empty_strided_cuda((1, s0, 2048), (2048*s0, 2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [hidden_states], Original ATen: [aten.mul, aten._to_copy, aten.sum, aten.div, aten.pow, aten.add]
	        stream0 = get_raw_stream(0)
	        triton_red_fused__to_copy_add_div_mul_pow_sum_0.run(tangents_1, primals_3, primals_2, rsqrt, buf1, s0, 2048, grid=grid(s0), stream=stream0)
	        del primals_2
	        del primals_3
	        del rsqrt
	        del tangents_1
	    return (None, buf1, None, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    primals_1 = 100
	    primals_2 = rand_strided((1, 100, 2048), (204800, 2048, 1), device='cuda:0', dtype=torch.float16)
	    primals_3 = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
	    rsqrt = rand_strided((1, 100, 1), (100, 1, 1), device='cuda:0', dtype=torch.float32)
	    tangents_1 = rand_strided((1, 100, 2048), (204800, 2048, 1), device='cuda:0', dtype=torch.float16)
	    fn = lambda: call([primals_1, primals_2, primals_3, rsqrt, tangents_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:52:05.049000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "75e9767e5d4fbe06ffbfdfc4c2bbed95"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742993525049680.8,
	"args": {
	"key": "f3hbkqnj3awzuwzxb6sotpsj3vylnuhmlmz4f5ydv2ptm47zw3qn",
	"components": [
	"[4plki53queiockxg4eroemrjav5skevi7ca4tonjgzft3exor6s] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3, rsqrt, tangents_1):\n    mul_26 = torch.ops.aten.mul.Tensor(tangents_1, primals_3);  tangents_1 = primals_3 = None\n    convert_element_type_2 = torch.ops.prims.convert_element_type.default(mul_26, torch.float32);  mul_26 = None\n    convert_element_type = torch.ops.prims.convert_element_type.default(primals_2, torch.float32);  primals_2 = None\n    mul_27 = torch.ops.aten.mul.Tensor(convert_element_type_2, convert_element_type)\n    mul_28 = torch.ops.aten.mul.Tensor(convert_element_type_2, rsqrt);  convert_element_type_2 = None\n    sum_1 = torch.ops.aten.sum.dim_IntList(mul_27, [2], True, dtype = torch.float32);  mul_27 = None\n    pow_2 = torch.ops.aten.pow.Tensor_Scalar(rsqrt, 3);  rsqrt = None\n    mul_29 = torch.ops.aten.mul.Scalar(sum_1, -0.5);  sum_1 = None\n    mul_30 = torch.ops.aten.mul.Tensor(mul_29, pow_2);  mul_29 = pow_2 = None\n    expand = torch.ops.aten.expand.default(mul_30, [1, primals_1, 2048]);  mul_30 = primals_1 = None\n    div = torch.ops.aten.div.Scalar(expand, 2048);  expand = None\n    pow_3 = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 1.0);  convert_element_type = None\n    mul_31 = torch.ops.aten.mul.Scalar(pow_3, 2.0);  pow_3 = None\n    mul_32 = torch.ops.aten.mul.Tensor(div, mul_31);  div = mul_31 = None\n    add_28 = torch.ops.aten.add.Tensor(mul_28, mul_32);  mul_28 = mul_32 = None\n    convert_element_type_3 = torch.ops.prims.convert_element_type.default(add_28, torch.float16);  add_28 = None\n    return (None, convert_element_type_3, None)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[xrcn5kean45trgkul3lxpg7fpefuuthsckieyj3ktmatzm2m27j] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4096*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[y2xmti2d5nfvoydvw5fwmyz3r65x7l5lz4kteznxv3qzpai5dku] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[fbavznyoas5zlq2ukff4higuprx6o7ghcq5att5foipfmcaeil7] example_inputs[3]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1, s0, 1]), stride=(s0, 1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_backward]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[rsuyy3mwcmazob2rjj4zloxesnv43jcmbn6pbssjvcym5t6zngb] fx_kwargs[static_input_idxs]: [0, 1, 2, 3]",
	"[rprlxq2vwpy5onx3seju2nad6bzjzrtekxvtmzeyhkcrcv6gu46] fx_kwargs[user_visible_outputs]: {'convert_element_type_3': None}",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[0]: 4",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 497150911,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:52:05.050000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "bb2bf452705b72f04980f12ca4d74d69"}
	{"key": "f3hbkqnj3awzuwzxb6sotpsj3vylnuhmlmz4f5ydv2ptm47zw3qn", "components": ["[4plki53queiockxg4eroemrjav5skevi7ca4tonjgzft3exor6s] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3, rsqrt, tangents_1):\n    mul_26 = torch.ops.aten.mul.Tensor(tangents_1, primals_3);  tangents_1 = primals_3 = None\n    convert_element_type_2 = torch.ops.prims.convert_element_type.default(mul_26, torch.float32);  mul_26 = None\n    convert_element_type = torch.ops.prims.convert_element_type.default(primals_2, torch.float32);  primals_2 = None\n    mul_27 = torch.ops.aten.mul.Tensor(convert_element_type_2, convert_element_type)\n    mul_28 = torch.ops.aten.mul.Tensor(convert_element_type_2, rsqrt);  convert_element_type_2 = None\n    sum_1 = torch.ops.aten.sum.dim_IntList(mul_27, [2], True, dtype = torch.float32);  mul_27 = None\n    pow_2 = torch.ops.aten.pow.Tensor_Scalar(rsqrt, 3);  rsqrt = None\n    mul_29 = torch.ops.aten.mul.Scalar(sum_1, -0.5);  sum_1 = None\n    mul_30 = torch.ops.aten.mul.Tensor(mul_29, pow_2);  mul_29 = pow_2 = None\n    expand = torch.ops.aten.expand.default(mul_30, [1, primals_1, 2048]);  mul_30 = primals_1 = None\n    div = torch.ops.aten.div.Scalar(expand, 2048);  expand = None\n    pow_3 = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 1.0);  convert_element_type = None\n    mul_31 = torch.ops.aten.mul.Scalar(pow_3, 2.0);  pow_3 = None\n    mul_32 = torch.ops.aten.mul.Tensor(div, mul_31);  div = mul_31 = None\n    add_28 = torch.ops.aten.add.Tensor(mul_28, mul_32);  mul_28 = mul_32 = None\n    convert_element_type_3 = torch.ops.prims.convert_element_type.default(add_28, torch.float16);  add_28 = None\n    return (None, convert_element_type_3, None)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[xrcn5kean45trgkul3lxpg7fpefuuthsckieyj3ktmatzm2m27j] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4096*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[y2xmti2d5nfvoydvw5fwmyz3r65x7l5lz4kteznxv3qzpai5dku] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[fbavznyoas5zlq2ukff4higuprx6o7ghcq5att5foipfmcaeil7] example_inputs[3]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1, s0, 1]), stride=(s0, 1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_backward]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[rsuyy3mwcmazob2rjj4zloxesnv43jcmbn6pbssjvcym5t6zngb] fx_kwargs[static_input_idxs]: [0, 1, 2, 3]", "[rprlxq2vwpy5onx3seju2nad6bzjzrtekxvtmzeyhkcrcv6gu46] fx_kwargs[user_visible_outputs]: {'convert_element_type_3': None}", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[0]: 4", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 497150911, "cache_state": "hit"}
V0326 23:52:05.050000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "dd753136ee5ffcb0e87064da1f282b8e"}
	{
	"name": "inductor_compile",
	"ts": 1742993525050954.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.051000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "e4b930986564eec7d7c193e621f7a94c"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993525051298.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.051000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"bwd_compilation_metrics": {"compile_id": "0/0", "inductor_compile_time_s": 0.012513399124145508, "code_gen_time_s": null, "fail_type": null, "fail_reason": null}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.051000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "9ffeead060824b12b8459c83546dcbf7"}
	{
	"name": "compile_fx.<locals>.bw_compiler",
	"ts": 1742993525051901.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.053000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "9a8d60e8d4b0f17b76e34e4949cb6cde"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993525053638.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.054000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "e9359b4f3c74bf38314a873190b6b127"}
	{
	"name": "backend_compile",
	"ts": 1742993525054097.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.054000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "f836d5fa178240b235e969752af499e0"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993525054451.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.059000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "7c08992dd47b1a9453bf2d3825e9c632"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)
	| | +- TYPE_MATCH: ___check_type_id(L['self'], 102513794945376)                
	| | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | +- GuardManager: source=L['self']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | +- DICT_LENGTH: len(L['self']._parameters) == 1                             
	| | | | +- GuardManager: source=L['self']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | +- TENSOR_MATCH: check_tensor(L['self']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[2048], stride=[1])
	| | | | | +- NO_TENSOR_ALIASING: check_no_aliasing(L['hidden_states'], L['self']._parameters['weight'])
	| | | +- GuardManager: source=L['self'].variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)
	| | | | +- EQUALS_MATCH: L['self'].variance_epsilon == 1e-05                         
	| +- GuardManager: source=L['hidden_states'], accessed_by=DictGetItemGuardAccessor(hidden_states)
	| | +- TENSOR_MATCH: check_tensor(L['hidden_states'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=True, size=[1, None, 2048], stride=[None, 2048, 1])
	| | +- NO_HASATTR: hasattr(L['hidden_states'], '_dynamo_dynamic_indices') == False
	| | +- NO_TENSOR_ALIASING
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)
	| | | +- ID_MATCH: ___check_obj_id(G['torch'], 140718364431024)                
	| | | +- GuardManager: source=G['torch'].rsqrt, accessed_by=GetAttrGuardAccessor(rsqrt)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].rsqrt, 140718363263520)          
	| | | +- GuardManager: source=G['torch'].float32, accessed_by=GetAttrGuardAccessor(float32)
	| | | | +- EQUALS_MATCH: G['torch'].float32 == torch.float32                         
	+- LAMBDA_GUARD: L['hidden_states'].stride()[0] == 2048*L['hidden_states'].size()[1]  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['hidden_states'].size()[1] <= 1048575                  # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0326 23:52:05.059000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "0ff9c9e327750951ada3848a82776208"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993525059873.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.060000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "8110f322a9a31de67966720f0054c920"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993525060113.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.060000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "0/0", "frame_key": "1", "co_name": "compiled_rms_layernorm", "co_filename": "/tmp/ipykernel_331427/3685874839.py", "co_firstlineno": 1, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": 14, "shape_env_guard_count": 2, "graph_op_count": 8, "graph_node_count": 12, "graph_input_count": 3, "start_time": 1742993524.0698552, "entire_frame_compile_time_s": 0.9899501800537109, "backend_compile_time_s": 0.9183135032653809, "inductor_compile_time_s": 0.31011319160461426, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.061000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py", 39]}
V0326 23:52:05.062000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/tmp/ipykernel_331427/2489781049.py", 40]}
V0326 23:52:05.062000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 336, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 270, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 496, "name": "forward", "filename": 39}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 102, "name": "inner_transpose_forward", "filename": 40}, {"line": 69, "name": "get_data_transposed", "filename": 40}]}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.062000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "8ece7a4d71a8238384d2e82f4b483f77"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993525062473.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.062000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "cc91b10a4d41e784020c8ab3a2968891"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993525062473.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.065000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 4, "size": 2097152}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.065000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [2097152, 1], "is_leaf": true, "stride": [1, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa34039630>", "describer_id": 4}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.065000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 4, "id": 0, "source": "L['param']._some_data"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.068000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_param_some_data": [2097152, 1], "t": [1, 2097152]}}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "022fe0355be5d1481a1de82dec2135af"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_param_some_data: "u8[2097152, 1][1, 1]cuda:0"):
	        l_param_some_data = L_param_some_data
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        t: "u8[1, 2097152][1, 1]cuda:0" = l_param_some_data.t();  l_param_some_data = None
	        return (t,)
	        
V0326 23:52:05.069000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "1ad70da2acea15f25401dc16c032fa3a"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993525069749.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.070000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "6c82ed60f7dadbee2c7bc90960dbb4a7"}
	{
	"name": "backend_compile",
	"ts": 1742993525069749.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.071000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "a5877f94f66f817be2466c169d962d8a"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993525071771.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.077000 331427 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:215] {"aot_forward_graph": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "67f85556fb084ad0da91ece87d4a1d0c"}
	class <lambda>(torch.nn.Module):
	    def forward(self, arg0_1: "u8[2097152, 1][1, 1]cuda:0"):
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute: "u8[1, 2097152][1, 1]cuda:0" = torch.ops.aten.permute.default(arg0_1, [1, 0]);  arg0_1 = None
	        return (permute,)
	        
V0326 23:52:05.078000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "0269ff8976b9f807373e77ce89d61a9e"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993525078049.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.078000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "ce22ab55279cedf2f2d2c09a936c067a"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993525078720.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.078000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "7c92531ff0d5c86dfb411f7db680dd74"}
	{
	"name": "inductor_compile",
	"ts": 1742993525078720.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.082000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/t5/ct5cdrlvhzqgati3qobbqjmjthnaa573nkk6dwx7azkdhct72btj.py"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "6f1ae6738be728abd508dcc7a1da35cd"}
	# AOT ID: ['1_inference']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    arg0_1, = args
	    args.clear()
	    assert_size_stride(arg0_1, (2097152, 1), (1, 1))
	    return (reinterpret_tensor(arg0_1, (1, 2097152), (1, 1), 0), )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    arg0_1 = rand_strided((2097152, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    fn = lambda: call([arg0_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:52:05.082000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "74e4fab4250311c43c1a5060a2856897"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742993525082672.0,
	"args": {
	"key": "frdyc5x44cxbjq26p2wj6co5c3qi53vxxjy7dm4vmjuzn6pgktgo",
	"components": [
	"[omke3y723kztjz7vngax6cqwepxyk5c3ln6ujnyovguytfqrfsu] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1):\n    permute = torch.ops.aten.permute.default(arg0_1, [1, 0]);  arg0_1 = None\n    return (permute,)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[ezlhk63hcc4wxnhvfwvow767xoda5sijwwnfqpayfzwwvfslyks] example_inputs[0]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([2097152, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []",
	"[5ckznxnwokx4wcssswt7y7bj7a6njtitxnutjq3p4vcfi2uszig] fx_kwargs[user_visible_outputs]: {'permute': None}",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 6391657,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:52:05.083000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "5adacf143046cfe094a0c14ce4377cd8"}
	{"key": "frdyc5x44cxbjq26p2wj6co5c3qi53vxxjy7dm4vmjuzn6pgktgo", "components": ["[omke3y723kztjz7vngax6cqwepxyk5c3ln6ujnyovguytfqrfsu] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1):\n    permute = torch.ops.aten.permute.default(arg0_1, [1, 0]);  arg0_1 = None\n    return (permute,)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[ezlhk63hcc4wxnhvfwvow767xoda5sijwwnfqpayfzwwvfslyks] example_inputs[0]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([2097152, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []", "[5ckznxnwokx4wcssswt7y7bj7a6njtitxnutjq3p4vcfi2uszig] fx_kwargs[user_visible_outputs]: {'permute': None}", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 6391657, "cache_state": "hit"}
V0326 23:52:05.083000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "58f6b056eb4ac4737a25e5f79d33dbe7"}
	{
	"name": "inductor_compile",
	"ts": 1742993525083485.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.083000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "8bc58780e1abf2b2f2d67720945a8dfc"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993525083721.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.084000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "d07ff46b03c87f80eddbab277d780f69"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993525084056.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.085000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "71c91794dc88bbb52ad87cb14e799343"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993525085338.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.085000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "91d8e295c681c4c8874375a9c1175053"}
	{
	"name": "backend_compile",
	"ts": 1742993525085680.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.086000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "72f7b29af6f8263c49d2e1fc22a68e17"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993525085992.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.087000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "19607434afe0b1d9f96fd62e05e58351"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['param'], accessed_by=DictGetItemGuardAccessor(param)
	| | +- TYPE_MATCH: ___check_type_id(L['param'], 102513768940032)               
	| | +- GuardManager: source=L['param']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | +- TENSOR_MATCH: check_tensor(L['param']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[2097152, 1], stride=[1, 1])
	| | | +- NO_HASATTR: hasattr(L['param']._some_data, '_dynamo_dynamic_indices') == False
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['Params4bit'], accessed_by=DictGetItemGuardAccessor(Params4bit)
	| | | +- ID_MATCH: ___check_obj_id(G['Params4bit'], 102513768940032)           
	| | +- GuardManager: source=G['__builtins_dict___2'], accessed_by=DictGetItemGuardAccessor(__builtins_dict___2)
	| | | +- GuardManager: source=G['__builtins_dict___2']['isinstance'], accessed_by=DictGetItemGuardAccessor(isinstance)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___2']['isinstance'], 140718464977472)
	
V0326 23:52:05.088000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "a00acbcad18d9a56451fcee16d7ceed7"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993525088102.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.088000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "b9652146d806af609718ab9734c611f1"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993525088342.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 3,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.088000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "1/0", "frame_key": "2", "co_name": "get_data_transposed", "co_filename": "/tmp/ipykernel_331427/2489781049.py", "co_firstlineno": 69, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": 9, "shape_env_guard_count": 0, "graph_op_count": 1, "graph_node_count": 3, "graph_input_count": 1, "start_time": 1742993525.0624685, "entire_frame_compile_time_s": 0.02559351921081543, "backend_compile_time_s": 0.015895366668701172, "inductor_compile_time_s": 0.004723310470581055, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.089000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/autograd/function.py", 41]}
V0326 23:52:05.089000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/tmp/ipykernel_331427/1346670402.py", 42]}
V0326 23:52:05.090000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/submit/Problem_C/kernels.py", 43]}
V0326 23:52:05.090000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 336, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 270, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 496, "name": "forward", "filename": 39}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 110, "name": "inner_transpose_forward", "filename": 40}, {"line": 575, "name": "apply", "filename": 41}, {"line": 34, "name": "forward", "filename": 40}, {"line": 18, "name": "augmented_dequantize_4bit", "filename": 42}, {"line": 330, "name": "fused_dequantize", "filename": 43}]}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.090000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "7013dc4fd3462dc0887712f82925e26b"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993525090532.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.090000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "32386521496fe45f40edb22d69ac38ec"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993525090532.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.093000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 6, "size": 2097152}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.093000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [2097152, 1], "is_leaf": true, "stride": [1, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa34039630>", "describer_id": 6}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.093000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [1, 2097152], "is_leaf": true, "is_view": true, "stride": [1, 1], "storage": 0, "base": 1, "creation_meta": "CreationMeta.DEFAULT", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa5717da90>", "describer_id": 6}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.094000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 6, "id": 0, "source": "L['A']"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.096000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 6, "size": 65536}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.096000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [65536], "is_leaf": true, "stride": [1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e82ecb0>", "describer_id": 6}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.096000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 6, "id": 3, "source": "L['quant_state'].absmax"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.097000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 6, "size": 1024}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.097000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e82fd90>", "describer_id": 6}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.097000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 6, "id": 4, "source": "L['quant_state'].state2.code"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.098000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 6, "size": 1024}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.098000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa30977b60>", "describer_id": 6}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.098000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 6, "id": 5, "source": "L['quant_state'].state2.absmax"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.099000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 6, "size": 2}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.099000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa30974eb0>", "describer_id": 6}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.099000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 6, "id": 6, "source": "L['quant_state'].offset"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.100000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 6, "size": 64}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.100000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e82ff70>", "describer_id": 6}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.101000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 6, "id": 7, "source": "L['quant_state'].code"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.109000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_a_": [1, 2097152], "l_quant_state_absmax": [65536], "l_quant_state_state2_code": [256], "l_quant_state_state2_absmax": [256], "l_quant_state_offset": [], "l_quant_state_code": [16], "output_ptr": [2048, 2048]}}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "d0a69d02c4165ad106b39184953050e9"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_A_: "u8[1, 2097152][1, 1]cuda:0", L_quant_state_absmax: "u8[65536][1]cuda:0", L_quant_state_state2_code: "f32[256][1]cuda:0", L_quant_state_state2_absmax: "f32[256][1]cuda:0", L_quant_state_offset: "f16[][]cuda:0", L_quant_state_code: "f32[16][1]cuda:0"):
	        l_a_ = L_A_
	        l_quant_state_absmax = L_quant_state_absmax
	        l_quant_state_state2_code = L_quant_state_state2_code
	        l_quant_state_state2_absmax = L_quant_state_state2_absmax
	        l_quant_state_offset = L_quant_state_offset
	        l_quant_state_code = L_quant_state_code
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	        output_ptr: "bf16[2048, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_quant_state_absmax, l_quant_state_state2_code, l_quant_state_state2_absmax, l_quant_state_offset, l_a_, l_quant_state_code, 4194304, 256, 32, 16384, 8192, 2048, 2048, torch.bfloat16);  l_quant_state_absmax = l_quant_state_state2_code = l_quant_state_state2_absmax = l_quant_state_offset = l_a_ = l_quant_state_code = None
	        return (output_ptr,)
	        
V0326 23:52:05.110000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "dfd0cc1e4e1079a7399fb194e8ffb624"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993525110095.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.110000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "c29c63679af5920593a7e1d95e08fa28"}
	{
	"name": "backend_compile",
	"ts": 1742993525110095.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.112000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "cea301e9cde14e903af42b61aa8d630e"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993525112284.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.121000 331427 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:215] {"aot_forward_graph": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "ad54c546273a9367de3e60c0105d5280"}
	class <lambda>(torch.nn.Module):
	    def forward(self, arg0_1: "u8[1, 2097152][1, 1]cuda:0", arg1_1: "u8[65536][1]cuda:0", arg2_1: "f32[256][1]cuda:0", arg3_1: "f32[256][1]cuda:0", arg4_1: "f16[][]cuda:0", arg5_1: "f32[16][1]cuda:0"):
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	        fused_dequantize_op: "bf16[2048, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(arg1_1, arg2_1, arg3_1, arg4_1, arg0_1, arg5_1, 4194304, 256, 32, 16384, 8192, 2048, 2048, torch.bfloat16);  arg1_1 = arg2_1 = arg3_1 = arg4_1 = arg0_1 = arg5_1 = None
	        return (fused_dequantize_op,)
	        
V0326 23:52:05.121000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "c38e118b8db86089867e9582a30879c2"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993525121854.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.122000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "8dc09e854fe686b0226e21c1e1379a2b"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993525122586.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.122000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "545a2ad179365cbdbf5b53aff8f2c9c2"}
	{
	"name": "inductor_compile",
	"ts": 1742993525122586.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.131000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/tu/ctunny5zrn6m4zbxe55td4fbnmrbnmej3onm4j537eilj3omfiad.py"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "b638b9a588e9cbc9c705eefdb1592930"}
	# AOT ID: ['2_inference']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/xa/cxaqzqrucz62vf6o3o6sl2ihhxptbp2wjf7gaoczyt27xbq6aopg.py
	# Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	# Source node to ATen node mapping:
	#   output_ptr => fused_dequantize_op
	# Graph fragment:
	#   %fused_dequantize_op : [num_users=1] = call_function[target=torch.ops.mylib.fused_dequantize_op.default](args = (%arg1_1, %arg2_1, %arg3_1, %arg4_1, %arg0_1, %arg5_1, 4194304, 256, 32, 16384, 8192, 2048, 2048, torch.bfloat16), kwargs = {})
	triton_poi_fused_fused_dequantize_op_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {2: 1}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=(2,))]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_fused_dequantize_op_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 1
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    tmp0 = tl.load(in_ptr0 + (0)).to(tl.float32)
	    tmp1 = tl.broadcast_to(tmp0, [XBLOCK])
	    tl.store(out_ptr0 + (tl.full([XBLOCK], 0, tl.int32)), tmp1, None)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1 = args
	    args.clear()
	    assert_size_stride(arg0_1, (1, 2097152), (1, 1))
	    assert_size_stride(arg1_1, (65536, ), (1, ))
	    assert_size_stride(arg2_1, (256, ), (1, ))
	    assert_size_stride(arg3_1, (256, ), (1, ))
	    assert_size_stride(arg4_1, (), ())
	    assert_size_stride(arg5_1, (16, ), (1, ))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((), (), torch.float16)
	        # Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_fused_dequantize_op_0.run(arg4_1, buf0, 1, grid=grid(1), stream=stream0)
	        del arg4_1
	        # Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	        buf1 = torch.ops.mylib.fused_dequantize_op.default(arg1_1, arg2_1, arg3_1, buf0, arg0_1, arg5_1, 4194304, 256, 32, 16384, 8192, 2048, 2048, torch.bfloat16)
	        del arg0_1
	        del arg1_1
	        del arg2_1
	        del arg3_1
	        del arg5_1
	        del buf0
	        buf2 = buf1
	        del buf1
	    return (buf2, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    arg0_1 = rand_strided((1, 2097152), (1, 1), device='cuda:0', dtype=torch.uint8)
	    arg1_1 = rand_strided((65536, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    arg2_1 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg3_1 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg4_1 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    arg5_1 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:52:05.132000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "ac00127c7e8b4bebef2b8d82754aab5d"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742993525132241.2,
	"args": {
	"key": "fdh2uwta5azcf55ly2rwah3ufrucn3zo5edy7spszb463gmznw6v",
	"components": [
	"[2bp3t6p5cxblmeq7qhno6dbosphmpkxzycvz54fqxuqeaocqtnp] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1):\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(arg1_1, arg2_1, arg3_1, arg4_1, arg0_1, arg5_1, 4194304, 256, 32, 16384, 8192, 2048, 2048, torch.bfloat16);  arg1_1 = arg2_1 = arg3_1 = arg4_1 = arg0_1 = arg5_1 = None\n    return (fused_dequantize_op,)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[4gvebibmcz6gre4gya45qksbmfls7626p25ek6tw3nszlewizy7] example_inputs[0]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([1, 2097152]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[53cooickqlnvm7pjzvcat3ztcgrqimbrib43vb3txz7otk7tjkk] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([65536]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[2]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[3]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[j23zbqrguggah6nscnzj4y55d5chhoardpgvlji4ilrfwrr6asz] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[2ylzn3y4vcv2t7rbq447jvkl6by5qf32ycwks3h2nbcmm6wvmnk] example_inputs[5]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []",
	"[jan4rfottd4caig5fc5h3ieb5ephfyhn7rn27jmu2ehbwgssm5j] fx_kwargs[user_visible_outputs]: {'fused_dequantize_op': None}",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[1]: 1",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inputs_to_check[2]: 2",
	"[kcuxe2zwm3mzv2uk6adm6iskoy35bqfv725twacrdewod2dbl5d] inputs_to_check[3]: 3",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[4]: 4",
	"[qs5hilycp4ew4ivtc7m5jaxp7q4pm5slioxw3fi3ur6ei65ybz4] inputs_to_check[5]: 5",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 75306289,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:52:05.132000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "b22c51cf728b0ed337ff337d8b7d1154"}
	{"key": "fdh2uwta5azcf55ly2rwah3ufrucn3zo5edy7spszb463gmznw6v", "components": ["[2bp3t6p5cxblmeq7qhno6dbosphmpkxzycvz54fqxuqeaocqtnp] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1):\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(arg1_1, arg2_1, arg3_1, arg4_1, arg0_1, arg5_1, 4194304, 256, 32, 16384, 8192, 2048, 2048, torch.bfloat16);  arg1_1 = arg2_1 = arg3_1 = arg4_1 = arg0_1 = arg5_1 = None\n    return (fused_dequantize_op,)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[4gvebibmcz6gre4gya45qksbmfls7626p25ek6tw3nszlewizy7] example_inputs[0]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([1, 2097152]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[53cooickqlnvm7pjzvcat3ztcgrqimbrib43vb3txz7otk7tjkk] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([65536]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[2]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[3]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[j23zbqrguggah6nscnzj4y55d5chhoardpgvlji4ilrfwrr6asz] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[2ylzn3y4vcv2t7rbq447jvkl6by5qf32ycwks3h2nbcmm6wvmnk] example_inputs[5]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []", "[jan4rfottd4caig5fc5h3ieb5ephfyhn7rn27jmu2ehbwgssm5j] fx_kwargs[user_visible_outputs]: {'fused_dequantize_op': None}", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[1]: 1", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inputs_to_check[2]: 2", "[kcuxe2zwm3mzv2uk6adm6iskoy35bqfv725twacrdewod2dbl5d] inputs_to_check[3]: 3", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[4]: 4", "[qs5hilycp4ew4ivtc7m5jaxp7q4pm5slioxw3fi3ur6ei65ybz4] inputs_to_check[5]: 5", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 75306289, "cache_state": "hit"}
V0326 23:52:05.133000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "cf59d80ec52a8489097d0f17bbeda99b"}
	{
	"name": "inductor_compile",
	"ts": 1742993525133555.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.133000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "36fd96e5c034b6793c24ccd1ee9cc365"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993525133884.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.134000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "8dd5e74d4b64a6b38f2a30cc8b3ab634"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993525134394.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.135000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "78b64e3055b61777eda4fcaaae1f6e23"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993525135883.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.136000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "99948c39ddba5dd0456be7c6e66c7d7a"}
	{
	"name": "backend_compile",
	"ts": 1742993525136308.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.136000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "3f3518bc3e7461670d3d2f2e38696fcf"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993525136592.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.140000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "58359ea74c40d94c2d9fdf496cc97358"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['A'], accessed_by=DictGetItemGuardAccessor(A)
	| | +- TENSOR_MATCH: check_tensor(L['A'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[1, 2097152], stride=[1, 1])
	| | +- NO_HASATTR: hasattr(L['A'], '_dynamo_dynamic_indices') == False         
	| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['A'], L['quant_state'].code, L['quant_state'].absmax, L['quant_state'].offset, L['quant_state'].state2.code, L['quant_state'].state2.absmax)
	| +- GuardManager: source=L['quant_state'], accessed_by=DictGetItemGuardAccessor(quant_state)
	| | +- TYPE_MATCH: ___check_type_id(L['quant_state'], 102513767659184)         
	| | +- GuardManager: source=L['quant_state'].code, accessed_by=GetAttrGuardAccessor(code)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].code, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | +- EQUALS_MATCH: L['quant_state'].dtype == torch.bfloat16                    
	| | +- GuardManager: source=L['quant_state'].shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].shape, 140717689517824)   
	| | | +- LENGTH_CHECK: len(L['quant_state'].shape) == 2                            
	| | | +- GuardManager: source=L['quant_state'].shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | +- EQUALS_MATCH: L['quant_state'].shape[0] == 2048                           
	| | | +- GuardManager: source=L['quant_state'].shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | +- EQUALS_MATCH: L['quant_state'].shape[1] == 2048                           
	| | +- GuardManager: source=L['quant_state'].absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[65536], stride=[1])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].absmax, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].offset, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].state2, 102513767659184)  
	| | | +- GuardManager: source=L['quant_state'].state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | +- TENSOR_MATCH: check_tensor(L['quant_state'].state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | +- NO_HASATTR: hasattr(L['quant_state'].state2.code, '_dynamo_dynamic_indices') == False
	| | | | +- NO_TENSOR_ALIASING
	| | | +- GuardManager: source=L['quant_state'].state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | +- TENSOR_MATCH: check_tensor(L['quant_state'].state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | +- NO_HASATTR: hasattr(L['quant_state'].state2.absmax, '_dynamo_dynamic_indices') == False
	| | | | +- NO_TENSOR_ALIASING
	| | | +- GuardManager: source=L['quant_state'].state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | +- EQUALS_MATCH: L['quant_state'].state2.blocksize == 256                    
	| | +- GuardManager: source=L['quant_state'].blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | +- EQUALS_MATCH: L['quant_state'].blocksize == 64                            
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)
	| | | +- ID_MATCH: ___check_obj_id(G['torch'], 140718364431024)                
	| | | +- GuardManager: source=G['torch'].numel, accessed_by=GetAttrGuardAccessor(numel)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].numel, 140718363174816)          
	| | +- GuardManager: source=G['DEBUG_FLAG'], accessed_by=DictGetItemGuardAccessor(DEBUG_FLAG)
	| | | +- ID_MATCH: ___check_obj_id(G['DEBUG_FLAG'], 140718482561760)           
	| | +- GuardManager: source=G['fused_dequantize_op'], accessed_by=DictGetItemGuardAccessor(fused_dequantize_op)
	| | | +- TYPE_MATCH: ___check_type_id(G['fused_dequantize_op'], 102513752736032) 
	| | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize_op'], 140712749840912)  
	| | | +- GuardManager: source=G['fused_dequantize_op']._opoverload, accessed_by=GetAttrGuardAccessor(_opoverload)
	| | | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize_op']._opoverload, 140712749959024)
	
V0326 23:52:05.140000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "3a37f720e9978eaf710cc55ceb5636dd"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993525140938.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.141000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "b16ef96be7a07b173de20bc7515909c8"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993525141184.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 4,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.141000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "2/0", "frame_key": "3", "co_name": "fused_dequantize", "co_filename": "/home/tom/unsloth-challenges/submit/Problem_C/kernels.py", "co_firstlineno": 330, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": 25, "shape_env_guard_count": 0, "graph_op_count": 1, "graph_node_count": 8, "graph_input_count": 6, "start_time": 1742993525.0905285, "entire_frame_compile_time_s": 0.05037212371826172, "backend_compile_time_s": 0.026176929473876953, "inductor_compile_time_s": 0.010905742645263672, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": ["mylib::fused_dequantize_op"], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.212000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2817] {"artifact": {"name": "recompile_reasons", "encoding": "json"}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "30ea098d0804bda888895ce1fd541df6"}
	[
	"1/0: tensor 'L['param']._some_data' size mismatch at index 0. expected 2097152, actual 524288"
	]
V0326 23:52:05.213000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 336, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 271, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 496, "name": "forward", "filename": 39}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 102, "name": "inner_transpose_forward", "filename": 40}, {"line": 69, "name": "get_data_transposed", "filename": 40}]}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.213000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "8534265ec55c7b1f395ae24770e0267d"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993525213690.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.213000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "e5ca57a6f465235a728c130bedfc9613"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993525213690.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.216000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 8, "size": 524288}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.216000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [524288, 1], "is_leaf": true, "stride": [1, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa34039450>", "describer_id": 8}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.216000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 8, "id": 0, "source": "L['param']._some_data"}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.220000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_param_some_data": ["s0", 1], "t": [1, "s0"]}}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "599e7d8ab93d1aa7414eedfa4f8364f7"}
	class GraphModule(torch.nn.Module):
	    def forward(self, s0: "Sym(s0)", L_param_some_data: "u8[s0, 1][1, 1]cuda:0"):
	        l_param_some_data = L_param_some_data
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        t: "u8[1, s0][1, 1]cuda:0" = l_param_some_data.t();  l_param_some_data = None
	        return (t,)
	        
V0326 23:52:05.221000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "2586f727f720fdc0590a805f04e77233"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993525221162.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.221000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "4688fe371d40736fc2d59c7c772d873b"}
	{
	"name": "backend_compile",
	"ts": 1742993525221162.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.223000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "ca63c01b99f63be35da3e8150208523d"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993525223316.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.229000 331427 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:215] {"aot_forward_graph": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "77105fcc56694f6a09036e7b4003e5ab"}
	class <lambda>(torch.nn.Module):
	    def forward(self, arg0_1: "Sym(s0)", arg1_1: "u8[s0, 1][1, 1]cuda:0"):
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute: "u8[1, s0][1, 1]cuda:0" = torch.ops.aten.permute.default(arg1_1, [1, 0]);  arg1_1 = None
	        return (permute,)
	        
V0326 23:52:05.230000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "2beebc1ad8c49b0c2c3bb9840191febc"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993525230158.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.230000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "e5173b2072ba4c4c1f8a9227a0198e21"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993525230788.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.231000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "9b0fa9d82895758959cd39ad70713f9f"}
	{
	"name": "inductor_compile",
	"ts": 1742993525230788.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.234000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/ro/cro4d6xcajw34kn3amfm77p5rbikp6iqqc45sigdaropf6dafvxw.py"}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "0a39d0091a3c2b437e370a8559861577"}
	# AOT ID: ['3_inference']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    arg0_1, arg1_1 = args
	    args.clear()
	    s0 = arg0_1
	    assert_size_stride(arg1_1, (s0, 1), (1, 1))
	    return (reinterpret_tensor(arg1_1, (1, s0), (1, 1), 0), )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    arg0_1 = 524288
	    arg1_1 = rand_strided((524288, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    fn = lambda: call([arg0_1, arg1_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:52:05.234000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "bfccad7c97fd11be9860491a88bcf074"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742993525234496.2,
	"args": {
	"key": "feywvgdkn732db3gwlpsdt4okq5o257p7ly7o33m6etbej3zcs6o",
	"components": [
	"[t5qozjxe7r42o4bunbvqfqasfgh5rki4fvvhuseni4bop27xuky] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1):\n    permute = torch.ops.aten.permute.default(arg1_1, [1, 0]);  arg1_1 = None\n    return (permute,)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[ra3m7brimdni4uv5mdit5d3d5ljftmi2buup3o3swwfvmvhdelk] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([s0, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []",
	"[5ckznxnwokx4wcssswt7y7bj7a6njtitxnutjq3p4vcfi2uszig] fx_kwargs[user_visible_outputs]: {'permute': None}",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 7272635,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:52:05.234000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "cc80d91c1e03448928ba963e755f925d"}
	{"key": "feywvgdkn732db3gwlpsdt4okq5o257p7ly7o33m6etbej3zcs6o", "components": ["[t5qozjxe7r42o4bunbvqfqasfgh5rki4fvvhuseni4bop27xuky] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1):\n    permute = torch.ops.aten.permute.default(arg1_1, [1, 0]);  arg1_1 = None\n    return (permute,)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[ra3m7brimdni4uv5mdit5d3d5ljftmi2buup3o3swwfvmvhdelk] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([s0, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []", "[5ckznxnwokx4wcssswt7y7bj7a6njtitxnutjq3p4vcfi2uszig] fx_kwargs[user_visible_outputs]: {'permute': None}", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 7272635, "cache_state": "hit"}
V0326 23:52:05.235000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "5e97b530161eeef4d32aa8384aa88835"}
	{
	"name": "inductor_compile",
	"ts": 1742993525235326.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.235000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "672bb2c57c9e9240fa99b792852103ff"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993525235569.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.235000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "5a722bf951acf111ebe7558f78565012"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993525235931.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.237000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "840f630126fa63b574464e39edfcc84e"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993525237258.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.237000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "bec8a6ec3a87a1b09c2343ac2947e0a4"}
	{
	"name": "backend_compile",
	"ts": 1742993525237594.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.237000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "b25f9372f43c54024256096338c138d2"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993525237896.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.240000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "558e383a94f9d746c2b79efb20df83e2"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['param'], accessed_by=DictGetItemGuardAccessor(param)
	| | +- TYPE_MATCH: ___check_type_id(L['param'], 102513768940032)               
	| | +- GuardManager: source=L['param']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | +- TENSOR_MATCH: check_tensor(L['param']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[None, 1], stride=[1, 1])
	| | | +- NO_HASATTR: hasattr(L['param']._some_data, '_dynamo_dynamic_indices') == False
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['Params4bit'], accessed_by=DictGetItemGuardAccessor(Params4bit)
	| | | +- ID_MATCH: ___check_obj_id(G['Params4bit'], 102513768940032)           
	| | +- GuardManager: source=G['__builtins_dict___6'], accessed_by=DictGetItemGuardAccessor(__builtins_dict___6)
	| | | +- GuardManager: source=G['__builtins_dict___6']['isinstance'], accessed_by=DictGetItemGuardAccessor(isinstance)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___6']['isinstance'], 140718464977472)
	+- LAMBDA_GUARD: 2 <= L['param']._some_data.size()[0]                          # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0326 23:52:05.240000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "9664d8589172cedf47de47d62e9f1671"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993525240541.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.240000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0, "has_payload": "8eb01b4ce812da6d9d901d9183e1e9a3"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993525240787.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 5,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.241000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "1/1", "frame_key": "4", "co_name": "get_data_transposed", "co_filename": "/tmp/ipykernel_331427/2489781049.py", "co_firstlineno": 69, "cache_size": 1, "accumulated_cache_size": 1, "guard_count": 9, "shape_env_guard_count": 0, "graph_op_count": 1, "graph_node_count": 4, "graph_input_count": 2, "start_time": 1742993525.213683, "entire_frame_compile_time_s": 0.026799917221069336, "backend_compile_time_s": 0.0164031982421875, "inductor_compile_time_s": 0.0044977664947509766, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 1, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.242000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2817] {"artifact": {"name": "recompile_reasons", "encoding": "json"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "e18c77bc8630f1456518e51d5e3eb07c"}
	[
	"2/0: tensor 'L['A']' size mismatch at index 1. expected 2097152, actual 524288"
	]
V0326 23:52:05.243000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 336, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 271, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 496, "name": "forward", "filename": 39}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 110, "name": "inner_transpose_forward", "filename": 40}, {"line": 575, "name": "apply", "filename": 41}, {"line": 34, "name": "forward", "filename": 40}, {"line": 18, "name": "augmented_dequantize_4bit", "filename": 42}, {"line": 330, "name": "fused_dequantize", "filename": 43}]}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.243000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "b09b8db4315eb9428049df0ccb101d2c"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993525243308.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.243000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "d32d2ea9ada233105dde35d368344df7"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993525243308.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.245000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 10, "size": 524288}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.246000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [524288, 1], "is_leaf": true, "stride": [1, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa34039450>", "describer_id": 10}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.246000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [1, 524288], "is_leaf": true, "is_view": true, "stride": [1, 1], "storage": 0, "base": 1, "creation_meta": "CreationMeta.DEFAULT", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ff9c2a83750>", "describer_id": 10}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.246000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 0, "source": "L['A']"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.248000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 10, "size": 16384}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.249000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [16384], "is_leaf": true, "stride": [1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3096e8a0>", "describer_id": 10}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.249000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 3, "source": "L['quant_state'].absmax"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.251000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 10, "size": 1024}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.251000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3096d4f0>", "describer_id": 10}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.251000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 4, "source": "L['quant_state'].state2.code"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.252000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 10, "size": 256}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.252000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [64], "is_leaf": true, "stride": [1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa30940140>", "describer_id": 10}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.252000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 5, "source": "L['quant_state'].state2.absmax"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.255000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 10, "size": 2}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.255000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3096fc00>", "describer_id": 10}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.255000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 6, "source": "L['quant_state'].offset"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.256000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 10, "size": 64}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.256000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3096fd40>", "describer_id": 10}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.256000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 7, "source": "L['quant_state'].code"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.263000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_a_": [1, "s1"], "l_quant_state_absmax": ["s2"], "l_quant_state_state2_code": [256], "l_quant_state_state2_absmax": ["s3"], "l_quant_state_offset": [], "l_quant_state_code": [16], "output_ptr": ["s4", 2048]}}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "7cb44ba18984348ae7f41158206ad8aa"}
	class GraphModule(torch.nn.Module):
	    def forward(self, s1: "Sym(s1)", L_A_: "u8[1, s1][1, 1]cuda:0", s2: "Sym(s2)", L_quant_state_absmax: "u8[s2][1]cuda:0", L_quant_state_state2_code: "f32[256][1]cuda:0", s3: "Sym(s3)", L_quant_state_state2_absmax: "f32[s3][1]cuda:0", L_quant_state_offset: "f16[][]cuda:0", L_quant_state_code: "f32[16][1]cuda:0", L_quant_state_shape_0_: "Sym(s4)"):
	        l_a_ = L_A_
	        l_quant_state_absmax = L_quant_state_absmax
	        l_quant_state_state2_code = L_quant_state_state2_code
	        l_quant_state_state2_absmax = L_quant_state_state2_absmax
	        l_quant_state_offset = L_quant_state_offset
	        l_quant_state_code = L_quant_state_code
	        l_quant_state_shape_0_ = L_quant_state_shape_0_
	        
	         # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:332 in fused_dequantize, code: n_elements = torch.numel(A) * 2
	        mul: "Sym(2*s1)" = s1 * 2;  s1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	        output_ptr: "bf16[s4, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_quant_state_absmax, l_quant_state_state2_code, l_quant_state_state2_absmax, l_quant_state_offset, l_a_, l_quant_state_code, mul, 256, 32, 16384, 8192, l_quant_state_shape_0_, 2048, torch.bfloat16);  l_quant_state_absmax = l_quant_state_state2_code = l_quant_state_state2_absmax = l_quant_state_offset = l_a_ = l_quant_state_code = mul = l_quant_state_shape_0_ = None
	        return (output_ptr,)
	        
V0326 23:52:05.264000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "4f8d34c31a4ecaf6b53b0af055771a44"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993525264526.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.264000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "17bf854b3da22385b6181389fe481191"}
	{
	"name": "backend_compile",
	"ts": 1742993525264526.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.267000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "2493d6163b6cbbb1d24d1090ee0c9d2f"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993525267359.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.282000 331427 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:215] {"aot_forward_graph": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "3d5c1887ea22b2eca1234c87f471283a"}
	class <lambda>(torch.nn.Module):
	    def forward(self, arg0_1: "Sym(s1)", arg1_1: "u8[1, s1][1, 1]cuda:0", arg2_1: "Sym(s2)", arg3_1: "u8[s2][1]cuda:0", arg4_1: "f32[256][1]cuda:0", arg5_1: "Sym(s3)", arg6_1: "f32[s3][1]cuda:0", arg7_1: "f16[][]cuda:0", arg8_1: "f32[16][1]cuda:0", arg9_1: "Sym(s4)"):
	         # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:332 in fused_dequantize, code: n_elements = torch.numel(A) * 2
	        mul: "Sym(2*s1)" = arg0_1 * 2;  arg0_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	        fused_dequantize_op: "bf16[s4, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, arg7_1, arg1_1, arg8_1, mul, 256, 32, 16384, 8192, arg9_1, 2048, torch.bfloat16);  arg3_1 = arg4_1 = arg6_1 = arg7_1 = arg1_1 = arg8_1 = mul = arg9_1 = None
	        return (fused_dequantize_op,)
	        
V0326 23:52:05.282000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "1b2116f06a5be9ab9578f0d165997f60"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993525282673.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.283000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "895d0b1009db31a517962c32469a4848"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993525283486.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.283000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "92f03b7b33fd459e538c6b9866760023"}
	{
	"name": "inductor_compile",
	"ts": 1742993525283486.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.293000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/we/cwejxmelilirrhy2igx7nsmi6g5rwqrfsgnr4fcqgfkbrwlsjorv.py"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "fac5e3747c2228c1dbcfc8fd69f50258"}
	# AOT ID: ['4_inference']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/xa/cxaqzqrucz62vf6o3o6sl2ihhxptbp2wjf7gaoczyt27xbq6aopg.py
	# Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	# Source node to ATen node mapping:
	#   output_ptr => fused_dequantize_op
	# Graph fragment:
	#   %fused_dequantize_op : [num_users=1] = call_function[target=torch.ops.mylib.fused_dequantize_op.default](args = (%arg3_1, %arg4_1, %arg6_1, %arg7_1, %arg1_1, %arg8_1, %mul, 256, 32, 16384, 8192, %arg9_1, 2048, torch.bfloat16), kwargs = {})
	triton_poi_fused_fused_dequantize_op_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {2: 1}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=(2,))]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_fused_dequantize_op_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 1
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    tmp0 = tl.load(in_ptr0 + (0)).to(tl.float32)
	    tmp1 = tl.broadcast_to(tmp0, [XBLOCK])
	    tl.store(out_ptr0 + (tl.full([XBLOCK], 0, tl.int32)), tmp1, None)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1 = args
	    args.clear()
	    s1 = arg0_1
	    s2 = arg2_1
	    s3 = arg5_1
	    s4 = arg9_1
	    assert_size_stride(arg1_1, (1, s1), (1, 1))
	    assert_size_stride(arg3_1, (s2, ), (1, ))
	    assert_size_stride(arg4_1, (256, ), (1, ))
	    assert_size_stride(arg6_1, (s3, ), (1, ))
	    assert_size_stride(arg7_1, (), ())
	    assert_size_stride(arg8_1, (16, ), (1, ))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((), (), torch.float16)
	        # Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_fused_dequantize_op_0.run(arg7_1, buf0, 1, grid=grid(1), stream=stream0)
	        del arg7_1
	        # Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	        buf1 = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, buf0, arg1_1, arg8_1, 2*s1, 256, 32, 16384, 8192, s4, 2048, torch.bfloat16)
	        del arg1_1
	        del arg3_1
	        del arg4_1
	        del arg6_1
	        del arg8_1
	        del buf0
	        buf2 = buf1
	        del buf1
	    return (buf2, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    arg0_1 = 524288
	    arg1_1 = rand_strided((1, 524288), (1, 1), device='cuda:0', dtype=torch.uint8)
	    arg2_1 = 16384
	    arg3_1 = rand_strided((16384, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    arg4_1 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg5_1 = 64
	    arg6_1 = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg7_1 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    arg8_1 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg9_1 = 512
	    fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:52:05.294000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "f7ca955d383869d7ef7e503423a7b347"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742993525294002.5,
	"args": {
	"key": "fodgjswdodzqgx3iq4x2v2ulrxwzzqobix2qlzyjko7fokxzlgfp",
	"components": [
	"[grf5bl6rpigmyiaz6c6m4gfmm52jce23fz2stipebcebees47ma] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1):\n    mul = arg0_1 * 2;  arg0_1 = None\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, arg7_1, arg1_1, arg8_1, mul, 256, 32, 16384, 8192, arg9_1, 2048, torch.bfloat16);  arg3_1 = arg4_1 = arg6_1 = arg7_1 = arg1_1 = arg8_1 = mul = arg9_1 = None\n    return (fused_dequantize_op,)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[tpvnjs264ct5lugh57xhauij37yhdwzbkxdyhhhhj5de2tskcds] example_inputs[0]: ('s1',)",
	"[7vwriczjssi3zjl7yvm6w4lda6j63tvcm76dxaglqafuowfzcmv] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([1, s1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[2]: ('s2',)",
	"[sqttp5i5wnsdcripsqrsj7sbdbvyr6p2v547c3del3vagfumq23] example_inputs[3]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([s2]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[4]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[5]: ('s3',)",
	"[osj4eueidclo5irb65z7qycfmcyzhzy3mt2msh2p7b3yhteohpt] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([s3]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[j23zbqrguggah6nscnzj4y55d5chhoardpgvlji4ilrfwrr6asz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[2ylzn3y4vcv2t7rbq447jvkl6by5qf32ycwks3h2nbcmm6wvmnk] example_inputs[8]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ioa426km4idgc234gbdjtmikotjmsh7hn3tdqahy4f4zia42h53] example_inputs[9]: ('s4',)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []",
	"[jan4rfottd4caig5fc5h3ieb5ephfyhn7rn27jmu2ehbwgssm5j] fx_kwargs[user_visible_outputs]: {'fused_dequantize_op': None}",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1",
	"[kcuxe2zwm3mzv2uk6adm6iskoy35bqfv725twacrdewod2dbl5d] inputs_to_check[1]: 3",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[2]: 4",
	"[agkvbkaha53nbz3aeeuhvxjvvc4glhfjofzkg6g2qjoo2e5otcx] inputs_to_check[3]: 6",
	"[j3s5elu6itwgjafc7rzhy4whrbufl6kfmlufjhh25grt643bk5f] inputs_to_check[4]: 7",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inputs_to_check[5]: 8",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 36553287,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:52:05.294000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "479c7fc7b4aeb8b6d83b60fc11accd35"}
	{"key": "fodgjswdodzqgx3iq4x2v2ulrxwzzqobix2qlzyjko7fokxzlgfp", "components": ["[grf5bl6rpigmyiaz6c6m4gfmm52jce23fz2stipebcebees47ma] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1):\n    mul = arg0_1 * 2;  arg0_1 = None\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, arg7_1, arg1_1, arg8_1, mul, 256, 32, 16384, 8192, arg9_1, 2048, torch.bfloat16);  arg3_1 = arg4_1 = arg6_1 = arg7_1 = arg1_1 = arg8_1 = mul = arg9_1 = None\n    return (fused_dequantize_op,)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[tpvnjs264ct5lugh57xhauij37yhdwzbkxdyhhhhj5de2tskcds] example_inputs[0]: ('s1',)", "[7vwriczjssi3zjl7yvm6w4lda6j63tvcm76dxaglqafuowfzcmv] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([1, s1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[2]: ('s2',)", "[sqttp5i5wnsdcripsqrsj7sbdbvyr6p2v547c3del3vagfumq23] example_inputs[3]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([s2]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[4]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[5]: ('s3',)", "[osj4eueidclo5irb65z7qycfmcyzhzy3mt2msh2p7b3yhteohpt] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([s3]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[j23zbqrguggah6nscnzj4y55d5chhoardpgvlji4ilrfwrr6asz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[2ylzn3y4vcv2t7rbq447jvkl6by5qf32ycwks3h2nbcmm6wvmnk] example_inputs[8]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ioa426km4idgc234gbdjtmikotjmsh7hn3tdqahy4f4zia42h53] example_inputs[9]: ('s4',)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []", "[jan4rfottd4caig5fc5h3ieb5ephfyhn7rn27jmu2ehbwgssm5j] fx_kwargs[user_visible_outputs]: {'fused_dequantize_op': None}", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1", "[kcuxe2zwm3mzv2uk6adm6iskoy35bqfv725twacrdewod2dbl5d] inputs_to_check[1]: 3", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[2]: 4", "[agkvbkaha53nbz3aeeuhvxjvvc4glhfjofzkg6g2qjoo2e5otcx] inputs_to_check[3]: 6", "[j3s5elu6itwgjafc7rzhy4whrbufl6kfmlufjhh25grt643bk5f] inputs_to_check[4]: 7", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inputs_to_check[5]: 8", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 36553287, "cache_state": "hit"}
V0326 23:52:05.294000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "fa8f3f9f34ded1bd4efb0194733063cd"}
	{
	"name": "inductor_compile",
	"ts": 1742993525294936.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.295000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "649b25b49606824c00d377d048e84ee5"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993525295178.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.295000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "4f23059fde987a6493b5ad53d718edb6"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993525295529.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.296000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "c4e96bb4503adccc9ea759ce6f62753f"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993525296808.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.297000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "eec57467677f8f2bfc9b69039aaaa391"}
	{
	"name": "backend_compile",
	"ts": 1742993525297205.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.297000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "0692df4a552c925a7830b12987d6505a"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993525297486.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.305000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "d343b98b9f0c128ab87c346737f452b6"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['A'], accessed_by=DictGetItemGuardAccessor(A)
	| | +- TENSOR_MATCH: check_tensor(L['A'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[1, None], stride=[1, 1])
	| | +- NO_HASATTR: hasattr(L['A'], '_dynamo_dynamic_indices') == False         
	| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['A'], L['quant_state'].code, L['quant_state'].absmax, L['quant_state'].offset, L['quant_state'].state2.code, L['quant_state'].state2.absmax)
	| +- GuardManager: source=L['quant_state'], accessed_by=DictGetItemGuardAccessor(quant_state)
	| | +- TYPE_MATCH: ___check_type_id(L['quant_state'], 102513767659184)         
	| | +- GuardManager: source=L['quant_state'].code, accessed_by=GetAttrGuardAccessor(code)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].code, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | +- EQUALS_MATCH: L['quant_state'].dtype == torch.bfloat16                    
	| | +- GuardManager: source=L['quant_state'].shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].shape, 140717689517824)   
	| | | +- LENGTH_CHECK: len(L['quant_state'].shape) == 2                            
	| | | +- GuardManager: source=L['quant_state'].shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].shape[0], 140718482538560)
	| | | +- GuardManager: source=L['quant_state'].shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | +- EQUALS_MATCH: L['quant_state'].shape[1] == 2048                           
	| | +- GuardManager: source=L['quant_state'].absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[None], stride=[1])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].absmax, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].offset, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].state2, 102513767659184)  
	| | | +- GuardManager: source=L['quant_state'].state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | +- TENSOR_MATCH: check_tensor(L['quant_state'].state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | +- NO_HASATTR: hasattr(L['quant_state'].state2.code, '_dynamo_dynamic_indices') == False
	| | | | +- NO_TENSOR_ALIASING
	| | | +- GuardManager: source=L['quant_state'].state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | +- TENSOR_MATCH: check_tensor(L['quant_state'].state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[None], stride=[1])
	| | | | +- NO_HASATTR: hasattr(L['quant_state'].state2.absmax, '_dynamo_dynamic_indices') == False
	| | | | +- NO_TENSOR_ALIASING
	| | | +- GuardManager: source=L['quant_state'].state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | +- EQUALS_MATCH: L['quant_state'].state2.blocksize == 256                    
	| | +- GuardManager: source=L['quant_state'].blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | +- EQUALS_MATCH: L['quant_state'].blocksize == 64                            
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)
	| | | +- ID_MATCH: ___check_obj_id(G['torch'], 140718364431024)                
	| | | +- GuardManager: source=G['torch'].numel, accessed_by=GetAttrGuardAccessor(numel)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].numel, 140718363174816)          
	| | +- GuardManager: source=G['DEBUG_FLAG'], accessed_by=DictGetItemGuardAccessor(DEBUG_FLAG)
	| | | +- ID_MATCH: ___check_obj_id(G['DEBUG_FLAG'], 140718482561760)           
	| | +- GuardManager: source=G['fused_dequantize_op'], accessed_by=DictGetItemGuardAccessor(fused_dequantize_op)
	| | | +- TYPE_MATCH: ___check_type_id(G['fused_dequantize_op'], 102513752736032) 
	| | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize_op'], 140712749840912)  
	| | | +- GuardManager: source=G['fused_dequantize_op']._opoverload, accessed_by=GetAttrGuardAccessor(_opoverload)
	| | | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize_op']._opoverload, 140712749959024)
	+- LAMBDA_GUARD: Ne(2048*L['quant_state'].shape[0], 0)                         # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: Ne(L['quant_state'].shape[0], 1)                              # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2048*L['quant_state'].shape[0] >= 2                           # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['A'].size()[1]                                         # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['quant_state'].absmax.size()[0]                        # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['quant_state'].state2.absmax.size()[0]                 # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 0 <= L['quant_state'].shape[0]                                # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0326 23:52:05.305000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "aad1c08210abc316743d51d71efdb8c5"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993525305857.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.306000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0, "has_payload": "335bef42f272025f1548f13b08a65d73"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993525306104.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 6,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.306000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "2/1", "frame_key": "5", "co_name": "fused_dequantize", "co_filename": "/home/tom/unsloth-challenges/submit/Problem_C/kernels.py", "co_firstlineno": 330, "cache_size": 1, "accumulated_cache_size": 1, "guard_count": 25, "shape_env_guard_count": 6, "graph_op_count": 2, "graph_node_count": 13, "graph_input_count": 10, "start_time": 1742993525.243304, "entire_frame_compile_time_s": 0.06251811981201172, "backend_compile_time_s": 0.03264164924621582, "inductor_compile_time_s": 0.011409759521484375, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": ["mylib::fused_dequantize_op"], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 2, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:05.315000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/tmp/ipykernel_331427/318589162.py", 44]}
V0326 23:52:05.315000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 10, "name": "compiled_llama_mlp", "filename": 44}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.316000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "da4e8fa04cdaeb4ac35498025c513f91"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993525316037.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.316000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "931bceda2380b5d3dde498d4c282665f"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993525316037.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.330000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 12, "size": 409600}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.331000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 3, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [1, 100, 2048], "requires_grad": true, "stride": [204800, 2048, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ff9c2a8c5a0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.331000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 0, "source": "L['x']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.339000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 12, "size": 8388608}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.339000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [8388608, 1], "is_leaf": true, "stride": [1, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa340392c0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.339000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 1, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.532000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 12, "size": 262144}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.532000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [262144], "is_leaf": true, "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3094dbd0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.533000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 3, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.533000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 12, "size": 1024}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.534000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3094de00>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.534000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 4, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.535000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 12, "size": 4096}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.535000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024], "is_leaf": true, "stride": [1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3093c690>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.535000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 5, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.536000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 12, "size": 2}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.536000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3094e3a0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.536000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 6, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.537000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 6, "describer_id": 12, "size": 64}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.537000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 6, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e831e50>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.537000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 7, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.594000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 7, "describer_id": 12, "size": 262144}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.594000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 19, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 2048], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2048, 1], "storage": 7, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7ffa2e835220>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.594000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 19, "source": "L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.599000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/lazy.py", 45]}
V0326 23:52:05.599000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/nn_module.py", 46]}
V0326 23:52:05.600000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py", 47]}
V0326 23:52:05.600000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/torch.py", 48]}
V0326 23:52:05.600000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_prims_common/wrappers.py", 49]}
V0326 23:52:05.600000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_decomp/decompositions.py", 50]}
V0326 23:52:05.600000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_ops.py", 51]}
V0326 23:52:05.600000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_meta_registrations.py", 52]}
V0326 23:52:05.601000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", 53]}
V0326 23:52:05.601000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s1", "sources": ["L['x'].size()[2]"], "value": "2048", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 897, "name": "call_function", "filename": 48}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 271, "name": "_fn", "filename": 49}, {"line": 4364, "name": "matmul", "filename": 50}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 2013, "name": "_dispatch_impl", "filename": 32}, {"line": 716, "name": "__call__", "filename": 51}, {"line": 273, "name": "_fn", "filename": 49}, {"line": 2100, "name": "meta_mm", "filename": 52}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 518, "name": "forward", "filename": 39}, {"line": 125, "name": "forward", "filename": 53}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.603000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 8, "describer_id": 12, "size": 1048576}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.604000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 24, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [8192, 32], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [32, 1], "storage": 8, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7ffa2e834dc0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.604000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 24, "source": "L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.610000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s5", "sources": ["L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0]"], "value": "8192", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2220, "name": "BINARY_OP", "filename": 27}, {"line": 301, "name": "impl", "filename": 27}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 943, "name": "_handle_insert_op_in_graph", "filename": 28}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 1924, "name": "_dispatch_impl", "filename": 32}, {"line": 831, "name": "fast_binary_impl", "filename": 33}, {"line": 785, "name": "infer_size", "filename": 33}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 536, "name": "forward", "filename": 39}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.634000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 9, "describer_id": 12, "size": 8388608}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.634000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 29, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [8388608, 1], "is_leaf": true, "stride": [1, 1], "storage": 9, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa34039270>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.634000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 29, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.641000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 10, "describer_id": 12, "size": 262144}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.641000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 30, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [262144], "is_leaf": true, "stride": [1], "storage": 10, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa30940fa0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.642000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 30, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.642000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 11, "describer_id": 12, "size": 1024}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.642000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 31, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 11, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e830af0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.643000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 31, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.643000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 12, "describer_id": 12, "size": 4096}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.644000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 32, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024], "is_leaf": true, "stride": [1], "storage": 12, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e833930>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.644000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 32, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.644000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 13, "describer_id": 12, "size": 2}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.645000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 33, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 13, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e8311d0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.645000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 33, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.646000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 14, "describer_id": 12, "size": 64}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.646000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 34, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 14, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa30942f30>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.646000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 34, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.664000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/misc.py", 54]}
V0326 23:52:05.664000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/higher_order_ops.py", 55]}
V0326 23:52:05.664000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s8", "sources": ["L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1]"], "value": "2048", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 1680, "name": "CALL_FUNCTION_EX", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 1024, "name": "call_function", "filename": 54}, {"line": 774, "name": "call_method", "filename": 54}, {"line": 699, "name": "call_apply", "filename": 54}, {"line": 2015, "name": "call_function", "filename": 55}, {"line": 462, "name": "speculate_subgraph", "filename": 55}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 897, "name": "call_function", "filename": 48}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 271, "name": "_fn", "filename": 49}, {"line": 4364, "name": "matmul", "filename": 50}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 2013, "name": "_dispatch_impl", "filename": 32}, {"line": 716, "name": "__call__", "filename": 51}, {"line": 273, "name": "_fn", "filename": 49}, {"line": 2100, "name": "meta_mm", "filename": 52}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 496, "name": "forward", "filename": 39}, {"line": 110, "name": "inner_transpose_forward", "filename": 40}, {"line": 34, "name": "forward", "filename": 40}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.688000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 15, "describer_id": 12, "size": 262144}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.688000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 46, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 2048], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2048, 1], "storage": 15, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7ffa2e834500>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.688000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 46, "source": "L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.691000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 16, "describer_id": 12, "size": 1048576}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.692000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 50, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [8192, 32], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [32, 1], "storage": 16, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7ffa2e834190>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.692000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 50, "source": "L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.696000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s9", "sources": ["L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0]"], "value": "8192", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2220, "name": "BINARY_OP", "filename": 27}, {"line": 301, "name": "impl", "filename": 27}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 943, "name": "_handle_insert_op_in_graph", "filename": 28}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 1924, "name": "_dispatch_impl", "filename": 32}, {"line": 831, "name": "fast_binary_impl", "filename": 33}, {"line": 785, "name": "infer_size", "filename": 33}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 536, "name": "forward", "filename": 39}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.711000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 17, "describer_id": 12, "size": 8388608}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.711000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 54, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [8388608, 1], "is_leaf": true, "stride": [1, 1], "storage": 17, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa34039680>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.711000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 54, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.718000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 18, "describer_id": 12, "size": 262144}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.718000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 55, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [262144], "is_leaf": true, "stride": [1], "storage": 18, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa30949db0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.719000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 55, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.719000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 19, "describer_id": 12, "size": 1024}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.720000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 56, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 19, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3094a1c0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.720000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 56, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.720000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 20, "describer_id": 12, "size": 4096}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.721000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 57, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024], "is_leaf": true, "stride": [1], "storage": 20, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa30938c80>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.721000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 57, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.721000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 21, "describer_id": 12, "size": 2}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.722000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 58, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 21, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa30948eb0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.722000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 58, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.723000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 22, "describer_id": 12, "size": 64}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.723000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 59, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 22, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3093dae0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.723000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 59, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.740000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s12", "sources": ["L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1]"], "value": "8192", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 1680, "name": "CALL_FUNCTION_EX", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 1024, "name": "call_function", "filename": 54}, {"line": 774, "name": "call_method", "filename": 54}, {"line": 699, "name": "call_apply", "filename": 54}, {"line": 2015, "name": "call_function", "filename": 55}, {"line": 462, "name": "speculate_subgraph", "filename": 55}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 897, "name": "call_function", "filename": 48}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 271, "name": "_fn", "filename": 49}, {"line": 4364, "name": "matmul", "filename": 50}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 2013, "name": "_dispatch_impl", "filename": 32}, {"line": 716, "name": "__call__", "filename": 51}, {"line": 273, "name": "_fn", "filename": 49}, {"line": 2100, "name": "meta_mm", "filename": 52}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 496, "name": "forward", "filename": 39}, {"line": 110, "name": "inner_transpose_forward", "filename": 40}, {"line": 34, "name": "forward", "filename": 40}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.764000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 23, "describer_id": 12, "size": 1048576}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.765000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 71, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 8192], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [8192, 1], "storage": 23, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7ffa2e8359f0>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.765000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 71, "source": "L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.769000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 24, "describer_id": 12, "size": 262144}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.769000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 76, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [2048, 32], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [32, 1], "storage": 24, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7ffa2e834050>", "describer_id": 12}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.769000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 76, "source": "L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.774000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s13", "sources": ["L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0]"], "value": "2048", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2220, "name": "BINARY_OP", "filename": 27}, {"line": 301, "name": "impl", "filename": 27}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 943, "name": "_handle_insert_op_in_graph", "filename": 28}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 1924, "name": "_dispatch_impl", "filename": 32}, {"line": 831, "name": "fast_binary_impl", "filename": 33}, {"line": 785, "name": "infer_size", "filename": 33}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 536, "name": "forward", "filename": 39}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.779000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", 56]}
V0326 23:52:05.779000 331427 .venv/lib/python3.12/site-packages/torch/_logging/structured.py:22] {"str": ["/home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/fx/passes/runtime_assert.py", 57]}
V0326 23:52:05.779000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s4", "sources": ["L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1]"], "value": "2048", "reason": "find", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2987, "name": "RETURN_VALUE", "filename": 27}, {"line": 2972, "name": "_return", "filename": 27}, {"line": 1142, "name": "compile_subgraph", "filename": 56}, {"line": 1317, "name": "compile_and_call_fx_graph", "filename": 56}, {"line": 287, "name": "insert_deferred_runtime_asserts", "filename": 57}, {"line": 280, "name": "match_symbol", "filename": 57}, {"line": 169, "name": "expr", "filename": 36}, {"line": 1532, "name": "wrapper", "filename": 35}, {"line": 4572, "name": "replace", "filename": 35}, {"line": 1532, "name": "wrapper", "filename": 35}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 4885, "name": "_find", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": null}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:05.789000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_x_": [1, "s0", "2048"], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data": [8388608, 1], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax": [262144], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code": [256], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax": [1024], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset": [], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code": [16], "l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_": [32, 2048], "l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_": [8192, 32], "l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data": [8388608, 1], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax": [262144], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code": [256], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax": [1024], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset": [], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code": [16], "l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_": [32, 2048], "l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_": [8192, 32], "l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data": [8388608, 1], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax": [262144], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code": [256], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax": [1024], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset": [], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code": [16], "l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_": [32, 8192], "l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_": [2048, 32], "x": [1, "s0", "2048"], "B": [1, 8388608], "autograd_function_apply": [1, "s0", "8192"], "result": [1, "s0", "8192"], "result_1": [1, "s0", "8192"], "linear": [1, "s0", 32], "linear_1": [1, "s0", 8192], "output": [1, "s0", 8192], "result_2": [1, "s0", 8192], "silu": [1, "s0", 8192], "x_1": [1, "s0", 2048], "B_1": [1, 8388608], "autograd_function_apply_1": [1, "s0", "8192"], "result_3": [1, "s0", "8192"], "result_4": [1, "s0", "8192"], "linear_2": [1, "s0", 32], "linear_3": [1, "s0", 8192], "output_1": [1, "s0", 8192], "result_5": [1, "s0", 8192], "mul_2": [1, "s0", 8192], "x_2": [1, "s0", 8192], "B_2": [1, 8388608], "autograd_function_apply_2": [1, "s0", "2048"], "result_6": [1, "s0", "2048"], "result_7": [1, "s0", "2048"], "linear_4": [1, "s0", 32], "linear_5": [1, "s0", 2048], "output_2": [1, "s0", 2048], "result_8": [1, "s0", 2048]}}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "9a36626f9a16bbb1ceafa7902a308e8e"}
	class GraphModule(torch.nn.Module):
	    def forward(self, s0: "Sym(s0)", L_x_: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data: "u8[8388608, 1][1, 1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s2)", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s3)", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", L_self_modules_gate_proj_modules_lora_A_modules_default_parameters_weight_: "f32[32, 2048][2048, 1]cuda:0", L_self_modules_gate_proj_modules_lora_B_modules_default_parameters_weight_: "f32[8192, 32][32, 1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_some_data: "u8[8388608, 1][1, 1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s6)", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s7)", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", L_self_modules_up_proj_modules_lora_A_modules_default_parameters_weight_: "f32[32, 2048][2048, 1]cuda:0", L_self_modules_up_proj_modules_lora_B_modules_default_parameters_weight_: "f32[8192, 32][32, 1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_some_data: "u8[8388608, 1][1, 1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s10)", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s11)", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(8192)", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(2048)", L_self_modules_down_proj_modules_lora_A_modules_default_parameters_weight_: "f32[32, 8192][8192, 1]cuda:0", L_self_modules_down_proj_modules_lora_B_modules_default_parameters_weight_: "f32[2048, 32][32, 1]cuda:0"):
	        l_x_ = L_x_
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data = L_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_
	        l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_ = L_self_modules_gate_proj_modules_lora_A_modules_default_parameters_weight_
	        l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_ = L_self_modules_gate_proj_modules_lora_B_modules_default_parameters_weight_
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data = L_self_modules_up_proj_modules_base_layer_parameters_weight_some_data
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_
	        l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_ = L_self_modules_up_proj_modules_lora_A_modules_default_parameters_weight_
	        l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_ = L_self_modules_up_proj_modules_lora_B_modules_default_parameters_weight_
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data = L_self_modules_down_proj_modules_base_layer_parameters_weight_some_data
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_
	        l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_ = L_self_modules_down_proj_modules_lora_A_modules_default_parameters_weight_
	        l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_ = L_self_modules_down_proj_modules_lora_B_modules_default_parameters_weight_
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        x: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = l_x_.to(torch.bfloat16)
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        B: "u8[1, 8388608][1, 1]cuda:0" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data.t();  l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        function_ctx = torch.autograd.function.FunctionCtx();  function_ctx = None
	        fwd_body_0 = self.fwd_body_0
	        bwd_body_0 = self.bwd_body_0
	        autograd_function_apply: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.higher_order.autograd_function_apply(fwd_body_0, bwd_body_0, x, B, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, args_tensor_mask = [True, True, False, False, False], non_differentiable_idx = []);  fwd_body_0 = bwd_body_0 = x = B = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	        result: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = autograd_function_apply.to(torch.float16);  autograd_function_apply = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        result_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result.clone();  result = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        linear: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_, None);  l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_ = None
	        linear_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(linear, l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_, None);  linear = l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_ = None
	        output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = linear_1 * 2.0;  linear_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        result_2: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result_1 + output;  result_1 = output = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        silu: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.nn.functional.silu(result_2, inplace = False);  result_2 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        x_1: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = l_x_.to(torch.bfloat16)
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        B_1: "u8[1, 8388608][1, 1]cuda:0" = l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data.t();  l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        function_ctx_1 = torch.autograd.function.FunctionCtx();  function_ctx_1 = None
	        fwd_body_1 = self.fwd_body_1
	        bwd_body_1 = self.bwd_body_1
	        autograd_function_apply_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.higher_order.autograd_function_apply(fwd_body_1, bwd_body_1, x_1, B_1, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, args_tensor_mask = [True, True, False, False, False], non_differentiable_idx = []);  fwd_body_1 = bwd_body_1 = x_1 = B_1 = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	        result_3: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = autograd_function_apply_1.to(torch.float16);  autograd_function_apply_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        result_4: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result_3.clone();  result_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        linear_2: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_, None);  l_x_ = l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_ = None
	        linear_3: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(linear_2, l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_, None);  linear_2 = l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_ = None
	        output_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = linear_3 * 2.0;  linear_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        result_5: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result_4 + output_1;  result_4 = output_1 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_2: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = silu * result_5;  silu = result_5 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        x_2: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = mul_2.to(torch.bfloat16)
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        B_2: "u8[1, 8388608][1, 1]cuda:0" = l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data.t();  l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        function_ctx_2 = torch.autograd.function.FunctionCtx();  function_ctx_2 = None
	        fwd_body_2 = self.fwd_body_2
	        bwd_body_2 = self.bwd_body_2
	        autograd_function_apply_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.higher_order.autograd_function_apply(fwd_body_2, bwd_body_2, x_2, B_2, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, args_tensor_mask = [True, True, False, False, False], non_differentiable_idx = []);  fwd_body_2 = bwd_body_2 = x_2 = B_2 = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	        result_6: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = autograd_function_apply_2.to(torch.float16);  autograd_function_apply_2 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        result_7: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = result_6.clone();  result_6 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        linear_4: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch._C._nn.linear(mul_2, l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_, None);  mul_2 = l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_ = None
	        linear_5: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch._C._nn.linear(linear_4, l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_, None);  linear_4 = l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_ = None
	        output_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = linear_5 * 2.0;  linear_5 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        result_8: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = result_7 + output_2;  result_7 = output_2 = None
	        return (result_8,)
	        
	    class fwd_body_0(torch.nn.Module):
	        def forward(self, ctx, A: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", B: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s3)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s2)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            a = A
	            b = B
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:21 in forward, code: if prod(A.shape) == 0:
	            size = a.size()
	            getitem = size[0];  getitem = None
	            getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
	            getitem_2: "Sym(2048)" = size[2];  getitem_2 = None
	            prod: "Sym(2048*s0)" = math_prod(size);  size = None
	            eq: "Sym(False)" = prod == 0;  prod = eq = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s3/2))" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s2*s3)" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s2*s3/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  rshift = mul = rshift_1 = None
	            
	             # File: /tmp/ipykernel_331427/1346670402.py:18 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:34 in forward, code: output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias) # NOTE the transposition
	            to: "bf16[2048, 8192][1, 2048]cuda:0" = t.to(torch.bfloat16);  t = None
	            t_1: "bf16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(a, t_1, None);  a = t_1 = None
	            return (output, [l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_])
	            
	    class bwd_body_0(torch.nn.Module):
	        def forward(self, ctx, grad_output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s3)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s2)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", b: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            # No stacktrace found for following nodes
	            _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s3/2))" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s2*s3)" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize;  l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s2*s3/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset = b = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = rshift = mul = rshift_1 = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	            
	             # File: /tmp/ipykernel_331427/1346670402.py:18 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:65 in backward, code: grad_A = torch.matmul(grad_output, F.dequantize_4bit(B, ctx.state).to(grad_output.dtype).t()) # NOTE the transposition
	            to: "f16[2048, 8192][1, 2048]cuda:0" = t.to(torch.float16);  t = None
	            t_1: "f16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            grad_A: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.matmul(grad_output, t_1);  grad_output = t_1 = None
	            
	            # No stacktrace found for following nodes
	            _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None
	            return (grad_A, None)
	            
	    class fwd_body_1(torch.nn.Module):
	        def forward(self, ctx, A: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", B: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s7)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s6)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            a = A
	            b = B
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:21 in forward, code: if prod(A.shape) == 0:
	            size = a.size()
	            getitem = size[0];  getitem = None
	            getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
	            getitem_2 = size[2];  getitem_2 = None
	            prod: "Sym(2048*s0)" = math_prod(size);  size = None
	            eq: "Sym(False)" = prod == 0;  prod = eq = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s7/2))" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s6*s7)" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s6*s7/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  rshift = mul = rshift_1 = None
	            
	             # File: /tmp/ipykernel_331427/1346670402.py:18 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:34 in forward, code: output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias) # NOTE the transposition
	            to: "bf16[2048, 8192][1, 2048]cuda:0" = t.to(torch.bfloat16);  t = None
	            t_1: "bf16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(a, t_1, None);  a = t_1 = None
	            return (output, [l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_])
	            
	    class bwd_body_1(torch.nn.Module):
	        def forward(self, ctx, grad_output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s7)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s6)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", b: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            # No stacktrace found for following nodes
	            _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s7/2))" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s6*s7)" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize;  l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s6*s7/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset = b = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = rshift = mul = rshift_1 = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	            
	             # File: /tmp/ipykernel_331427/1346670402.py:18 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:65 in backward, code: grad_A = torch.matmul(grad_output, F.dequantize_4bit(B, ctx.state).to(grad_output.dtype).t()) # NOTE the transposition
	            to: "f16[2048, 8192][1, 2048]cuda:0" = t.to(torch.float16);  t = None
	            t_1: "f16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            grad_A: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.matmul(grad_output, t_1);  grad_output = t_1 = None
	            
	            # No stacktrace found for following nodes
	            _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None
	            return (grad_A, None)
	            
	    class fwd_body_2(torch.nn.Module):
	        def forward(self, ctx, A: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", B: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s11)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s10)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(2048)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(8192)"):
	            a = A
	            b = B
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:21 in forward, code: if prod(A.shape) == 0:
	            size = a.size()
	            getitem = size[0];  getitem = None
	            getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
	            getitem_2 = size[2];  getitem_2 = None
	            prod: "Sym(8192*s0)" = math_prod(size);  size = None
	            eq: "Sym(False)" = prod == 0;  prod = eq = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s11/2))" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s10*s11)" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s10*s11/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  rshift = mul = rshift_1 = None
	            
	             # File: /tmp/ipykernel_331427/1346670402.py:18 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[8192, 2048][1, 8192]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:34 in forward, code: output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias) # NOTE the transposition
	            to: "bf16[8192, 2048][1, 8192]cuda:0" = t.to(torch.bfloat16);  t = None
	            t_1: "bf16[2048, 8192][8192, 1]cuda:0" = to.t();  to = None
	            output: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch._C._nn.linear(a, t_1, None);  a = t_1 = None
	            return (output, [l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_])
	            
	    class bwd_body_2(torch.nn.Module):
	        def forward(self, ctx, grad_output: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s11)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s10)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", b: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(2048)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(8192)"):
	            # No stacktrace found for following nodes
	            _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s11/2))" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s10*s11)" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize;  l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s10*s11/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset = b = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = rshift = mul = rshift_1 = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	            
	             # File: /tmp/ipykernel_331427/1346670402.py:18 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[8192, 2048][1, 8192]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:65 in backward, code: grad_A = torch.matmul(grad_output, F.dequantize_4bit(B, ctx.state).to(grad_output.dtype).t()) # NOTE the transposition
	            to: "f16[8192, 2048][1, 8192]cuda:0" = t.to(torch.float16);  t = None
	            t_1: "f16[2048, 8192][8192, 1]cuda:0" = to.t();  to = None
	            grad_A: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.matmul(grad_output, t_1);  grad_output = t_1 = None
	            
	            # No stacktrace found for following nodes
	            _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None
	            return (grad_A, None)
	            
V0326 23:52:05.790000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "d90deedddcb933456a32e74493bc22b5"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993525789981.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.790000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "d9349f147117b25473556bc13e7b465e"}
	{
	"name": "backend_compile",
	"ts": 1742993525789981.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:05.802000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "31423cd110a675994bae9d2f66cc82ea"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993525802252.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.058000 331427 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "9d573e932b412c71350208d5afea101b"}
	class joint_helper(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "Sym(s0)"; primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"; primals_3: "u8[8388608, 1][1, 1]cuda:0"; primals_4: "Sym(s2)"; primals_5: "Sym(s3)"; primals_6: "u8[262144][1]cuda:0"; primals_7: "f32[256][1]cuda:0"; primals_8: "f32[1024][1]cuda:0"; primals_9: "f16[][]cuda:0"; primals_10: "f32[16][1]cuda:0"; primals_11: "Sym(2048)"; primals_12: "Sym(8192)"; primals_13: "f32[32, 2048][2048, 1]cuda:0"; primals_14: "f32[8192, 32][32, 1]cuda:0"; primals_15: "u8[8388608, 1][1, 1]cuda:0"; primals_16: "Sym(s6)"; primals_17: "Sym(s7)"; primals_18: "u8[262144][1]cuda:0"; primals_19: "f32[256][1]cuda:0"; primals_20: "f32[1024][1]cuda:0"; primals_21: "f16[][]cuda:0"; primals_22: "f32[16][1]cuda:0"; primals_23: "Sym(2048)"; primals_24: "Sym(8192)"; primals_25: "f32[32, 2048][2048, 1]cuda:0"; primals_26: "f32[8192, 32][32, 1]cuda:0"; primals_27: "u8[8388608, 1][1, 1]cuda:0"; primals_28: "Sym(s10)"; primals_29: "Sym(s11)"; primals_30: "u8[262144][1]cuda:0"; primals_31: "f32[256][1]cuda:0"; primals_32: "f32[1024][1]cuda:0"; primals_33: "f16[][]cuda:0"; primals_34: "f32[16][1]cuda:0"; primals_35: "Sym(8192)"; primals_36: "Sym(2048)"; primals_37: "f32[32, 8192][8192, 1]cuda:0"; primals_38: "f32[2048, 32][32, 1]cuda:0"; tangents_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.bfloat16)
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift: "Sym(floor(s3/2))" = primals_5 >> 1
	        mul_4: "Sym(s2*s3)" = primals_4 * primals_5;  primals_5 = None
	        rshift_1: "Sym(floor(s2*s3/2))" = mul_4 >> 1
	        fused_dequantize_op: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16)
	        permute_1: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None
	        permute_2: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0]);  permute_1 = None
	        convert_element_type_1: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None
	        convert_element_type_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type, torch.float16);  convert_element_type = None
	        permute_3: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None
	        view: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2, [primals_1, 2048]);  convert_element_type_2 = None
	        mm: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view, permute_3);  view = permute_3 = None
	        view_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        clone: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.clone.default(view_1);  view_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_5: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None
	        permute_4: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None
	        view_2: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(primals_2, [primals_1, 2048])
	        mm_1: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_2, permute_4)
	        view_3: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None
	        convert_element_type_8: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None
	        permute_5: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None
	        view_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None
	        mm_2: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_4, permute_5)
	        view_5: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None
	        mul_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(clone, mul_39);  clone = mul_39 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        convert_element_type_11: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_13: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.bfloat16)
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_6: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_2: "Sym(floor(s7/2))" = primals_17 >> 1
	        mul_54: "Sym(s6*s7)" = primals_16 * primals_17;  primals_17 = None
	        rshift_3: "Sym(floor(s6*s7/2))" = mul_54 >> 1
	        fused_dequantize_op_1: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16)
	        permute_7: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None
	        permute_8: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0]);  permute_7 = None
	        convert_element_type_14: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None
	        convert_element_type_15: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_13, torch.float16);  convert_element_type_13 = None
	        permute_9: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None
	        view_6: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_15, [primals_1, 2048]);  convert_element_type_15 = None
	        mm_3: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_6, permute_9);  view_6 = permute_9 = None
	        view_7: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        clone_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.clone.default(view_7);  view_7 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_18: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None
	        permute_10: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None
	        view_8: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None
	        mm_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_8, permute_10)
	        view_9: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None
	        convert_element_type_21: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None
	        permute_11: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None
	        view_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None
	        mm_5: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_10, permute_11)
	        view_11: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None
	        mul_87: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_85: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(clone_1, mul_87);  clone_1 = mul_87 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_94: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85)
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_24: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_94, torch.bfloat16)
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_12: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_4: "Sym(floor(s11/2))" = primals_29 >> 1
	        mul_102: "Sym(s10*s11)" = primals_28 * primals_29;  primals_29 = None
	        rshift_5: "Sym(floor(s10*s11/2))" = mul_102 >> 1
	        fused_dequantize_op_2: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16)
	        permute_13: "bf16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None
	        permute_14: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_13, [1, 0]);  permute_13 = None
	        convert_element_type_25: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None
	        convert_element_type_26: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_24, torch.float16);  convert_element_type_24 = None
	        permute_15: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None
	        view_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_26, [primals_1, 8192]);  convert_element_type_26 = None
	        mm_6: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None
	        view_13: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        clone_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.clone.default(view_13);  view_13 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_29: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None
	        permute_16: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None
	        view_14: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None
	        mm_7: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_14, permute_16)
	        view_15: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None
	        convert_element_type_32: "f16[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None
	        permute_17: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None
	        view_16: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None
	        mm_8: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_16, permute_17)
	        view_17: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None
	        mul_137: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_131: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(clone_2, mul_137);  clone_2 = mul_137 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_144: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, 2.0)
	        view_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None
	        permute_18: "f16[2048, s0][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_18, [1, 0])
	        mm_9: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None
	        permute_19: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None
	        permute_20: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None
	        mm_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None
	        view_19: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None
	        permute_21: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None
	        convert_element_type_39: "f32[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None
	        view_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None
	        permute_22: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_20, [1, 0])
	        mm_11: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None
	        permute_23: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None
	        permute_24: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None
	        mm_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None
	        view_21: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None
	        permute_25: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None
	        convert_element_type_44: "f32[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        fused_dequantize_op_3: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None
	        permute_26: "bf16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_3, [1, 0]);  fused_dequantize_op_3 = None
	        convert_element_type_45: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.prims.convert_element_type.default(permute_26, torch.float16);  permute_26 = None
	        permute_27: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None
	        view_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None
	        mm_13: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None
	        view_23: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None
	        convert_element_type_48: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_23, torch.bfloat16);  view_23 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_49: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_48, torch.float16);  convert_element_type_48 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_135: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_21, convert_element_type_49);  view_21 = convert_element_type_49 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_146: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None
	        mul_147: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_148: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_146, 2.0)
	        view_24: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None
	        permute_28: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_24, [1, 0])
	        mm_14: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None
	        permute_29: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None
	        permute_30: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None
	        mm_15: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None
	        view_25: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None
	        permute_31: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None
	        convert_element_type_54: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None
	        view_26: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None
	        permute_32: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_26, [1, 0])
	        mm_16: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_32, view_8);  permute_32 = view_8 = None
	        permute_33: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None
	        permute_34: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None
	        mm_17: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None
	        view_27: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None
	        permute_35: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None
	        convert_element_type_59: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        fused_dequantize_op_4: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None
	        permute_36: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_4, [1, 0]);  fused_dequantize_op_4 = None
	        convert_element_type_60: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_36, torch.float16);  permute_36 = None
	        permute_37: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None
	        view_28: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None
	        mm_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None
	        view_29: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None
	        convert_element_type_63: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_29, torch.bfloat16);  view_29 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_64: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_63, torch.float16);  convert_element_type_63 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_136: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_27, convert_element_type_64);  view_27 = convert_element_type_64 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        sigmoid_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(add_39)
	        full: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_44: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(full, sigmoid_1);  full = None
	        mul_150: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None
	        add_137: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None
	        mul_151: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None
	        mul_152: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_153: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_152, 2.0)
	        view_30: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None
	        permute_39: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_30, [1, 0])
	        mm_19: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None
	        permute_40: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None
	        permute_41: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None
	        mm_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None
	        view_31: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None
	        permute_42: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None
	        convert_element_type_69: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None
	        view_32: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None
	        permute_43: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_32, [1, 0])
	        mm_21: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None
	        permute_44: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None
	        permute_45: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None
	        mm_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None
	        view_33: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        add_138: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_46: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None
	        convert_element_type_74: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        fused_dequantize_op_5: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None
	        permute_47: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_5, [1, 0]);  fused_dequantize_op_5 = None
	        convert_element_type_75: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_47, torch.float16);  permute_47 = None
	        permute_48: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None
	        view_34: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None
	        mm_23: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None
	        view_35: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None
	        convert_element_type_78: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_35, torch.bfloat16);  view_35 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_79: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_78, torch.float16);  convert_element_type_78 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_139: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_138, convert_element_type_79);  add_138 = convert_element_type_79 = None
	        return pytree.tree_unflatten([add_131, None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39], self._out_spec)
	        
V0326 23:52:06.125000 331427 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "502e69bfe3710ecee71d8da8593e0422"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "Sym(s0)", primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", primals_3: "u8[8388608, 1][1, 1]cuda:0", primals_4: "Sym(s2)", primals_5: "Sym(s3)", primals_6: "u8[262144][1]cuda:0", primals_7: "f32[256][1]cuda:0", primals_8: "f32[1024][1]cuda:0", primals_9: "f16[][]cuda:0", primals_10: "f32[16][1]cuda:0", primals_11: "Sym(2048)", primals_12: "Sym(8192)", primals_13: "f32[32, 2048][2048, 1]cuda:0", primals_14: "f32[8192, 32][32, 1]cuda:0", primals_15: "u8[8388608, 1][1, 1]cuda:0", primals_16: "Sym(s6)", primals_17: "Sym(s7)", primals_18: "u8[262144][1]cuda:0", primals_19: "f32[256][1]cuda:0", primals_20: "f32[1024][1]cuda:0", primals_21: "f16[][]cuda:0", primals_22: "f32[16][1]cuda:0", primals_23: "Sym(2048)", primals_24: "Sym(8192)", primals_25: "f32[32, 2048][2048, 1]cuda:0", primals_26: "f32[8192, 32][32, 1]cuda:0", primals_27: "u8[8388608, 1][1, 1]cuda:0", primals_28: "Sym(s10)", primals_29: "Sym(s11)", primals_30: "u8[262144][1]cuda:0", primals_31: "f32[256][1]cuda:0", primals_32: "f32[1024][1]cuda:0", primals_33: "f16[][]cuda:0", primals_34: "f32[16][1]cuda:0", primals_35: "Sym(8192)", primals_36: "Sym(2048)", primals_37: "f32[32, 8192][8192, 1]cuda:0", primals_38: "f32[2048, 32][32, 1]cuda:0"):
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift: "Sym(floor(s3/2))" = primals_5 >> 1
	        mul_4: "Sym(s2*s3)" = primals_4 * primals_5;  primals_5 = None
	        rshift_1: "Sym(floor(s2*s3/2))" = mul_4 >> 1
	        fused_dequantize_op: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None
	        permute_1: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None
	        permute_2: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0])
	        convert_element_type_1: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None
	        convert_element_type_default_5: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.float16)
	        permute_3: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None
	        view: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_default_5, [primals_1, 2048]);  convert_element_type_default_5 = None
	        mm: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view, permute_3);  permute_3 = None
	        view_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_5: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None
	        permute_4: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None
	        view_2: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None
	        mm_1: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_2, permute_4)
	        view_3: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None
	        convert_element_type_8: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None
	        permute_5: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None
	        view_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None
	        mm_2: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_4, permute_5)
	        view_5: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None
	        mul_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1, mul_39);  view_1 = mul_39 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        convert_element_type_11: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_6: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_2: "Sym(floor(s7/2))" = primals_17 >> 1
	        mul_54: "Sym(s6*s7)" = primals_16 * primals_17;  primals_17 = None
	        rshift_3: "Sym(floor(s6*s7/2))" = mul_54 >> 1
	        fused_dequantize_op_1: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None
	        permute_7: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None
	        permute_8: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0])
	        convert_element_type_14: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None
	        permute_9: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None
	        mm_3: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view, permute_9);  view = permute_9 = None
	        view_7: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_18: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None
	        permute_10: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None
	        mm_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_2, permute_10)
	        view_9: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None
	        convert_element_type_21: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None
	        permute_11: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None
	        view_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None
	        mm_5: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_10, permute_11)
	        view_11: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None
	        mul_87: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_85: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_7, mul_87);  view_7 = mul_87 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_94: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85);  convert_element_type_12 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_12: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_4: "Sym(floor(s11/2))" = primals_29 >> 1
	        mul_102: "Sym(s10*s11)" = primals_28 * primals_29;  primals_29 = None
	        rshift_5: "Sym(floor(s10*s11/2))" = mul_102 >> 1
	        fused_dequantize_op_2: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None
	        permute_13: "bf16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None
	        permute_14: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_13, [1, 0])
	        convert_element_type_25: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None
	        convert_element_type_default_3: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_94, torch.float16)
	        permute_15: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None
	        view_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_default_3, [primals_1, 8192]);  convert_element_type_default_3 = None
	        mm_6: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None
	        view_13: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_29: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None
	        permute_16: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None
	        view_14: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None
	        mm_7: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_14, permute_16)
	        view_15: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None
	        convert_element_type_32: "f16[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None
	        permute_17: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None
	        view_16: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None
	        mm_8: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_16, permute_17)
	        view_17: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None
	        mul_137: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_131: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_13, mul_137);  view_13 = mul_137 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_20: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None
	        permute_24: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_45: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.prims.convert_element_type.default(permute_13, torch.float16);  permute_13 = None
	        permute_27: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_30: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None
	        permute_34: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_60: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_7, torch.float16);  permute_7 = None
	        permute_37: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_41: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None
	        permute_45: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_75: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_1, torch.float16);  permute_1 = None
	        permute_48: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None
	        return (add_131, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, primals_1)
	        
V0326 23:52:06.128000 331427 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "e0aad3d7ff152d08541cef6a309ac5cb"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "Sym(s0)", view_2: "f16[s0, 2048][2048, 1]cuda:0", view_4: "f16[s0, 32][32, 1]cuda:0", add_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", view_10: "f16[s0, 32][32, 1]cuda:0", add_85: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", view_14: "f16[s0, 8192][8192, 1]cuda:0", view_16: "f16[s0, 32][32, 1]cuda:0", permute_20: "f16[2048, 32][32, 1]cuda:0", permute_24: "f16[32, 8192][8192, 1]cuda:0", permute_27: "f16[2048, 8192][8192, 1]cuda:0", permute_30: "f16[8192, 32][32, 1]cuda:0", permute_34: "f16[32, 2048][2048, 1]cuda:0", permute_37: "f16[8192, 2048][2048, 1]cuda:0", permute_41: "f16[8192, 32][32, 1]cuda:0", permute_45: "f16[32, 2048][2048, 1]cuda:0", permute_48: "f16[8192, 2048][2048, 1]cuda:0", tangents_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"):
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_144: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, 2.0)
	        view_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None
	        permute_18: "f16[2048, s0][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_18, [1, 0])
	        mm_9: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None
	        permute_19: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None
	        mm_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None
	        view_19: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None
	        permute_21: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None
	        convert_element_type_39: "f32[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None
	        view_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None
	        permute_22: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_20, [1, 0])
	        mm_11: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None
	        permute_23: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None
	        mm_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None
	        view_21: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None
	        permute_25: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None
	        convert_element_type_44: "f32[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None
	        mm_13: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None
	        view_23: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_default_2: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_23, torch.float16);  view_23 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_135: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_21, convert_element_type_default_2);  view_21 = convert_element_type_default_2 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        convert_element_type_11: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        mul_146: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None
	        mul_147: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_148: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_146, 2.0)
	        view_24: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None
	        permute_28: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_24, [1, 0])
	        mm_14: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None
	        permute_29: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None
	        mm_15: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None
	        view_25: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None
	        permute_31: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None
	        convert_element_type_54: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None
	        view_26: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None
	        permute_32: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_26, [1, 0])
	        mm_16: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_32, view_2);  permute_32 = None
	        permute_33: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None
	        mm_17: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None
	        view_27: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None
	        permute_35: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None
	        convert_element_type_59: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_28: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None
	        mm_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None
	        view_29: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_default_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_29, torch.float16);  view_29 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_136: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_27, convert_element_type_default_1);  view_27 = convert_element_type_default_1 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        sigmoid_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(add_39)
	        full_default: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_44: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_default, sigmoid_1);  full_default = None
	        mul_150: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None
	        add_137: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None
	        mul_151: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None
	        mul_152: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_153: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_152, 2.0)
	        view_30: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None
	        permute_39: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_30, [1, 0])
	        mm_19: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None
	        permute_40: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None
	        mm_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None
	        view_31: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None
	        permute_42: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None
	        convert_element_type_69: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None
	        view_32: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None
	        permute_43: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_32, [1, 0])
	        mm_21: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None
	        permute_44: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None
	        mm_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None
	        view_33: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        add_138: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_46: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None
	        convert_element_type_74: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_34: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None
	        mm_23: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None
	        view_35: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_default: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_35, torch.float16);  view_35 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_139: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_138, convert_element_type_default);  add_138 = convert_element_type_default = None
	        return (None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39)
	        
V0326 23:52:06.129000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "2c209d0fa4e9facb0b61f457bbc89f3c"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993526129202.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.129000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "271ea1e607ac2e2aa8608f94048cc5fb"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993526129710.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.129000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "6b0f3e46b7d43fd3d37ca492f07cdb62"}
	{
	"name": "inductor_compile",
	"ts": 1742993526129710.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.202000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/6t/c6ti5avby2m7pt3ut73u7cv6pskab7oqrj32flivbrhi5omzahyd.py"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "2cfff416bd40de843c972c9ab081a285"}
	# AOT ID: ['5_forward']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/tc/ctc3wd5ie7s2fjcr2f4u2qxws22nb2vss5um5266to6nceu4xt3o.py
	# Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [mylib.fused_dequantize_op]
	# Source node to ATen node mapping:
	#   autograd_function_apply => fused_dequantize_op
	# Graph fragment:
	#   %fused_dequantize_op : [num_users=1] = call_function[target=torch.ops.mylib.fused_dequantize_op.default](args = (%primals_6, %primals_7, %primals_8, %primals_9, %permute, %primals_10, 16777216, %primals_4, %rshift, %mul_4, %rshift_1, 8192, 2048, torch.bfloat16), kwargs = {})
	triton_poi_fused_fused_dequantize_op_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {2: 1}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=(2,))]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_fused_dequantize_op_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 1
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    tmp0 = tl.load(in_ptr0 + (0)).to(tl.float32)
	    tmp1 = tl.broadcast_to(tmp0, [XBLOCK])
	    tl.store(out_ptr0 + (tl.full([XBLOCK], 0, tl.int32)), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/4i/c4ighga7fod2ed36hdz67v2ofqmyk7mlh2kc24zdiinrkgsdjog7.py
	# Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	#   autograd_function_apply => convert_element_type_1
	# Graph fragment:
	#   %convert_element_type_1 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_2, torch.float16), kwargs = {})
	triton_poi_fused__to_copy_1 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[16777216], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*bf16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_1', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 16777216
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/dm/cdm5ebpcdjlaqnrcavlevwhnlhakhmbh235waumdxi2dlz4oqdc3.py
	# Topologically Sorted Source Nodes: [linear], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	#   linear => convert_element_type_5, permute_4
	# Graph fragment:
	#   %convert_element_type_5 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_13, torch.float16), kwargs = {})
	#   %permute_4 : [num_users=2] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_5, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_2 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[65536], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp32', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_2', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 65536
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/4v/c4vy7sd4hdenvrautochni2fe3ce3tkssjud74ei4t45g5jo4bxq.py
	# Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	#   linear_1 => convert_element_type_8, permute_5
	# Graph fragment:
	#   %convert_element_type_8 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_14, torch.float16), kwargs = {})
	#   %permute_5 : [num_users=2] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_8, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_3 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp32', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_3', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 262144
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/5b/c5bxmvhk7qwah3r5zprbeosehsg4smud74ljqbertg7zky4hs4tq.py
	# Topologically Sorted Source Nodes: [output, result_2, silu, output_1, result_5, mul_2], Original ATen: [aten.mul, aten.add, aten.silu]
	# Source node to ATen node mapping:
	#   mul_2 => mul_94
	#   output => mul_39
	#   output_1 => mul_87
	#   result_2 => add_39
	#   result_5 => add_85
	#   silu => convert_element_type_11, convert_element_type_12, mul_46, sigmoid
	# Graph fragment:
	#   %mul_39 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_5, 2.0), kwargs = {})
	#   %add_39 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_1, %mul_39), kwargs = {})
	#   %convert_element_type_11 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_39, torch.float32), kwargs = {})
	#   %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_11,), kwargs = {})
	#   %mul_46 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_11, %sigmoid), kwargs = {})
	#   %convert_element_type_12 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_46, torch.float16), kwargs = {})
	#   %mul_87 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_11, 2.0), kwargs = {})
	#   %add_85 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_7, %mul_87), kwargs = {})
	#   %mul_94 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_12, %add_85), kwargs = {})
	triton_poi_fused_add_mul_silu_4 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1048576], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: '*fp16', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mul_silu_4', 'mutated_arg_names': ['in_out_ptr0', 'in_out_ptr1'], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_out_ptr0, in_out_ptr1, in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_out_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp5 = tl.load(in_out_ptr1 + (x0), None).to(tl.float32)
	    tmp6 = tl.load(in_ptr1 + (x0), None).to(tl.float32)
	    tmp2 = 2.0
	    tmp3 = tmp1 * tmp2
	    tmp4 = tmp0 + tmp3
	    tmp7 = tmp6 * tmp2
	    tmp8 = tmp5 + tmp7
	    tmp9 = tmp4.to(tl.float32)
	    tmp10 = tl.sigmoid(tmp9)
	    tmp11 = tmp9 * tmp10
	    tmp12 = tmp11.to(tl.float32)
	    tmp13 = tmp12 * tmp8
	    tl.store(in_out_ptr0 + (x0), tmp4, None)
	    tl.store(in_out_ptr1 + (x0), tmp8, None)
	    tl.store(out_ptr0 + (x0), tmp13, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/3b/c3bktvkn3n6hu4v7tiwk7rpos2wp4lajxxzrdgoicggbyxk5d3cb.py
	# Topologically Sorted Source Nodes: [output_2, result_8], Original ATen: [aten.mul, aten.add]
	# Source node to ATen node mapping:
	#   output_2 => mul_137
	#   result_8 => add_131
	# Graph fragment:
	#   %mul_137 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_17, 2.0), kwargs = {})
	#   %add_131 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_13, %mul_137), kwargs = {})
	triton_poi_fused_add_mul_5 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mul_5', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_out_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp2 = 2.0
	    tmp3 = tmp1 * tmp2
	    tmp4 = tmp0 + tmp3
	    tl.store(in_out_ptr0 + (x0), tmp4, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/j7/cj776okodjzzoh7cfozhudvywfzftzc6cabab2mmcdjjege6gbpi.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_45 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_13, torch.float16), kwargs = {})
	triton_poi_fused__to_copy_6 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[8192, 2048], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*bf16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_6', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 8192
	    xnumel = 2048
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (8192*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (2048*y0)), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/mi/cmiz6hpfs7acuhgi57mdp7dpphsyd43zzboziig5wum6ghgma4e2.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_45 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_13, torch.float16), kwargs = {})
	#   %permute_27 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_45, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_7 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[2048, 8192], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_7', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 2048
	    xnumel = 8192
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (2048*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (8192*y0)), tmp0, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/vd/cvds2jaf5sirynr2qnt7rpujbamyslbkrjowetcdyb6ep4yhq4gq.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_60 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_7, torch.float16), kwargs = {})
	triton_poi_fused__to_copy_8 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[2048, 8192], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*bf16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_8', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 2048
	    xnumel = 8192
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (2048*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (8192*y0)), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/ai/caikh3jmq5xv3uwakclrkwfn3zmg36elfcn5skhch7uvaivem3uj.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_60 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_7, torch.float16), kwargs = {})
	#   %permute_37 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_60, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_9 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[8192, 2048], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_9', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 8192
	    xnumel = 2048
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (8192*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (2048*y0)), tmp0, None)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38 = args
	    args.clear()
	    s0 = primals_1
	    s2 = primals_4
	    s3 = primals_5
	    s6 = primals_16
	    s7 = primals_17
	    s10 = primals_28
	    s11 = primals_29
	    assert_size_stride(primals_2, (1, s0, 2048), (2048*s0, 2048, 1))
	    assert_size_stride(primals_3, (8388608, 1), (1, 1))
	    assert_size_stride(primals_6, (262144, ), (1, ))
	    assert_size_stride(primals_7, (256, ), (1, ))
	    assert_size_stride(primals_8, (1024, ), (1, ))
	    assert_size_stride(primals_9, (), ())
	    assert_size_stride(primals_10, (16, ), (1, ))
	    assert_size_stride(primals_13, (32, 2048), (2048, 1))
	    assert_size_stride(primals_14, (8192, 32), (32, 1))
	    assert_size_stride(primals_15, (8388608, 1), (1, 1))
	    assert_size_stride(primals_18, (262144, ), (1, ))
	    assert_size_stride(primals_19, (256, ), (1, ))
	    assert_size_stride(primals_20, (1024, ), (1, ))
	    assert_size_stride(primals_21, (), ())
	    assert_size_stride(primals_22, (16, ), (1, ))
	    assert_size_stride(primals_25, (32, 2048), (2048, 1))
	    assert_size_stride(primals_26, (8192, 32), (32, 1))
	    assert_size_stride(primals_27, (8388608, 1), (1, 1))
	    assert_size_stride(primals_30, (262144, ), (1, ))
	    assert_size_stride(primals_31, (256, ), (1, ))
	    assert_size_stride(primals_32, (1024, ), (1, ))
	    assert_size_stride(primals_33, (), ())
	    assert_size_stride(primals_34, (16, ), (1, ))
	    assert_size_stride(primals_37, (32, 8192), (8192, 1))
	    assert_size_stride(primals_38, (2048, 32), (32, 1))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((), (), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [mylib.fused_dequantize_op]
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_fused_dequantize_op_0.run(primals_9, buf0, 1, grid=grid(1), stream=stream0)
	        del primals_9
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [mylib.fused_dequantize_op]
	        buf1 = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, buf0, reinterpret_tensor(primals_3, (1, 8388608), (1, 1), 0), primals_10, 16777216, s2, math.floor((1/2)*s3), s2*s3, math.floor((1/2)*s2*s3), 8192, 2048, torch.bfloat16)
	        del primals_10
	        del primals_3
	        del primals_6
	        del primals_7
	        del primals_8
	        buf2 = buf1
	        del buf1
	        buf3 = empty_strided_cuda((8192, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf2, buf3, 16777216, grid=grid(16777216), stream=stream0)
	        buf4 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), reinterpret_tensor(buf3, (2048, 8192), (1, 2048), 0), out=buf4)
	        buf5 = empty_strided_cuda((2048, 32), (1, 2048), torch.float16)
	        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_2.run(primals_13, buf5, 65536, grid=grid(65536), stream=stream0)
	        del primals_13
	        buf6 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), buf5, out=buf6)
	        buf7 = empty_strided_cuda((32, 8192), (1, 32), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_3.run(primals_14, buf7, 262144, grid=grid(262144), stream=stream0)
	        del primals_14
	        buf8 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten.mm]
	        extern_kernels.mm(buf6, buf7, out=buf8)
	        buf10 = buf0; del buf0  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [mylib.fused_dequantize_op]
	        triton_poi_fused_fused_dequantize_op_0.run(primals_21, buf10, 1, grid=grid(1), stream=stream0)
	        del primals_21
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [mylib.fused_dequantize_op]
	        buf11 = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, buf10, reinterpret_tensor(primals_15, (1, 8388608), (1, 1), 0), primals_22, 16777216, s6, math.floor((1/2)*s7), s6*s7, math.floor((1/2)*s6*s7), 8192, 2048, torch.bfloat16)
	        del primals_15
	        del primals_18
	        del primals_19
	        del primals_20
	        del primals_22
	        buf12 = buf11
	        del buf11
	        buf13 = buf3; del buf3  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf12, buf13, 16777216, grid=grid(16777216), stream=stream0)
	        buf14 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), reinterpret_tensor(buf13, (2048, 8192), (1, 2048), 0), out=buf14)
	        buf15 = empty_strided_cuda((2048, 32), (1, 2048), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_2], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_2.run(primals_25, buf15, 65536, grid=grid(65536), stream=stream0)
	        del primals_25
	        buf16 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_2], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), buf15, out=buf16)
	        buf17 = empty_strided_cuda((32, 8192), (1, 32), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_3], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_3.run(primals_26, buf17, 262144, grid=grid(262144), stream=stream0)
	        del primals_26
	        buf18 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_3], Original ATen: [aten.mm]
	        extern_kernels.mm(buf16, buf17, out=buf18)
	        buf9 = reinterpret_tensor(buf4, (1, s0, 8192), (8192*s0, 8192, 1), 0); del buf4  # reuse
	        buf19 = reinterpret_tensor(buf14, (1, s0, 8192), (8192*s0, 8192, 1), 0); del buf14  # reuse
	        buf23 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [output, result_2, silu, output_1, result_5, mul_2], Original ATen: [aten.mul, aten.add, aten.silu]
	        triton_poi_fused_add_mul_silu_4_xnumel = 8192*s0
	        triton_poi_fused_add_mul_silu_4.run(buf9, buf19, buf8, buf18, buf23, triton_poi_fused_add_mul_silu_4_xnumel, grid=grid(triton_poi_fused_add_mul_silu_4_xnumel), stream=stream0)
	        del buf18
	        del buf8
	        buf20 = buf10; del buf10  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [mylib.fused_dequantize_op]
	        triton_poi_fused_fused_dequantize_op_0.run(primals_33, buf20, 1, grid=grid(1), stream=stream0)
	        del primals_33
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [mylib.fused_dequantize_op]
	        buf21 = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, buf20, reinterpret_tensor(primals_27, (1, 8388608), (1, 1), 0), primals_34, 16777216, s10, math.floor((1/2)*s11), s10*s11, math.floor((1/2)*s10*s11), 2048, 8192, torch.bfloat16)
	        del buf20
	        del primals_27
	        del primals_30
	        del primals_31
	        del primals_32
	        del primals_34
	        buf22 = buf21
	        del buf21
	        buf24 = reinterpret_tensor(buf13, (2048, 8192), (8192, 1), 0); del buf13  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf22, buf24, 16777216, grid=grid(16777216), stream=stream0)
	        buf25 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf23, (s0, 8192), (8192, 1), 0), reinterpret_tensor(buf24, (8192, 2048), (1, 8192), 0), out=buf25)
	        buf26 = empty_strided_cuda((8192, 32), (1, 8192), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_3.run(primals_37, buf26, 262144, grid=grid(262144), stream=stream0)
	        del primals_37
	        buf27 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf23, (s0, 8192), (8192, 1), 0), buf26, out=buf27)
	        buf28 = empty_strided_cuda((32, 2048), (1, 32), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_5], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_2.run(primals_38, buf28, 65536, grid=grid(65536), stream=stream0)
	        del primals_38
	        buf29 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_5], Original ATen: [aten.mm]
	        extern_kernels.mm(buf27, buf28, out=buf29)
	        buf30 = reinterpret_tensor(buf25, (1, s0, 2048), (2048*s0, 2048, 1), 0); del buf25  # reuse
	        # Topologically Sorted Source Nodes: [output_2, result_8], Original ATen: [aten.mul, aten.add]
	        triton_poi_fused_add_mul_5_xnumel = 2048*s0
	        triton_poi_fused_add_mul_5.run(buf30, buf29, triton_poi_fused_add_mul_5_xnumel, grid=grid(triton_poi_fused_add_mul_5_xnumel), stream=stream0)
	        del buf29
	        buf31 = reinterpret_tensor(buf24, (8192, 2048), (2048, 1), 0); del buf24  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_6.run(buf22, buf31, 8192, 2048, grid=grid(8192, 2048), stream=stream0)
	        del buf22
	        buf32 = empty_strided_cuda((2048, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_7.run(buf31, buf32, 2048, 8192, grid=grid(2048, 8192), stream=stream0)
	        buf33 = reinterpret_tensor(buf31, (2048, 8192), (8192, 1), 0); del buf31  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_8.run(buf12, buf33, 2048, 8192, grid=grid(2048, 8192), stream=stream0)
	        del buf12
	        buf34 = empty_strided_cuda((8192, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_9.run(buf33, buf34, 8192, 2048, grid=grid(8192, 2048), stream=stream0)
	        buf35 = buf33; del buf33  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_8.run(buf2, buf35, 2048, 8192, grid=grid(2048, 8192), stream=stream0)
	        del buf2
	        buf36 = empty_strided_cuda((8192, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_9.run(buf35, buf36, 8192, 2048, grid=grid(8192, 2048), stream=stream0)
	        del buf35
	    return (buf30, reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), buf6, buf9, buf16, buf19, reinterpret_tensor(buf23, (s0, 8192), (8192, 1), 0), buf27, reinterpret_tensor(buf28, (2048, 32), (32, 1), 0), reinterpret_tensor(buf26, (32, 8192), (8192, 1), 0), buf32, reinterpret_tensor(buf17, (8192, 32), (32, 1), 0), reinterpret_tensor(buf15, (32, 2048), (2048, 1), 0), buf34, reinterpret_tensor(buf7, (8192, 32), (32, 1), 0), reinterpret_tensor(buf5, (32, 2048), (2048, 1), 0), buf36, s0, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    primals_1 = 100
	    primals_2 = rand_strided((1, 100, 2048), (204800, 2048, 1), device='cuda:0', dtype=torch.float16)
	    primals_3 = rand_strided((8388608, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    primals_4 = 256
	    primals_5 = 64
	    primals_6 = rand_strided((262144, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    primals_7 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_8 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_9 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    primals_10 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_11 = 2048
	    primals_12 = 8192
	    primals_13 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)
	    primals_14 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float32)
	    primals_15 = rand_strided((8388608, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    primals_16 = 256
	    primals_17 = 64
	    primals_18 = rand_strided((262144, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    primals_19 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_20 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_21 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    primals_22 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_23 = 2048
	    primals_24 = 8192
	    primals_25 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)
	    primals_26 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float32)
	    primals_27 = rand_strided((8388608, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    primals_28 = 256
	    primals_29 = 64
	    primals_30 = rand_strided((262144, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    primals_31 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_32 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_33 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    primals_34 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_35 = 8192
	    primals_36 = 2048
	    primals_37 = rand_strided((32, 8192), (8192, 1), device='cuda:0', dtype=torch.float32)
	    primals_38 = rand_strided((2048, 32), (32, 1), device='cuda:0', dtype=torch.float32)
	    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:52:06.202000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "d69a83f4ac763a742acf419ff240d5b2"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742993526202479.0,
	"args": {
	"key": "fibzwaj5j2pxd4mgs6jpctih67rusxg2lyckoxqaw3hr5wi42ks3",
	"components": [
	"[iu3kgzgo5x6udk6uhcxvnilg7lheh3rln2fhyjpg5qgapvm76o4] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38):\n    permute = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None\n    rshift = primals_5 >> 1\n    mul_4 = primals_4 * primals_5;  primals_5 = None\n    rshift_1 = mul_4 >> 1\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None\n    permute_1 = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None\n    permute_2 = torch.ops.aten.permute.default(permute_1, [1, 0])\n    convert_element_type_1 = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None\n    convert_element_type_default_5 = torch.ops.prims.convert_element_type.default(primals_2, torch.float16)\n    permute_3 = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None\n    view = torch.ops.aten.view.default(convert_element_type_default_5, [primals_1, 2048]);  convert_element_type_default_5 = None\n    mm = torch.ops.aten.mm.default(view, permute_3);  permute_3 = None\n    view_1 = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None\n    convert_element_type_5 = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None\n    permute_4 = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None\n    view_2 = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None\n    mm_1 = torch.ops.aten.mm.default(view_2, permute_4)\n    view_3 = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None\n    convert_element_type_8 = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None\n    permute_5 = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None\n    view_4 = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None\n    mm_2 = torch.ops.aten.mm.default(view_4, permute_5)\n    view_5 = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None\n    mul_39 = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None\n    add_39 = torch.ops.aten.add.Tensor(view_1, mul_39);  view_1 = mul_39 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    permute_6 = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None\n    rshift_2 = primals_17 >> 1\n    mul_54 = primals_16 * primals_17;  primals_17 = None\n    rshift_3 = mul_54 >> 1\n    fused_dequantize_op_1 = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None\n    permute_7 = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None\n    permute_8 = torch.ops.aten.permute.default(permute_7, [1, 0])\n    convert_element_type_14 = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None\n    permute_9 = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None\n    mm_3 = torch.ops.aten.mm.default(view, permute_9);  view = permute_9 = None\n    view_7 = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None\n    convert_element_type_18 = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None\n    permute_10 = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None\n    mm_4 = torch.ops.aten.mm.default(view_2, permute_10)\n    view_9 = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None\n    convert_element_type_21 = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None\n    permute_11 = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None\n    view_10 = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None\n    mm_5 = torch.ops.aten.mm.default(view_10, permute_11)\n    view_11 = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None\n    mul_87 = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None\n    add_85 = torch.ops.aten.add.Tensor(view_7, mul_87);  view_7 = mul_87 = None\n    mul_94 = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85);  convert_element_type_12 = None\n    permute_12 = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None\n    rshift_4 = primals_29 >> 1\n    mul_102 = primals_28 * primals_29;  primals_29 = None\n    rshift_5 = mul_102 >> 1\n    fused_dequantize_op_2 = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None\n    permute_13 = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None\n    permute_14 = torch.ops.aten.permute.default(permute_13, [1, 0])\n    convert_element_type_25 = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None\n    convert_element_type_default_3 = torch.ops.prims.convert_element_type.default(mul_94, torch.float16)\n    permute_15 = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None\n    view_12 = torch.ops.aten.view.default(convert_element_type_default_3, [primals_1, 8192]);  convert_element_type_default_3 = None\n    mm_6 = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None\n    view_13 = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None\n    convert_element_type_29 = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None\n    permute_16 = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None\n    view_14 = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None\n    mm_7 = torch.ops.aten.mm.default(view_14, permute_16)\n    view_15 = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None\n    convert_element_type_32 = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None\n    permute_17 = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None\n    view_16 = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None\n    mm_8 = torch.ops.aten.mm.default(view_16, permute_17)\n    view_17 = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None\n    mul_137 = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None\n    add_131 = torch.ops.aten.add.Tensor(view_13, mul_137);  view_13 = mul_137 = None\n    permute_20 = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None\n    permute_24 = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None\n    convert_element_type_45 = torch.ops.prims.convert_element_type.default(permute_13, torch.float16);  permute_13 = None\n    permute_27 = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None\n    permute_30 = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None\n    permute_34 = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None\n    convert_element_type_60 = torch.ops.prims.convert_element_type.default(permute_7, torch.float16);  permute_7 = None\n    permute_37 = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None\n    permute_41 = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None\n    permute_45 = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None\n    convert_element_type_75 = torch.ops.prims.convert_element_type.default(permute_1, torch.float16);  permute_1 = None\n    permute_48 = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None\n    return (add_131, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, primals_1)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[2]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[3]: ('s2',)",
	"[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[4]: ('s3',)",
	"[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[5]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[7]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[9]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[10]: ('2048',)",
	"[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[11]: ('8192',)",
	"[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[12]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[13]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[14]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[bp5m6l3n24hn6zbxwmg6wfbnn5g5tujtrpje3hczu5umrrf2qjx] example_inputs[15]: ('s6',)",
	"[i3xrsw5sisbji2ivhzqy3l7zuqzhmvjpsyt77nanlhezk7f6tur] example_inputs[16]: ('s7',)",
	"[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[17]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[18]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[19]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[20]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[21]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[22]: ('2048',)",
	"[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[23]: ('8192',)",
	"[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[24]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[25]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[26]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hj7nkho4e2pgg3rxjgt2xo2ofzqotfhatuhlt6bxcifwqbvipzw] example_inputs[27]: ('s10',)",
	"[kosoqok7aznz5iahzbmrdsrqfq3jkju4dvvd7cfij3xrvzxbmb4] example_inputs[28]: ('s11',)",
	"[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[29]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[30]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[31]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[32]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[33]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[34]: ('8192',)",
	"[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[35]: ('2048',)",
	"[zs5vjd7lduxixuzsgnizmtcmiocu75spefi5z3zyshqbfmva6ge] example_inputs[36]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[chru3fsd44dmnozqmsiynvahv26l3bk7j747je77m6edbjsd2kq] example_inputs[37]: TensorMetadata(dtype=torch.float32, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[34jl2fvzyx6odxtpy45nz66wulvmgs5bvsvvhx2zwehki6ym5kw] fx_kwargs[static_input_idxs]: [2, 5, 6, 7, 8, 9, 12, 13, 14, 17, 18, 19, 20, 21, 24, 25, 26, 29, 30, 31, 32, 33, 36, 37]",
	"[2ruvd6wcktb7xtp52gxtldu3ygrqbnwbiluwcx3yagimk7erlhx] fx_kwargs[user_visible_outputs]: {'add_131': None}",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 688660460,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:52:06.203000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "37a16f91963db61f5f565392fb5de95d"}
	{"key": "fibzwaj5j2pxd4mgs6jpctih67rusxg2lyckoxqaw3hr5wi42ks3", "components": ["[iu3kgzgo5x6udk6uhcxvnilg7lheh3rln2fhyjpg5qgapvm76o4] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38):\n    permute = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None\n    rshift = primals_5 >> 1\n    mul_4 = primals_4 * primals_5;  primals_5 = None\n    rshift_1 = mul_4 >> 1\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None\n    permute_1 = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None\n    permute_2 = torch.ops.aten.permute.default(permute_1, [1, 0])\n    convert_element_type_1 = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None\n    convert_element_type_default_5 = torch.ops.prims.convert_element_type.default(primals_2, torch.float16)\n    permute_3 = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None\n    view = torch.ops.aten.view.default(convert_element_type_default_5, [primals_1, 2048]);  convert_element_type_default_5 = None\n    mm = torch.ops.aten.mm.default(view, permute_3);  permute_3 = None\n    view_1 = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None\n    convert_element_type_5 = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None\n    permute_4 = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None\n    view_2 = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None\n    mm_1 = torch.ops.aten.mm.default(view_2, permute_4)\n    view_3 = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None\n    convert_element_type_8 = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None\n    permute_5 = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None\n    view_4 = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None\n    mm_2 = torch.ops.aten.mm.default(view_4, permute_5)\n    view_5 = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None\n    mul_39 = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None\n    add_39 = torch.ops.aten.add.Tensor(view_1, mul_39);  view_1 = mul_39 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    permute_6 = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None\n    rshift_2 = primals_17 >> 1\n    mul_54 = primals_16 * primals_17;  primals_17 = None\n    rshift_3 = mul_54 >> 1\n    fused_dequantize_op_1 = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None\n    permute_7 = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None\n    permute_8 = torch.ops.aten.permute.default(permute_7, [1, 0])\n    convert_element_type_14 = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None\n    permute_9 = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None\n    mm_3 = torch.ops.aten.mm.default(view, permute_9);  view = permute_9 = None\n    view_7 = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None\n    convert_element_type_18 = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None\n    permute_10 = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None\n    mm_4 = torch.ops.aten.mm.default(view_2, permute_10)\n    view_9 = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None\n    convert_element_type_21 = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None\n    permute_11 = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None\n    view_10 = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None\n    mm_5 = torch.ops.aten.mm.default(view_10, permute_11)\n    view_11 = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None\n    mul_87 = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None\n    add_85 = torch.ops.aten.add.Tensor(view_7, mul_87);  view_7 = mul_87 = None\n    mul_94 = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85);  convert_element_type_12 = None\n    permute_12 = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None\n    rshift_4 = primals_29 >> 1\n    mul_102 = primals_28 * primals_29;  primals_29 = None\n    rshift_5 = mul_102 >> 1\n    fused_dequantize_op_2 = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None\n    permute_13 = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None\n    permute_14 = torch.ops.aten.permute.default(permute_13, [1, 0])\n    convert_element_type_25 = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None\n    convert_element_type_default_3 = torch.ops.prims.convert_element_type.default(mul_94, torch.float16)\n    permute_15 = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None\n    view_12 = torch.ops.aten.view.default(convert_element_type_default_3, [primals_1, 8192]);  convert_element_type_default_3 = None\n    mm_6 = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None\n    view_13 = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None\n    convert_element_type_29 = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None\n    permute_16 = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None\n    view_14 = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None\n    mm_7 = torch.ops.aten.mm.default(view_14, permute_16)\n    view_15 = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None\n    convert_element_type_32 = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None\n    permute_17 = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None\n    view_16 = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None\n    mm_8 = torch.ops.aten.mm.default(view_16, permute_17)\n    view_17 = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None\n    mul_137 = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None\n    add_131 = torch.ops.aten.add.Tensor(view_13, mul_137);  view_13 = mul_137 = None\n    permute_20 = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None\n    permute_24 = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None\n    convert_element_type_45 = torch.ops.prims.convert_element_type.default(permute_13, torch.float16);  permute_13 = None\n    permute_27 = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None\n    permute_30 = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None\n    permute_34 = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None\n    convert_element_type_60 = torch.ops.prims.convert_element_type.default(permute_7, torch.float16);  permute_7 = None\n    permute_37 = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None\n    permute_41 = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None\n    permute_45 = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None\n    convert_element_type_75 = torch.ops.prims.convert_element_type.default(permute_1, torch.float16);  permute_1 = None\n    permute_48 = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None\n    return (add_131, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, primals_1)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[2]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[3]: ('s2',)", "[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[4]: ('s3',)", "[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[5]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[7]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[9]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[10]: ('2048',)", "[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[11]: ('8192',)", "[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[12]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[13]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[14]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[bp5m6l3n24hn6zbxwmg6wfbnn5g5tujtrpje3hczu5umrrf2qjx] example_inputs[15]: ('s6',)", "[i3xrsw5sisbji2ivhzqy3l7zuqzhmvjpsyt77nanlhezk7f6tur] example_inputs[16]: ('s7',)", "[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[17]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[18]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[19]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[20]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[21]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[22]: ('2048',)", "[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[23]: ('8192',)", "[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[24]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[25]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[26]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hj7nkho4e2pgg3rxjgt2xo2ofzqotfhatuhlt6bxcifwqbvipzw] example_inputs[27]: ('s10',)", "[kosoqok7aznz5iahzbmrdsrqfq3jkju4dvvd7cfij3xrvzxbmb4] example_inputs[28]: ('s11',)", "[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[29]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[30]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[31]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[32]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[33]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[34]: ('8192',)", "[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[35]: ('2048',)", "[zs5vjd7lduxixuzsgnizmtcmiocu75spefi5z3zyshqbfmva6ge] example_inputs[36]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[chru3fsd44dmnozqmsiynvahv26l3bk7j747je77m6edbjsd2kq] example_inputs[37]: TensorMetadata(dtype=torch.float32, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[34jl2fvzyx6odxtpy45nz66wulvmgs5bvsvvhx2zwehki6ym5kw] fx_kwargs[static_input_idxs]: [2, 5, 6, 7, 8, 9, 12, 13, 14, 17, 18, 19, 20, 21, 24, 25, 26, 29, 30, 31, 32, 33, 36, 37]", "[2ruvd6wcktb7xtp52gxtldu3ygrqbnwbiluwcx3yagimk7erlhx] fx_kwargs[user_visible_outputs]: {'add_131': None}", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 688660460, "cache_state": "hit"}
V0326 23:52:06.205000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "e23c1a68436d1543aed03655e7564276"}
	{
	"name": "inductor_compile",
	"ts": 1742993526205306.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 7,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.205000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "27bd8b9c08163c6a290a754453b94e11"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993526205657.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 7,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.206000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "dbc4a542fb7c2936bb14ca1fe5e39b37"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993526206159.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 7,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.207000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "eeff64fb6df66c3294e634e0204be440"}
	{
	"name": "compile_fx.<locals>.bw_compiler",
	"ts": 1742993526206912.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.207000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "f3effcb01d083a585369e632802e2924"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993526207347.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.207000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "5ba45f88200974bcc4ccda426285700f"}
	{
	"name": "inductor_compile",
	"ts": 1742993526207347.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.253000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/hi/chi6hm2i2r5fg3p735q2myqwx4uplr5ji3czydmtzsbsjw6fmtl2.py"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "ccae98e057b275846abfbc378763924c"}
	# AOT ID: ['5_backward']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/oz/cozopk2ehguq32bevie5q4mjctwrvknb2efn2zmifydb43kq7u2a.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten.mul, aten.view]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %mul_144 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%tangents_1, 2.0), kwargs = {})
	#   %view_18 : [num_users=2] = call_function[target=torch.ops.aten.reshape.default](args = (%mul_144, [%primals_1, 2048]), kwargs = {})
	triton_poi_fused_mul_view_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_view_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = 2.0
	    tmp2 = tmp0 * tmp1
	    tl.store(out_ptr0 + (x0), tmp2, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/6p/c6pwujwafd7jyea2qiskauahv6a3isypaxtecr7nxxt4nrphxtq6.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_39 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_21, torch.float32), kwargs = {})
	triton_poi_fused__to_copy_1 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[65536], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_1', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 65536
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/2o/c2odmo35jco5222u2qehkh6jj67bnvs7ccevylkmgiqf4prmli3t.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_44 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_25, torch.float32), kwargs = {})
	triton_poi_fused__to_copy_2 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_2', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 262144
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/ut/cutu6nbumwt37rt3w5ivi5f2exxnf3nqjnpdd3meykytx4gvjuio.py
	# Topologically Sorted Source Nodes: [silu], Original ATen: [aten.add, aten.silu, aten.mul, aten.sigmoid, aten.fill, aten.sub]
	# Source node to ATen node mapping:
	#   silu => convert_element_type_11, convert_element_type_12, mul_46, sigmoid
	# Graph fragment:
	#   %add_135 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_21, %view_23), kwargs = {})
	#   %convert_element_type_11 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_39, torch.float32), kwargs = {})
	#   %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_11,), kwargs = {})
	#   %mul_46 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_11, %sigmoid), kwargs = {})
	#   %convert_element_type_12 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_46, torch.float16), kwargs = {})
	#   %mul_146 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_135, %convert_element_type_12), kwargs = {})
	#   %mul_147 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_135, %add_85), kwargs = {})
	#   %mul_148 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_146, 2.0), kwargs = {})
	#   %sigmoid_1 : [num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%add_39,), kwargs = {})
	#   %full_default : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([1, %primals_1, 8192], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
	#   %sub_44 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default, %sigmoid_1), kwargs = {})
	#   %mul_150 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_39, %sub_44), kwargs = {})
	#   %add_137 : [num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_150, 1), kwargs = {})
	#   %mul_151 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_1, %add_137), kwargs = {})
	#   %mul_152 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_147, %mul_151), kwargs = {})
	#   %mul_153 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_152, 2.0), kwargs = {})
	triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1048576], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: '*fp16', 5: '*fp16', 6: '*fp16', 7: '*fp16', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, out_ptr2, out_ptr3, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr1 + (x0), None).to(tl.float32)
	    tmp3 = tl.load(in_ptr2 + (x0), None).to(tl.float32)
	    tmp11 = tl.load(in_ptr3 + (x0), None).to(tl.float32)
	    tmp2 = tmp0 + tmp1
	    tmp4 = tmp3.to(tl.float32)
	    tmp5 = tl.sigmoid(tmp4)
	    tmp6 = tmp4 * tmp5
	    tmp7 = tmp6.to(tl.float32)
	    tmp8 = tmp2 * tmp7
	    tmp9 = 2.0
	    tmp10 = tmp8 * tmp9
	    tmp12 = tmp2 * tmp11
	    tmp13 = tl.sigmoid(tmp3)
	    tmp14 = 1.0
	    tmp15 = tmp14 - tmp13
	    tmp16 = tmp3 * tmp15
	    tmp17 = tmp16 + tmp14
	    tmp18 = tmp13 * tmp17
	    tmp19 = tmp12 * tmp18
	    tmp20 = tmp19 * tmp9
	    tl.store(out_ptr0 + (x0), tmp10, None)
	    tl.store(out_ptr1 + (x0), tmp8, None)
	    tl.store(out_ptr2 + (x0), tmp20, None)
	    tl.store(out_ptr3 + (x0), tmp19, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/vd/cvdgoxiwnbzp7azzk6zja6cs44uw5kuroxt6qq5umvp5rqdwji7r.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten.add]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %add_136 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_27, %view_29), kwargs = {})
	#   %add_138 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_136, %view_33), kwargs = {})
	#   %add_139 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_138, %view_35), kwargs = {})
	triton_poi_fused_add_4 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_4', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_out_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp3 = tl.load(in_ptr1 + (x0), None).to(tl.float32)
	    tmp5 = tl.load(in_ptr2 + (x0), None).to(tl.float32)
	    tmp2 = tmp0 + tmp1
	    tmp4 = tmp2 + tmp3
	    tmp6 = tmp4 + tmp5
	    tl.store(in_out_ptr0 + (x0), tmp6, None)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1 = args
	    args.clear()
	    s0 = primals_1
	    assert_size_stride(view_2, (s0, 2048), (2048, 1))
	    assert_size_stride(view_4, (s0, 32), (32, 1))
	    assert_size_stride(add_39, (1, s0, 8192), (8192*s0, 8192, 1))
	    assert_size_stride(view_10, (s0, 32), (32, 1))
	    assert_size_stride(add_85, (1, s0, 8192), (8192*s0, 8192, 1))
	    assert_size_stride(view_14, (s0, 8192), (8192, 1))
	    assert_size_stride(view_16, (s0, 32), (32, 1))
	    assert_size_stride(permute_20, (2048, 32), (32, 1))
	    assert_size_stride(permute_24, (32, 8192), (8192, 1))
	    assert_size_stride(permute_27, (2048, 8192), (8192, 1))
	    assert_size_stride(permute_30, (8192, 32), (32, 1))
	    assert_size_stride(permute_34, (32, 2048), (2048, 1))
	    assert_size_stride(permute_37, (8192, 2048), (2048, 1))
	    assert_size_stride(permute_41, (8192, 32), (32, 1))
	    assert_size_stride(permute_45, (32, 2048), (2048, 1))
	    assert_size_stride(permute_48, (8192, 2048), (2048, 1))
	    assert_size_stride(tangents_1, (1, s0, 2048), (2048*s0, 2048, 1))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mul, aten.view]
	        triton_poi_fused_mul_view_0_xnumel = 2048*s0
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_mul_view_0.run(tangents_1, buf0, triton_poi_fused_mul_view_0_xnumel, grid=grid(triton_poi_fused_mul_view_0_xnumel), stream=stream0)
	        buf1 = empty_strided_cuda((2048, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf0, (2048, s0), (1, 2048), 0), view_16, out=buf1)
	        del view_16
	        buf2 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf0, permute_20, out=buf2)
	        del permute_20
	        buf3 = empty_strided_cuda((2048, 32), (32, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf1, buf3, 65536, grid=grid(65536), stream=stream0)
	        buf4 = empty_strided_cuda((32, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf2, (32, s0), (1, 32), 0), view_14, out=buf4)
	        del view_14
	        buf5 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf2, permute_24, out=buf5)
	        del permute_24
	        buf6 = empty_strided_cuda((32, 8192), (8192, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_2.run(buf4, buf6, 262144, grid=grid(262144), stream=stream0)
	        buf7 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(tangents_1, (s0, 2048), (2048, 1), 0), permute_27, out=buf7)
	        del permute_27
	        del tangents_1
	        buf8 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        buf15 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        buf17 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        buf24 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [silu], Original ATen: [aten.add, aten.silu, aten.mul, aten.sigmoid, aten.fill, aten.sub]
	        triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3_xnumel = 8192*s0
	        triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3.run(buf5, buf7, add_39, add_85, buf8, buf15, buf17, buf24, triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3_xnumel, grid=grid(triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3_xnumel), stream=stream0)
	        del add_39
	        del add_85
	        del buf5
	        del buf7
	        buf9 = reinterpret_tensor(buf4, (8192, 32), (32, 1), 0); del buf4  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf8, (8192, s0), (1, 8192), 0), view_10, out=buf9)
	        del view_10
	        buf10 = buf2; del buf2  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf8, (s0, 8192), (8192, 1), 0), permute_30, out=buf10)
	        del buf8
	        del permute_30
	        buf11 = empty_strided_cuda((8192, 32), (32, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_2.run(buf9, buf11, 262144, grid=grid(262144), stream=stream0)
	        buf12 = reinterpret_tensor(buf1, (32, 2048), (2048, 1), 0); del buf1  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf10, (32, s0), (1, 32), 0), view_2, out=buf12)
	        buf13 = buf0; del buf0  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf10, permute_34, out=buf13)
	        del permute_34
	        buf14 = empty_strided_cuda((32, 2048), (2048, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf12, buf14, 65536, grid=grid(65536), stream=stream0)
	        buf16 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf15, (s0, 8192), (8192, 1), 0), permute_37, out=buf16)
	        del buf15
	        del permute_37
	        buf18 = buf9; del buf9  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf17, (8192, s0), (1, 8192), 0), view_4, out=buf18)
	        del view_4
	        buf19 = buf10; del buf10  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf17, (s0, 8192), (8192, 1), 0), permute_41, out=buf19)
	        del buf17
	        del permute_41
	        buf20 = empty_strided_cuda((8192, 32), (32, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_2.run(buf18, buf20, 262144, grid=grid(262144), stream=stream0)
	        del buf18
	        buf21 = buf12; del buf12  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf19, (32, s0), (1, 32), 0), view_2, out=buf21)
	        del view_2
	        buf22 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf19, permute_45, out=buf22)
	        del buf19
	        del permute_45
	        buf23 = empty_strided_cuda((32, 2048), (2048, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf21, buf23, 65536, grid=grid(65536), stream=stream0)
	        del buf21
	        buf25 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf24, (s0, 8192), (8192, 1), 0), permute_48, out=buf25)
	        del buf24
	        del permute_48
	        buf26 = reinterpret_tensor(buf13, (1, s0, 2048), (2048*s0, 2048, 1), 0); del buf13  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.add]
	        triton_poi_fused_add_4_xnumel = 2048*s0
	        triton_poi_fused_add_4.run(buf26, buf16, buf22, buf25, triton_poi_fused_add_4_xnumel, grid=grid(triton_poi_fused_add_4_xnumel), stream=stream0)
	        del buf16
	        del buf22
	        del buf25
	    return (None, buf26, None, None, None, None, None, None, None, None, None, None, buf23, buf20, None, None, None, None, None, None, None, None, None, None, buf14, buf11, None, None, None, None, None, None, None, None, None, None, buf6, buf3, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    primals_1 = 100
	    view_2 = rand_strided((100, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    view_4 = rand_strided((100, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    add_39 = rand_strided((1, 100, 8192), (819200, 8192, 1), device='cuda:0', dtype=torch.float16)
	    view_10 = rand_strided((100, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    add_85 = rand_strided((1, 100, 8192), (819200, 8192, 1), device='cuda:0', dtype=torch.float16)
	    view_14 = rand_strided((100, 8192), (8192, 1), device='cuda:0', dtype=torch.float16)
	    view_16 = rand_strided((100, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_20 = rand_strided((2048, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_24 = rand_strided((32, 8192), (8192, 1), device='cuda:0', dtype=torch.float16)
	    permute_27 = rand_strided((2048, 8192), (8192, 1), device='cuda:0', dtype=torch.float16)
	    permute_30 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_34 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    permute_37 = rand_strided((8192, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    permute_41 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_45 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    permute_48 = rand_strided((8192, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    tangents_1 = rand_strided((1, 100, 2048), (204800, 2048, 1), device='cuda:0', dtype=torch.float16)
	    fn = lambda: call([primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:52:06.254000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "dca2ed38633e8db43dd6cfa39d0d710d"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742993526254162.0,
	"args": {
	"key": "fuwpd53kuljrtdqmdkesfbb743gwbdes3cogyfbosttj4mjzbydy",
	"components": [
	"[cjhczcrgi3pzc6cive3k3kqwasc5syujhgv54rquueioi4qb6w2] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1):\n    mul_144 = torch.ops.aten.mul.Tensor(tangents_1, 2.0)\n    view_18 = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None\n    permute_18 = torch.ops.aten.permute.default(view_18, [1, 0])\n    mm_9 = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None\n    permute_19 = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None\n    mm_10 = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None\n    view_19 = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None\n    permute_21 = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None\n    convert_element_type_39 = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None\n    view_20 = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None\n    permute_22 = torch.ops.aten.permute.default(view_20, [1, 0])\n    mm_11 = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None\n    permute_23 = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None\n    mm_12 = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None\n    view_21 = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None\n    permute_25 = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None\n    convert_element_type_44 = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None\n    view_22 = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None\n    mm_13 = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None\n    view_23 = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None\n    convert_element_type_default_2 = torch.ops.prims.convert_element_type.default(view_23, torch.float16);  view_23 = None\n    add_135 = torch.ops.aten.add.Tensor(view_21, convert_element_type_default_2);  view_21 = convert_element_type_default_2 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    mul_146 = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None\n    mul_147 = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None\n    mul_148 = torch.ops.aten.mul.Tensor(mul_146, 2.0)\n    view_24 = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None\n    permute_28 = torch.ops.aten.permute.default(view_24, [1, 0])\n    mm_14 = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None\n    permute_29 = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None\n    mm_15 = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None\n    view_25 = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None\n    permute_31 = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None\n    convert_element_type_54 = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None\n    view_26 = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None\n    permute_32 = torch.ops.aten.permute.default(view_26, [1, 0])\n    mm_16 = torch.ops.aten.mm.default(permute_32, view_2);  permute_32 = None\n    permute_33 = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None\n    mm_17 = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None\n    view_27 = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None\n    permute_35 = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None\n    convert_element_type_59 = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None\n    view_28 = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None\n    mm_18 = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None\n    view_29 = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None\n    convert_element_type_default_1 = torch.ops.prims.convert_element_type.default(view_29, torch.float16);  view_29 = None\n    add_136 = torch.ops.aten.add.Tensor(view_27, convert_element_type_default_1);  view_27 = convert_element_type_default_1 = None\n    sigmoid_1 = torch.ops.aten.sigmoid.default(add_39)\n    full_default = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)\n    sub_44 = torch.ops.aten.sub.Tensor(full_default, sigmoid_1);  full_default = None\n    mul_150 = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None\n    add_137 = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None\n    mul_151 = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None\n    mul_152 = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None\n    mul_153 = torch.ops.aten.mul.Tensor(mul_152, 2.0)\n    view_30 = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None\n    permute_39 = torch.ops.aten.permute.default(view_30, [1, 0])\n    mm_19 = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None\n    permute_40 = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None\n    mm_20 = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None\n    view_31 = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None\n    permute_42 = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None\n    convert_element_type_69 = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None\n    view_32 = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None\n    permute_43 = torch.ops.aten.permute.default(view_32, [1, 0])\n    mm_21 = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None\n    permute_44 = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None\n    mm_22 = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None\n    view_33 = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None\n    add_138 = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None\n    permute_46 = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None\n    convert_element_type_74 = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None\n    view_34 = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None\n    mm_23 = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None\n    view_35 = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None\n    convert_element_type_default = torch.ops.prims.convert_element_type.default(view_35, torch.float16);  view_35 = None\n    add_139 = torch.ops.aten.add.Tensor(add_138, convert_element_type_default);  add_138 = convert_element_type_default = None\n    return (None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[6pt52nx6wf2o3fnlscvbb6mpkzglnsxgtp26liujc4wx2tvtpm7] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4096*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[3]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[5]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[4ofuwkiqpjt5dr73pqxsydaoru4cj2w3m4sn674mztswxzkzyv5] example_inputs[6]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[kjiu5nyliqwsnl7icaeauzcjyk3boczmm4vcgqglpeyga4ezc26] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[buqdocmqej3phjqjaio73y24jqcnui6yzgcbitztcq67svbntcd] example_inputs[9]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hxsqnh7shhh7eup67u4ogv42dqcuywp53tt5tp7sdfgeqxuk32j] example_inputs[10]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[11]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[12]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[13]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[14]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[15]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[16]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[17]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_backward]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[n2y367w3cvigrpqrkfggnd4ki5vge46t7f3bihnrmz2hx2qkg6o] fx_kwargs[static_input_idxs]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]",
	"[giofkincgu73hppmew27gpl3zgvnfyyigbbqjyi5t7e6a5ldenw] fx_kwargs[user_visible_outputs]: {'add_139': None, 'convert_element_type_74': None, 'convert_element_type_69': None, 'convert_element_type_59': None, 'convert_element_type_54': None, 'convert_element_type_44': None, 'convert_element_type_39': None}",
	"[vhi4lnshnjmzxgwsuiowu552sqjv64oariyhqqha4hh3x7qlxkw] inputs_to_check[0]: 17",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 532666105,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:52:06.254000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "db354f4ac35fe5c12607d9e5788c27e9"}
	{"key": "fuwpd53kuljrtdqmdkesfbb743gwbdes3cogyfbosttj4mjzbydy", "components": ["[cjhczcrgi3pzc6cive3k3kqwasc5syujhgv54rquueioi4qb6w2] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1):\n    mul_144 = torch.ops.aten.mul.Tensor(tangents_1, 2.0)\n    view_18 = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None\n    permute_18 = torch.ops.aten.permute.default(view_18, [1, 0])\n    mm_9 = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None\n    permute_19 = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None\n    mm_10 = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None\n    view_19 = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None\n    permute_21 = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None\n    convert_element_type_39 = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None\n    view_20 = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None\n    permute_22 = torch.ops.aten.permute.default(view_20, [1, 0])\n    mm_11 = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None\n    permute_23 = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None\n    mm_12 = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None\n    view_21 = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None\n    permute_25 = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None\n    convert_element_type_44 = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None\n    view_22 = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None\n    mm_13 = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None\n    view_23 = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None\n    convert_element_type_default_2 = torch.ops.prims.convert_element_type.default(view_23, torch.float16);  view_23 = None\n    add_135 = torch.ops.aten.add.Tensor(view_21, convert_element_type_default_2);  view_21 = convert_element_type_default_2 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    mul_146 = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None\n    mul_147 = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None\n    mul_148 = torch.ops.aten.mul.Tensor(mul_146, 2.0)\n    view_24 = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None\n    permute_28 = torch.ops.aten.permute.default(view_24, [1, 0])\n    mm_14 = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None\n    permute_29 = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None\n    mm_15 = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None\n    view_25 = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None\n    permute_31 = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None\n    convert_element_type_54 = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None\n    view_26 = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None\n    permute_32 = torch.ops.aten.permute.default(view_26, [1, 0])\n    mm_16 = torch.ops.aten.mm.default(permute_32, view_2);  permute_32 = None\n    permute_33 = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None\n    mm_17 = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None\n    view_27 = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None\n    permute_35 = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None\n    convert_element_type_59 = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None\n    view_28 = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None\n    mm_18 = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None\n    view_29 = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None\n    convert_element_type_default_1 = torch.ops.prims.convert_element_type.default(view_29, torch.float16);  view_29 = None\n    add_136 = torch.ops.aten.add.Tensor(view_27, convert_element_type_default_1);  view_27 = convert_element_type_default_1 = None\n    sigmoid_1 = torch.ops.aten.sigmoid.default(add_39)\n    full_default = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)\n    sub_44 = torch.ops.aten.sub.Tensor(full_default, sigmoid_1);  full_default = None\n    mul_150 = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None\n    add_137 = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None\n    mul_151 = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None\n    mul_152 = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None\n    mul_153 = torch.ops.aten.mul.Tensor(mul_152, 2.0)\n    view_30 = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None\n    permute_39 = torch.ops.aten.permute.default(view_30, [1, 0])\n    mm_19 = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None\n    permute_40 = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None\n    mm_20 = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None\n    view_31 = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None\n    permute_42 = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None\n    convert_element_type_69 = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None\n    view_32 = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None\n    permute_43 = torch.ops.aten.permute.default(view_32, [1, 0])\n    mm_21 = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None\n    permute_44 = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None\n    mm_22 = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None\n    view_33 = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None\n    add_138 = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None\n    permute_46 = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None\n    convert_element_type_74 = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None\n    view_34 = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None\n    mm_23 = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None\n    view_35 = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None\n    convert_element_type_default = torch.ops.prims.convert_element_type.default(view_35, torch.float16);  view_35 = None\n    add_139 = torch.ops.aten.add.Tensor(add_138, convert_element_type_default);  add_138 = convert_element_type_default = None\n    return (None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[6pt52nx6wf2o3fnlscvbb6mpkzglnsxgtp26liujc4wx2tvtpm7] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4096*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[3]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[5]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[4ofuwkiqpjt5dr73pqxsydaoru4cj2w3m4sn674mztswxzkzyv5] example_inputs[6]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[kjiu5nyliqwsnl7icaeauzcjyk3boczmm4vcgqglpeyga4ezc26] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[buqdocmqej3phjqjaio73y24jqcnui6yzgcbitztcq67svbntcd] example_inputs[9]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hxsqnh7shhh7eup67u4ogv42dqcuywp53tt5tp7sdfgeqxuk32j] example_inputs[10]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[11]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[12]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[13]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[14]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[15]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[16]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[17]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_backward]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[n2y367w3cvigrpqrkfggnd4ki5vge46t7f3bihnrmz2hx2qkg6o] fx_kwargs[static_input_idxs]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]", "[giofkincgu73hppmew27gpl3zgvnfyyigbbqjyi5t7e6a5ldenw] fx_kwargs[user_visible_outputs]: {'add_139': None, 'convert_element_type_74': None, 'convert_element_type_69': None, 'convert_element_type_59': None, 'convert_element_type_54': None, 'convert_element_type_44': None, 'convert_element_type_39': None}", "[vhi4lnshnjmzxgwsuiowu552sqjv64oariyhqqha4hh3x7qlxkw] inputs_to_check[0]: 17", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 532666105, "cache_state": "hit"}
V0326 23:52:06.255000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "e857b16ef606b90359c332bec5c0280d"}
	{
	"name": "inductor_compile",
	"ts": 1742993526255846.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 8,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.256000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "444d8de651b56d053297273dead903c6"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993526256250.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 8,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.256000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"bwd_compilation_metrics": {"compile_id": "3/0", "inductor_compile_time_s": 0.04840707778930664, "code_gen_time_s": null, "fail_type": null, "fail_reason": null}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:06.256000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "5c24ba8079a1e74d3bbf0da833b77685"}
	{
	"name": "compile_fx.<locals>.bw_compiler",
	"ts": 1742993526256916.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 8,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.258000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "b14cbcd20a51ecd285135510d6906511"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993526258669.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 8,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.259000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "de7501defd7e17d4a59fd3ba06d7eec4"}
	{
	"name": "backend_compile",
	"ts": 1742993526259129.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 8,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.259000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "e20038e3fce20d6429520b47916644c7"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993526259382.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 8,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.303000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "90a7d0fe1b653fa056052890d9d64200"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['x'], accessed_by=DictGetItemGuardAccessor(x)
	| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=True, size=[1, None, 2048], stride=[None, 2048, 1])
	| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False         
	| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['x'], L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight'], L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight'], L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight'], L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight'], L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight'], L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight'])
	| +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)
	| | +- TYPE_MATCH: ___check_type_id(L['self'], 102513794958288)                
	| | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | +- DICT_LENGTH: len(L['self']._modules) == 4                                
	| | | | +- GuardManager: source=L['self']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor(gate_proj)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj'], 102513828834816)
	| | | | | +- GuardManager: source=L['self']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].scaling, accessed_by=DictGetItemGuardAccessor(scaling)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj'].scaling) == 1           
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].scaling['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj'].scaling['default'] == 2.0   
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules) == 7          
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'], accessed_by=DictGetItemGuardAccessor(base_layer)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer'], 102513768941808)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['base_layer'].__dict__)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._buffers, accessed_by=DictGetItemGuardAccessor(_buffers)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._modules['base_layer']._buffers
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._modules['base_layer']._modules
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['base_layer']._parameters) == 2
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'], 102513768940032)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[8388608, 1], stride=[1, 1])
	| | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state, accessed_by=GetAttrGuardAccessor(quant_state)
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state, 102513767659184)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype == torch.bfloat16
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, 140717689517824)
	| | | | | | | | | | | | | +- LENGTH_CHECK: len(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], 140718482538560)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], 140718482538560)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[262144], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, 102513767659184)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[1024], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, 140718482538560)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, 140718482538560)
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['bias'], 140718482527040)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'].compute_dtype, accessed_by=DictGetItemGuardAccessor(compute_dtype)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj']._modules['base_layer'].compute_dtype == torch.bfloat16
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._modules['base_layer']._backward_hooks
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._modules['base_layer']._backward_pre_hooks
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'].compute_type_is_set, accessed_by=DictGetItemGuardAccessor(compute_type_is_set)
	| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['base_layer'].compute_type_is_set, 140718482561760)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout'], accessed_by=DictGetItemGuardAccessor(lora_dropout)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_dropout'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_dropout']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'], 102513751199760)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'].__dict__)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A'], accessed_by=DictGetItemGuardAccessor(lora_A)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_A'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DictGuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- KeyValueManager pair at index=0
	| | | | | | | | | | | +- KeyManager: GuardManager: source=list(L['self']._modules['gate_proj']._modules['lora_A']._modules.keys())[0]
	| | | | | | | | | | | | +- EQUALS_MATCH: list(L['self']._modules['gate_proj']._modules['lora_A']._modules.keys())[0] == 'default'
	| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_A']._modules['default'], 102513750850736)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['lora_A']._modules['default'].__dict__)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[32, 2048], stride=[2048, 1])
	| | | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['bias'], 140718482527040)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B'], accessed_by=DictGetItemGuardAccessor(lora_B)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_B'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_B']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_B']._modules['default'], 102513750850736)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['lora_B']._modules['default'].__dict__)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[8192, 32], stride=[32, 1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['bias'], 140718482527040)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_embedding_A'], accessed_by=DictGetItemGuardAccessor(lora_embedding_A)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_embedding_B'], accessed_by=DictGetItemGuardAccessor(lora_embedding_B)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_magnitude_vector'], accessed_by=DictGetItemGuardAccessor(lora_magnitude_vector)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].use_dora, accessed_by=DictGetItemGuardAccessor(use_dora)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj'].use_dora) == 1          
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].use_dora['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj'].use_dora['default'], 140718482561760)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._parameters             
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._active_adapter, accessed_by=DictGetItemGuardAccessor(_active_adapter)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._active_adapter, 140718482539552)
	| | | | | | | +- LENGTH_CHECK: len(L['self']._modules['gate_proj']._active_adapter) == 1   
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['up_proj']._active_adapter
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['down_proj']._active_adapter
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._active_adapter[0], accessed_by=ListGetItemGuardAccessor(0)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj']._active_adapter[0] == 'default'
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._backward_hooks         
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].merged_adapters, accessed_by=DictGetItemGuardAccessor(merged_adapters)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj'].merged_adapters, 140718482539552)
	| | | | | | | +- LENGTH_CHECK: not L['self']._modules['gate_proj'].merged_adapters         
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._disable_adapters, accessed_by=DictGetItemGuardAccessor(_disable_adapters)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._disable_adapters, 140718482561760)
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._backward_pre_hooks     
	| | | | +- GuardManager: source=L['self']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor(up_proj)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj'], 102513828834816)
	| | | | | +- GuardManager: source=L['self']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj'].scaling, accessed_by=DictGetItemGuardAccessor(scaling)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj'].scaling) == 1             
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj'].scaling['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['up_proj'].scaling['default'] == 2.0     
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules) == 7            
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'], accessed_by=DictGetItemGuardAccessor(base_layer)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer'], 102513768941808)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['base_layer'].__dict__)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._buffers, accessed_by=DictGetItemGuardAccessor(_buffers)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._modules['base_layer']._buffers
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._modules['base_layer']._modules
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['base_layer']._parameters) == 2
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'], 102513768940032)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[8388608, 1], stride=[1, 1])
	| | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state, accessed_by=GetAttrGuardAccessor(quant_state)
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state, 102513767659184)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype == torch.bfloat16
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, 140717689517824)
	| | | | | | | | | | | | | +- LENGTH_CHECK: len(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], 140718482538560)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], 140718482538560)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[262144], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, 102513767659184)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[1024], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, 140718482538560)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, 140718482538560)
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['bias'], 140718482527040)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'].compute_dtype, accessed_by=DictGetItemGuardAccessor(compute_dtype)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['up_proj']._modules['base_layer'].compute_dtype == torch.bfloat16
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._modules['base_layer']._backward_hooks
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._modules['base_layer']._backward_pre_hooks
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'].compute_type_is_set, accessed_by=DictGetItemGuardAccessor(compute_type_is_set)
	| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['base_layer'].compute_type_is_set, 140718482561760)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout'], accessed_by=DictGetItemGuardAccessor(lora_dropout)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_dropout'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_dropout']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'], 102513751199760)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'].__dict__)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A'], accessed_by=DictGetItemGuardAccessor(lora_A)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_A'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DictGuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- KeyValueManager pair at index=0
	| | | | | | | | | | | +- KeyManager: GuardManager: source=list(L['self']._modules['up_proj']._modules['lora_A']._modules.keys())[0]
	| | | | | | | | | | | | +- EQUALS_MATCH: list(L['self']._modules['up_proj']._modules['lora_A']._modules.keys())[0] == 'default'
	| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_A']._modules['default'], 102513750850736)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['lora_A']._modules['default'].__dict__)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[32, 2048], stride=[2048, 1])
	| | | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['bias'], 140718482527040)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B'], accessed_by=DictGetItemGuardAccessor(lora_B)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_B'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_B']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_B']._modules['default'], 102513750850736)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['lora_B']._modules['default'].__dict__)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[8192, 32], stride=[32, 1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['bias'], 140718482527040)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_embedding_A'], accessed_by=DictGetItemGuardAccessor(lora_embedding_A)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_embedding_B'], accessed_by=DictGetItemGuardAccessor(lora_embedding_B)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_magnitude_vector'], accessed_by=DictGetItemGuardAccessor(lora_magnitude_vector)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj'].use_dora, accessed_by=DictGetItemGuardAccessor(use_dora)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj'].use_dora) == 1            
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj'].use_dora['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj'].use_dora['default'], 140718482561760)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._parameters               
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._backward_hooks           
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj'].merged_adapters, accessed_by=DictGetItemGuardAccessor(merged_adapters)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj'].merged_adapters, 140718482539552)
	| | | | | | | +- LENGTH_CHECK: not L['self']._modules['up_proj'].merged_adapters           
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._disable_adapters, accessed_by=DictGetItemGuardAccessor(_disable_adapters)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._disable_adapters, 140718482561760)
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._backward_pre_hooks       
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._active_adapter, accessed_by=DictGetItemGuardAccessor(_active_adapter)
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['up_proj']._active_adapter
	| | | | +- GuardManager: source=L['self']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor(down_proj)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj'], 102513828834816)
	| | | | | +- GuardManager: source=L['self']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj'].scaling, accessed_by=DictGetItemGuardAccessor(scaling)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj'].scaling) == 1           
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj'].scaling['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['down_proj'].scaling['default'] == 2.0   
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules) == 7          
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'], accessed_by=DictGetItemGuardAccessor(base_layer)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer'], 102513768941808)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['base_layer'].__dict__)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._buffers, accessed_by=DictGetItemGuardAccessor(_buffers)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._modules['base_layer']._buffers
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._modules['base_layer']._modules
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['base_layer']._parameters) == 2
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'], 102513768940032)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[8388608, 1], stride=[1, 1])
	| | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state, accessed_by=GetAttrGuardAccessor(quant_state)
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state, 102513767659184)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype == torch.bfloat16
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, 140717689517824)
	| | | | | | | | | | | | | +- LENGTH_CHECK: len(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], 140718482538560)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], 140718482538560)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[262144], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, 102513767659184)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[1024], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, 140718482538560)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, 140718482538560)
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['bias'], 140718482527040)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'].compute_dtype, accessed_by=DictGetItemGuardAccessor(compute_dtype)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['down_proj']._modules['base_layer'].compute_dtype == torch.bfloat16
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._modules['base_layer']._backward_hooks
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._modules['base_layer']._backward_pre_hooks
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'].compute_type_is_set, accessed_by=DictGetItemGuardAccessor(compute_type_is_set)
	| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['base_layer'].compute_type_is_set, 140718482561760)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout'], accessed_by=DictGetItemGuardAccessor(lora_dropout)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_dropout'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_dropout']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'], 102513751199760)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'].__dict__)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A'], accessed_by=DictGetItemGuardAccessor(lora_A)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_A'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DictGuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- KeyValueManager pair at index=0
	| | | | | | | | | | | +- KeyManager: GuardManager: source=list(L['self']._modules['down_proj']._modules['lora_A']._modules.keys())[0]
	| | | | | | | | | | | | +- EQUALS_MATCH: list(L['self']._modules['down_proj']._modules['lora_A']._modules.keys())[0] == 'default'
	| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_A']._modules['default'], 102513750850736)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['lora_A']._modules['default'].__dict__)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[32, 8192], stride=[8192, 1])
	| | | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['bias'], 140718482527040)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B'], accessed_by=DictGetItemGuardAccessor(lora_B)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_B'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_B']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_B']._modules['default'], 102513750850736)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['lora_B']._modules['default'].__dict__)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[2048, 32], stride=[32, 1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['bias'], 140718482527040)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_embedding_A'], accessed_by=DictGetItemGuardAccessor(lora_embedding_A)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_embedding_B'], accessed_by=DictGetItemGuardAccessor(lora_embedding_B)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_magnitude_vector'], accessed_by=DictGetItemGuardAccessor(lora_magnitude_vector)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj'].use_dora, accessed_by=DictGetItemGuardAccessor(use_dora)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj'].use_dora) == 1          
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj'].use_dora['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj'].use_dora['default'], 140718482561760)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._parameters             
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._backward_hooks         
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj'].merged_adapters, accessed_by=DictGetItemGuardAccessor(merged_adapters)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj'].merged_adapters, 140718482539552)
	| | | | | | | +- LENGTH_CHECK: not L['self']._modules['down_proj'].merged_adapters         
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._disable_adapters, accessed_by=DictGetItemGuardAccessor(_disable_adapters)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._disable_adapters, 140718482561760)
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._backward_pre_hooks     
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._active_adapter, accessed_by=DictGetItemGuardAccessor(_active_adapter)
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['down_proj']._active_adapter
	| | | | +- GuardManager: source=L['self']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor(act_fn)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['act_fn'], 102513751241264)
	| | | | | +- GuardManager: source=L['self']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['act_fn'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor(inplace)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['act_fn'].inplace, 140718482561760)
	| | | +- GuardManager: source=L['self']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | +- DICT_LENGTH: not L['self']._parameters                                   
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['F'], accessed_by=DictGetItemGuardAccessor(F)
	| | | +- ID_MATCH: ___check_obj_id(G['F'], 140713154810064)                    
	| | | +- GuardManager: source=G['F'].dequantize_4bit, accessed_by=GetAttrGuardAccessor(dequantize_4bit)
	| | | | +- GuardManager: source=G['F'].dequantize_4bit.__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | | +- ID_MATCH: ___check_obj_id(G['F'].dequantize_4bit.__code__, 140715525934704)
	| | | | +- GuardManager: source=G['F'].dequantize_4bit, accessed_by=FuncDefaultsGuardAccessor
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[1], accessed_by=GetItemGuardAccessor(1)
	| | | | | | +- ID_MATCH: ___check_obj_id(G['F'].dequantize_4bit.__defaults__[1], 140718482527040)
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[2], accessed_by=GetItemGuardAccessor(2)
	| | | | | | +- ID_MATCH: ___check_obj_id(G['F'].dequantize_4bit.__defaults__[2], 140718482527040)
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[3], accessed_by=GetItemGuardAccessor(3)
	| | | | | | +- EQUALS_MATCH: G['F'].dequantize_4bit.__defaults__[3] == 64                
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[4], accessed_by=GetItemGuardAccessor(4)
	| | | | | | +- EQUALS_MATCH: G['F'].dequantize_4bit.__defaults__[4] == 'fp4'             
	| | +- GuardManager: source=G['prod'], accessed_by=DictGetItemGuardAccessor(prod)
	| | | +- ID_MATCH: ___check_obj_id(G['prod'], 140718462147760)                 
	| | +- GuardManager: source=G['Params4bit'], accessed_by=DictGetItemGuardAccessor(Params4bit)
	| | | +- ID_MATCH: ___check_obj_id(G['Params4bit'], 102513768940032)           
	| | +- GuardManager: source=G['__import_kernels'], accessed_by=DictGetItemGuardAccessor(__import_kernels)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'], 140712752735120)     
	| | | +- GuardManager: source=G['__import_kernels'].DEBUG_FLAG, accessed_by=GetAttrGuardAccessor(DEBUG_FLAG)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'].DEBUG_FLAG, 140718482561760)
	| | | +- GuardManager: source=G['__import_kernels'].fused_dequantize_op, accessed_by=GetAttrGuardAccessor(fused_dequantize_op)
	| | | | +- TYPE_MATCH: ___check_type_id(G['__import_kernels'].fused_dequantize_op, 102513752736032)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'].fused_dequantize_op, 140712749840912)
	| | | | +- GuardManager: source=G['__import_kernels'].fused_dequantize_op._opoverload, accessed_by=GetAttrGuardAccessor(_opoverload)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'].fused_dequantize_op._opoverload, 140712749959024)
	| | | +- GuardManager: source=G['__import_kernels'].torch, accessed_by=GetAttrGuardAccessor(torch)
	| | | | +- OBJECT_ALIASING: G['__import_bitsandbytes_dot_nn_dot_modules'].torch is G['__import_kernels'].torch
	| | +- GuardManager: source=G['fused_dequantize'], accessed_by=DictGetItemGuardAccessor(fused_dequantize)
	| | | +- GuardManager: source=G['fused_dequantize'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize'].__code__, 102513784148304)
	| | +- GuardManager: source=G['ENABLE_ASSERTIONS'], accessed_by=DictGetItemGuardAccessor(ENABLE_ASSERTIONS)
	| | | +- ID_MATCH: ___check_obj_id(G['ENABLE_ASSERTIONS'], 140718482561760)    
	| | +- GuardManager: source=G['get_data_transposed'], accessed_by=DictGetItemGuardAccessor(get_data_transposed)
	| | | +- GuardManager: source=G['get_data_transposed'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | +- ID_MATCH: ___check_obj_id(G['get_data_transposed'].__code__, 102513784148304)
	| | +- GuardManager: source=G['TransposeBMatMul4Bit'], accessed_by=DictGetItemGuardAccessor(TransposeBMatMul4Bit)
	| | | +- ID_MATCH: ___check_obj_id(G['TransposeBMatMul4Bit'], 102513789618464) 
	| | | +- GuardManager: source=G['TransposeBMatMul4Bit'].forward, accessed_by=GetAttrGuardAccessor(forward)
	| | | | +- ID_MATCH: ___check_obj_id(G['TransposeBMatMul4Bit'].forward, 140712750173696)
	| | +- GuardManager: source=G['__builtins_dict___10'], accessed_by=DictGetItemGuardAccessor(__builtins_dict___10)
	| | | +- GuardManager: source=G['__builtins_dict___10']['any'], accessed_by=DictGetItemGuardAccessor(any)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['any'], 140718464975872)
	| | | +- GuardManager: source=G['__builtins_dict___10']['str'], accessed_by=DictGetItemGuardAccessor(str)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['str'], 140718482509248)
	| | | +- GuardManager: source=G['__builtins_dict___10']['bool'], accessed_by=DictGetItemGuardAccessor(bool)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['bool'], 140718482561792)
	| | | +- GuardManager: source=G['__builtins_dict___10']['super'], accessed_by=DictGetItemGuardAccessor(super)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['super'], 140718482519008)
	| | | +- GuardManager: source=G['__builtins_dict___10']['getattr'], accessed_by=DictGetItemGuardAccessor(getattr)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['getattr'], 140718464976912)
	| | | +- GuardManager: source=G['__builtins_dict___10']['isinstance'], accessed_by=DictGetItemGuardAccessor(isinstance)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['isinstance'], 140718464977472)
	| | +- GuardManager: source=G['fix_4bit_weight_quant_state_from_module'], accessed_by=DictGetItemGuardAccessor(fix_4bit_weight_quant_state_from_module)
	| | | +- GuardManager: source=G['fix_4bit_weight_quant_state_from_module'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | +- ID_MATCH: ___check_obj_id(G['fix_4bit_weight_quant_state_from_module'].__code__, 102513768746192)
	| | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'], accessed_by=DictGetItemGuardAccessor(__import_bitsandbytes_dot_nn_dot_modules)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'], 140713153122992)
	| | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch, accessed_by=GetAttrGuardAccessor(torch)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'].torch, 140718364431024)
	| | | | +- OBJECT_ALIASING: G['__import_bitsandbytes_dot_nn_dot_modules'].torch is G['torch']
	| | | | +- OBJECT_ALIASING: G['__import_bitsandbytes_dot_nn_dot_modules'].torch is G['__import_kernels'].torch
	| | | | +- OBJECT_ALIASING: G['__import_bitsandbytes_dot_nn_dot_modules'].torch is G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch
	| | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn, accessed_by=GetAttrGuardAccessor(nn)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn, 140713181445024)
	| | | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn.functional, accessed_by=GetAttrGuardAccessor(functional)
	| | | | | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn.functional, 140713178274832)
	| | | | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn.functional
	| | | | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn.functional.linear, accessed_by=GetAttrGuardAccessor(linear)
	| | | | | | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn.functional.linear, 140715851772960)
	| | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.numel, accessed_by=GetAttrGuardAccessor(numel)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'].torch.numel, 140718363174816)
	| | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.matmul, accessed_by=GetAttrGuardAccessor(matmul)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'].torch.matmul, 140718363201264)
	| | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.float16, accessed_by=GetAttrGuardAccessor(float16)
	| | | | | +- EQUALS_MATCH: G['__import_bitsandbytes_dot_nn_dot_modules'].torch.float16 == torch.float16
	| | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.float32, accessed_by=GetAttrGuardAccessor(float32)
	| | | | | +- EQUALS_MATCH: G['__import_bitsandbytes_dot_nn_dot_modules'].torch.float32 == torch.float32
	| | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.bfloat16, accessed_by=GetAttrGuardAccessor(bfloat16)
	| | | | | +- EQUALS_MATCH: G['__import_bitsandbytes_dot_nn_dot_modules'].torch.bfloat16 == torch.bfloat16
	| | | | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'].torch.is_autocast_enabled, accessed_by=GetAttrGuardAccessor(is_autocast_enabled)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'].torch.is_autocast_enabled, 140718363048544)
	| | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'], accessed_by=DictGetItemGuardAccessor(__import_peft_dot_tuners_dot_lora_dot_bnb)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_lora_dot_bnb'], 140712533784624)
	| | | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch, accessed_by=GetAttrGuardAccessor(torch)
	| | | | +- OBJECT_ALIASING: G['__import_bitsandbytes_dot_nn_dot_modules'].torch is G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch
	| | +- GuardManager: source=G['__import_peft_dot_tuners_dot_tuners_utils'], accessed_by=DictGetItemGuardAccessor(__import_peft_dot_tuners_dot_tuners_utils)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_tuners_utils'], 140712539645616)
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_linear)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'], 140713178274672)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F, accessed_by=GetAttrGuardAccessor(F)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F, 140713178274832)
	| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_torch_dot_nn_dot_modules_dot_activation'].F
	| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_bitsandbytes_dot_nn_dot_modules'].torch.nn.functional
	| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F.silu, accessed_by=GetAttrGuardAccessor(silu)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F.silu, 140713176241856)
	| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F.linear, accessed_by=GetAttrGuardAccessor(linear)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F.linear, 140715851772960)
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_module)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'], 140713181532528)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].Buffer, accessed_by=GetAttrGuardAccessor(Buffer)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].Buffer, 102513747618640)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].Module, accessed_by=GetAttrGuardAccessor(Module)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].Module, 102513749305376)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].Parameter, accessed_by=GetAttrGuardAccessor(Parameter)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].Parameter, 102513747612032)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_pre_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_pre_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_activation'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_activation)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_activation'], 140713176751280)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_activation'].F, accessed_by=GetAttrGuardAccessor(F)
	| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_torch_dot_nn_dot_modules_dot_activation'].F
	| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)
	| | | +- OBJECT_ALIASING: G['__import_bitsandbytes_dot_nn_dot_modules'].torch is G['torch']
	+- LAMBDA_GUARD: L['x'].stride()[0] == 2048*L['x'].size()[1]                   # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1] == 2048  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0] == 8192  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1] == 2048  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0] == 8192  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1] == 8192  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0] == 2048  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['x'].size()[1] <= 262143                               # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0326 23:52:06.304000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "33d572c0cb23593c4d28151004d87611"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993526303969.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 8,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.304000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "380be8ad844fc49a12f6c50f54b95700"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993526304251.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 8,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.304000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "3/0", "frame_key": "6", "co_name": "compiled_llama_mlp", "co_filename": "/tmp/ipykernel_331427/318589162.py", "co_firstlineno": 10, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": 362, "shape_env_guard_count": 29, "graph_op_count": 32, "graph_node_count": 77, "graph_input_count": 38, "start_time": 1742993525.3160312, "entire_frame_compile_time_s": 0.9878687858581543, "backend_compile_time_s": 0.46910715103149414, "inductor_compile_time_s": 0.07549047470092773, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": ["mylib::fused_dequantize_op"], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0326 23:52:06.410000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2817] {"artifact": {"name": "recompile_reasons", "encoding": "json"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "d37806bf77ab47f5baa6d814cca4993c"}
	[
	"2/1: tensor 'L['A']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA)",
	"2/0: tensor 'L['A']' dispatch key set mismatch. expected DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), actual DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA)"
	]
V0326 23:52:06.411000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 307, "name": "apply", "filename": 41}, {"line": 65, "name": "backward", "filename": 40}, {"line": 18, "name": "augmented_dequantize_4bit", "filename": 42}, {"line": 330, "name": "fused_dequantize", "filename": 43}]}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.411000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "7958a9d0bc9c69c4852a41295912b04e"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993526411459.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.411000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "9d2c8eeffd6c635d88c8c5954763fbe2"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993526411459.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.414000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 14, "size": 2097152}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.414000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [2097152, 1], "is_leaf": true, "stride": [1, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3403b4d0>", "describer_id": 14}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.414000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [1, 2097152], "is_leaf": true, "is_view": true, "stride": [1, 1], "storage": 0, "base": 1, "creation_meta": "CreationMeta.DEFAULT", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ff9c22e1b80>", "describer_id": 14}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.414000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 14, "id": 0, "source": "L['A']"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.417000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 14, "size": 65536}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.417000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [65536], "is_leaf": true, "stride": [1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e84e850>", "describer_id": 14}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.417000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 14, "id": 3, "source": "L['quant_state'].absmax"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.418000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 14, "size": 1024}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.418000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e84e800>", "describer_id": 14}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.418000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 14, "id": 4, "source": "L['quant_state'].state2.code"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.419000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 14, "size": 1024}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.419000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e84e760>", "describer_id": 14}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.419000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 14, "id": 5, "source": "L['quant_state'].state2.absmax"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.420000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 14, "size": 2}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.421000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e84e7b0>", "describer_id": 14}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.421000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 14, "id": 6, "source": "L['quant_state'].offset"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.421000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 14, "size": 64}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.421000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e84e8a0>", "describer_id": 14}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.422000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 14, "id": 7, "source": "L['quant_state'].code"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.427000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_a_": [1, "s1"], "l_quant_state_absmax": ["s2"], "l_quant_state_state2_code": [256], "l_quant_state_state2_absmax": ["s3"], "l_quant_state_offset": [], "l_quant_state_code": [16], "output_ptr": ["s4", 2048]}}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "7cb44ba18984348ae7f41158206ad8aa"}
	class GraphModule(torch.nn.Module):
	    def forward(self, s1: "Sym(s1)", L_A_: "u8[1, s1][1, 1]cuda:0", s2: "Sym(s2)", L_quant_state_absmax: "u8[s2][1]cuda:0", L_quant_state_state2_code: "f32[256][1]cuda:0", s3: "Sym(s3)", L_quant_state_state2_absmax: "f32[s3][1]cuda:0", L_quant_state_offset: "f16[][]cuda:0", L_quant_state_code: "f32[16][1]cuda:0", L_quant_state_shape_0_: "Sym(s4)"):
	        l_a_ = L_A_
	        l_quant_state_absmax = L_quant_state_absmax
	        l_quant_state_state2_code = L_quant_state_state2_code
	        l_quant_state_state2_absmax = L_quant_state_state2_absmax
	        l_quant_state_offset = L_quant_state_offset
	        l_quant_state_code = L_quant_state_code
	        l_quant_state_shape_0_ = L_quant_state_shape_0_
	        
	         # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:332 in fused_dequantize, code: n_elements = torch.numel(A) * 2
	        mul: "Sym(2*s1)" = s1 * 2;  s1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	        output_ptr: "bf16[s4, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_quant_state_absmax, l_quant_state_state2_code, l_quant_state_state2_absmax, l_quant_state_offset, l_a_, l_quant_state_code, mul, 256, 32, 16384, 8192, l_quant_state_shape_0_, 2048, torch.bfloat16);  l_quant_state_absmax = l_quant_state_state2_code = l_quant_state_state2_absmax = l_quant_state_offset = l_a_ = l_quant_state_code = mul = l_quant_state_shape_0_ = None
	        return (output_ptr,)
	        
V0326 23:52:06.427000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "aa7be82492a7f130ee32e51b80824e3f"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993526427473.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.427000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "958c606046b8e551c0d6991ccdd834f1"}
	{
	"name": "backend_compile",
	"ts": 1742993526427473.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.429000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "314e71802e756c6a76c682dd56e200fe"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993526429953.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.442000 331427 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:215] {"aot_forward_graph": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "3d5c1887ea22b2eca1234c87f471283a"}
	class <lambda>(torch.nn.Module):
	    def forward(self, arg0_1: "Sym(s1)", arg1_1: "u8[1, s1][1, 1]cuda:0", arg2_1: "Sym(s2)", arg3_1: "u8[s2][1]cuda:0", arg4_1: "f32[256][1]cuda:0", arg5_1: "Sym(s3)", arg6_1: "f32[s3][1]cuda:0", arg7_1: "f16[][]cuda:0", arg8_1: "f32[16][1]cuda:0", arg9_1: "Sym(s4)"):
	         # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:332 in fused_dequantize, code: n_elements = torch.numel(A) * 2
	        mul: "Sym(2*s1)" = arg0_1 * 2;  arg0_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	        fused_dequantize_op: "bf16[s4, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, arg7_1, arg1_1, arg8_1, mul, 256, 32, 16384, 8192, arg9_1, 2048, torch.bfloat16);  arg3_1 = arg4_1 = arg6_1 = arg7_1 = arg1_1 = arg8_1 = mul = arg9_1 = None
	        return (fused_dequantize_op,)
	        
V0326 23:52:06.443000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "edb9b2e17e2d4984f5c930a07dfa63bd"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993526443187.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.444000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "3a04b5b8d5b0198cc23e420d1f117aec"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993526443967.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.444000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "57e60dfb4d1f279bf7fd6fdbd7f14624"}
	{
	"name": "inductor_compile",
	"ts": 1742993526443967.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.450000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/we/cwejxmelilirrhy2igx7nsmi6g5rwqrfsgnr4fcqgfkbrwlsjorv.py"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "fac5e3747c2228c1dbcfc8fd69f50258"}
	# AOT ID: ['4_inference']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/xa/cxaqzqrucz62vf6o3o6sl2ihhxptbp2wjf7gaoczyt27xbq6aopg.py
	# Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	# Source node to ATen node mapping:
	#   output_ptr => fused_dequantize_op
	# Graph fragment:
	#   %fused_dequantize_op : [num_users=1] = call_function[target=torch.ops.mylib.fused_dequantize_op.default](args = (%arg3_1, %arg4_1, %arg6_1, %arg7_1, %arg1_1, %arg8_1, %mul, 256, 32, 16384, 8192, %arg9_1, 2048, torch.bfloat16), kwargs = {})
	triton_poi_fused_fused_dequantize_op_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {2: 1}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=(2,))]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_fused_dequantize_op_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 1
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    tmp0 = tl.load(in_ptr0 + (0)).to(tl.float32)
	    tmp1 = tl.broadcast_to(tmp0, [XBLOCK])
	    tl.store(out_ptr0 + (tl.full([XBLOCK], 0, tl.int32)), tmp1, None)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1 = args
	    args.clear()
	    s1 = arg0_1
	    s2 = arg2_1
	    s3 = arg5_1
	    s4 = arg9_1
	    assert_size_stride(arg1_1, (1, s1), (1, 1))
	    assert_size_stride(arg3_1, (s2, ), (1, ))
	    assert_size_stride(arg4_1, (256, ), (1, ))
	    assert_size_stride(arg6_1, (s3, ), (1, ))
	    assert_size_stride(arg7_1, (), ())
	    assert_size_stride(arg8_1, (16, ), (1, ))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((), (), torch.float16)
	        # Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_fused_dequantize_op_0.run(arg7_1, buf0, 1, grid=grid(1), stream=stream0)
	        del arg7_1
	        # Topologically Sorted Source Nodes: [output_ptr], Original ATen: [mylib.fused_dequantize_op]
	        buf1 = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, buf0, arg1_1, arg8_1, 2*s1, 256, 32, 16384, 8192, s4, 2048, torch.bfloat16)
	        del arg1_1
	        del arg3_1
	        del arg4_1
	        del arg6_1
	        del arg8_1
	        del buf0
	        buf2 = buf1
	        del buf1
	    return (buf2, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    arg0_1 = 524288
	    arg1_1 = rand_strided((1, 524288), (1, 1), device='cuda:0', dtype=torch.uint8)
	    arg2_1 = 16384
	    arg3_1 = rand_strided((16384, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    arg4_1 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg5_1 = 64
	    arg6_1 = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg7_1 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    arg8_1 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    arg9_1 = 512
	    fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:52:06.450000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "22c16861dc39484cdf1af32a29a18a3e"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742993526450317.2,
	"args": {
	"key": "fodgjswdodzqgx3iq4x2v2ulrxwzzqobix2qlzyjko7fokxzlgfp",
	"components": [
	"[grf5bl6rpigmyiaz6c6m4gfmm52jce23fz2stipebcebees47ma] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1):\n    mul = arg0_1 * 2;  arg0_1 = None\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, arg7_1, arg1_1, arg8_1, mul, 256, 32, 16384, 8192, arg9_1, 2048, torch.bfloat16);  arg3_1 = arg4_1 = arg6_1 = arg7_1 = arg1_1 = arg8_1 = mul = arg9_1 = None\n    return (fused_dequantize_op,)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[tpvnjs264ct5lugh57xhauij37yhdwzbkxdyhhhhj5de2tskcds] example_inputs[0]: ('s1',)",
	"[7vwriczjssi3zjl7yvm6w4lda6j63tvcm76dxaglqafuowfzcmv] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([1, s1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[2]: ('s2',)",
	"[sqttp5i5wnsdcripsqrsj7sbdbvyr6p2v547c3del3vagfumq23] example_inputs[3]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([s2]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[4]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[5]: ('s3',)",
	"[osj4eueidclo5irb65z7qycfmcyzhzy3mt2msh2p7b3yhteohpt] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([s3]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[j23zbqrguggah6nscnzj4y55d5chhoardpgvlji4ilrfwrr6asz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[2ylzn3y4vcv2t7rbq447jvkl6by5qf32ycwks3h2nbcmm6wvmnk] example_inputs[8]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ioa426km4idgc234gbdjtmikotjmsh7hn3tdqahy4f4zia42h53] example_inputs[9]: ('s4',)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []",
	"[jan4rfottd4caig5fc5h3ieb5ephfyhn7rn27jmu2ehbwgssm5j] fx_kwargs[user_visible_outputs]: {'fused_dequantize_op': None}",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1",
	"[kcuxe2zwm3mzv2uk6adm6iskoy35bqfv725twacrdewod2dbl5d] inputs_to_check[1]: 3",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[2]: 4",
	"[agkvbkaha53nbz3aeeuhvxjvvc4glhfjofzkg6g2qjoo2e5otcx] inputs_to_check[3]: 6",
	"[j3s5elu6itwgjafc7rzhy4whrbufl6kfmlufjhh25grt643bk5f] inputs_to_check[4]: 7",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inputs_to_check[5]: 8",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 36553287,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:52:06.450000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "479c7fc7b4aeb8b6d83b60fc11accd35"}
	{"key": "fodgjswdodzqgx3iq4x2v2ulrxwzzqobix2qlzyjko7fokxzlgfp", "components": ["[grf5bl6rpigmyiaz6c6m4gfmm52jce23fz2stipebcebees47ma] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1):\n    mul = arg0_1 * 2;  arg0_1 = None\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(arg3_1, arg4_1, arg6_1, arg7_1, arg1_1, arg8_1, mul, 256, 32, 16384, 8192, arg9_1, 2048, torch.bfloat16);  arg3_1 = arg4_1 = arg6_1 = arg7_1 = arg1_1 = arg8_1 = mul = arg9_1 = None\n    return (fused_dequantize_op,)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[tpvnjs264ct5lugh57xhauij37yhdwzbkxdyhhhhj5de2tskcds] example_inputs[0]: ('s1',)", "[7vwriczjssi3zjl7yvm6w4lda6j63tvcm76dxaglqafuowfzcmv] example_inputs[1]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([1, s1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[2]: ('s2',)", "[sqttp5i5wnsdcripsqrsj7sbdbvyr6p2v547c3del3vagfumq23] example_inputs[3]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([s2]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hazxvscuvstlxppmnmybkqlmrmewl2nazc5rfhfeo7ytzkw2au3] example_inputs[4]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[5]: ('s3',)", "[osj4eueidclo5irb65z7qycfmcyzhzy3mt2msh2p7b3yhteohpt] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([s3]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[j23zbqrguggah6nscnzj4y55d5chhoardpgvlji4ilrfwrr6asz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[2ylzn3y4vcv2t7rbq447jvkl6by5qf32ycwks3h2nbcmm6wvmnk] example_inputs[8]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ioa426km4idgc234gbdjtmikotjmsh7hn3tdqahy4f4zia42h53] example_inputs[9]: ('s4',)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []", "[jan4rfottd4caig5fc5h3ieb5ephfyhn7rn27jmu2ehbwgssm5j] fx_kwargs[user_visible_outputs]: {'fused_dequantize_op': None}", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1", "[kcuxe2zwm3mzv2uk6adm6iskoy35bqfv725twacrdewod2dbl5d] inputs_to_check[1]: 3", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inputs_to_check[2]: 4", "[agkvbkaha53nbz3aeeuhvxjvvc4glhfjofzkg6g2qjoo2e5otcx] inputs_to_check[3]: 6", "[j3s5elu6itwgjafc7rzhy4whrbufl6kfmlufjhh25grt643bk5f] inputs_to_check[4]: 7", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inputs_to_check[5]: 8", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 36553287, "cache_state": "hit"}
V0326 23:52:06.451000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "af0087d4634f96dfb7f0eea9eb420077"}
	{
	"name": "inductor_compile",
	"ts": 1742993526451280.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.451000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "85e874a18b7212b4bc390aa6bc8a3b76"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993526451523.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.451000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "fcc5e63b8f1d931cc3f81b8c0fbcbe1c"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993526451885.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.453000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "daab09d393cdb2f91580d5707dcedaf8"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993526453138.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.453000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "278750e842be01a634a796ea4b7f89df"}
	{
	"name": "backend_compile",
	"ts": 1742993526453468.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.453000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "c240687e17e0ec1ef985da6e59e41279"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993526453711.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.460000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "e3cebb343d4e531a25fc7f9e175cd3e1"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['A'], accessed_by=DictGetItemGuardAccessor(A)
	| | +- TENSOR_MATCH: check_tensor(L['A'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.uint8, device=0, requires_grad=False, size=[1, None], stride=[1, 1])
	| | +- NO_HASATTR: hasattr(L['A'], '_dynamo_dynamic_indices') == False         
	| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['A'], L['quant_state'].code, L['quant_state'].absmax, L['quant_state'].offset, L['quant_state'].state2.code, L['quant_state'].state2.absmax)
	| +- GuardManager: source=L['quant_state'], accessed_by=DictGetItemGuardAccessor(quant_state)
	| | +- TYPE_MATCH: ___check_type_id(L['quant_state'], 102513767659184)         
	| | +- GuardManager: source=L['quant_state'].code, accessed_by=GetAttrGuardAccessor(code)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].code, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | +- EQUALS_MATCH: L['quant_state'].dtype == torch.bfloat16                    
	| | +- GuardManager: source=L['quant_state'].shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].shape, 140717689517824)   
	| | | +- LENGTH_CHECK: len(L['quant_state'].shape) == 2                            
	| | | +- GuardManager: source=L['quant_state'].shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].shape[0], 140718482538560)
	| | | +- GuardManager: source=L['quant_state'].shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | +- EQUALS_MATCH: L['quant_state'].shape[1] == 2048                           
	| | +- GuardManager: source=L['quant_state'].absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.uint8, device=0, requires_grad=False, size=[None], stride=[1])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].absmax, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | +- TENSOR_MATCH: check_tensor(L['quant_state'].offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | +- NO_HASATTR: hasattr(L['quant_state'].offset, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['quant_state'].state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | +- TYPE_MATCH: ___check_type_id(L['quant_state'].state2, 102513767659184)  
	| | | +- GuardManager: source=L['quant_state'].state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | +- TENSOR_MATCH: check_tensor(L['quant_state'].state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | +- NO_HASATTR: hasattr(L['quant_state'].state2.code, '_dynamo_dynamic_indices') == False
	| | | | +- NO_TENSOR_ALIASING
	| | | +- GuardManager: source=L['quant_state'].state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | +- TENSOR_MATCH: check_tensor(L['quant_state'].state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[None], stride=[1])
	| | | | +- NO_HASATTR: hasattr(L['quant_state'].state2.absmax, '_dynamo_dynamic_indices') == False
	| | | | +- NO_TENSOR_ALIASING
	| | | +- GuardManager: source=L['quant_state'].state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | +- EQUALS_MATCH: L['quant_state'].state2.blocksize == 256                    
	| | +- GuardManager: source=L['quant_state'].blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | +- EQUALS_MATCH: L['quant_state'].blocksize == 64                            
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)
	| | | +- ID_MATCH: ___check_obj_id(G['torch'], 140718364431024)                
	| | | +- GuardManager: source=G['torch'].numel, accessed_by=GetAttrGuardAccessor(numel)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].numel, 140718363174816)          
	| | +- GuardManager: source=G['DEBUG_FLAG'], accessed_by=DictGetItemGuardAccessor(DEBUG_FLAG)
	| | | +- ID_MATCH: ___check_obj_id(G['DEBUG_FLAG'], 140718482561760)           
	| | +- GuardManager: source=G['fused_dequantize_op'], accessed_by=DictGetItemGuardAccessor(fused_dequantize_op)
	| | | +- TYPE_MATCH: ___check_type_id(G['fused_dequantize_op'], 102513752736032) 
	| | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize_op'], 140712749840912)  
	| | | +- GuardManager: source=G['fused_dequantize_op']._opoverload, accessed_by=GetAttrGuardAccessor(_opoverload)
	| | | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize_op']._opoverload, 140712749959024)
	+- LAMBDA_GUARD: Ne(2048*L['quant_state'].shape[0], 0)                         # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: Ne(L['quant_state'].shape[0], 1)                              # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2048*L['quant_state'].shape[0] >= 2                           # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['A'].size()[1]                                         # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['quant_state'].absmax.size()[0]                        # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['quant_state'].state2.absmax.size()[0]                 # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 0 <= L['quant_state'].shape[0]                                # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0326 23:52:06.461000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "0a3ad54b182a4b2a527d6a10e49069b4"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993526461152.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.461000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0, "has_payload": "94415f7ab8eb1f2f40694acd71afcf6d"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993526461396.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 9,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.461000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "2/2", "frame_key": "7", "co_name": "fused_dequantize", "co_filename": "/home/tom/unsloth-challenges/submit/Problem_C/kernels.py", "co_firstlineno": 330, "cache_size": 2, "accumulated_cache_size": 2, "guard_count": 25, "shape_env_guard_count": 6, "graph_op_count": 2, "graph_node_count": 13, "graph_input_count": 10, "start_time": 1742993526.4114516, "entire_frame_compile_time_s": 0.049643754959106445, "backend_compile_time_s": 0.025967121124267578, "inductor_compile_time_s": 0.007273197174072266, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": ["mylib::fused_dequantize_op"], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 2, "frame_compile_id": 2, "attempt": 0}
V0326 23:52:06.500000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2817] {"artifact": {"name": "recompile_reasons", "encoding": "json"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "fd5dbf1710af84cc3760f9196c3580c7"}
	[
	"3/0: ___check_obj_id(L['self']._modules['gate_proj']._modules['base_layer'].compute_type_is_set, 140718482561760)"
	]
V0326 23:52:06.501000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 10, "name": "compiled_llama_mlp", "filename": 44}]}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.501000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "010100ff71532d71c01fcb90a36f1172"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993526501316.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.501000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "15587aecd87935f9d4f0631fa22d2c8a"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993526501316.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.512000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 16, "size": 1064960}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.512000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 3, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [1, 260, 2048], "requires_grad": true, "stride": [532480, 2048, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3ea43b60>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.512000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 0, "source": "L['x']"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.517000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 16, "size": 8388608}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.517000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [8388608, 1], "is_leaf": true, "stride": [1, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa340392c0>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.518000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 1, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.524000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 16, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.524000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [262144], "is_leaf": true, "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3094dbd0>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.524000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 2, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.525000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 16, "size": 1024}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.525000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3094de00>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.526000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 3, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.526000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 16, "size": 4096}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.526000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024], "is_leaf": true, "stride": [1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3093c690>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.527000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 4, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.527000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 16, "size": 2}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.527000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3094e3a0>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.528000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 5, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.528000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 6, "describer_id": 16, "size": 64}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.528000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 6, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e831e50>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.529000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 6, "source": "L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.563000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 7, "describer_id": 16, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.563000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 8, "describer_id": 16, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.563000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 19, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 2048], "is_leaf": true, "stride": [2048, 1], "storage": 8, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ff9c22d4870>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.563000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 18, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 2048], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2048, 1], "storage": 7, "grad": 19, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7ffa2e835220>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.564000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 18, "source": "L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.566000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s1", "sources": ["L['x'].size()[2]"], "value": "2048", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 897, "name": "call_function", "filename": 48}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 271, "name": "_fn", "filename": 49}, {"line": 4364, "name": "matmul", "filename": 50}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 2013, "name": "_dispatch_impl", "filename": 32}, {"line": 716, "name": "__call__", "filename": 51}, {"line": 273, "name": "_fn", "filename": 49}, {"line": 2100, "name": "meta_mm", "filename": 52}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 518, "name": "forward", "filename": 39}, {"line": 125, "name": "forward", "filename": 53}]}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.569000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 9, "describer_id": 16, "size": 1048576}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.569000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 10, "describer_id": 16, "size": 1048576}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.569000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 24, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [8192, 32], "is_leaf": true, "stride": [32, 1], "storage": 10, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ff9c22d7660>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.569000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 23, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [8192, 32], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [32, 1], "storage": 9, "grad": 24, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7ffa2e834dc0>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.570000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 23, "source": "L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.574000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s5", "sources": ["L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0]"], "value": "8192", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2220, "name": "BINARY_OP", "filename": 27}, {"line": 301, "name": "impl", "filename": 27}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 943, "name": "_handle_insert_op_in_graph", "filename": 28}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 1924, "name": "_dispatch_impl", "filename": 32}, {"line": 831, "name": "fast_binary_impl", "filename": 33}, {"line": 785, "name": "infer_size", "filename": 33}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 536, "name": "forward", "filename": 39}]}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.590000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 11, "describer_id": 16, "size": 8388608}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.590000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 28, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [8388608, 1], "is_leaf": true, "stride": [1, 1], "storage": 11, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa34039270>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.590000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 28, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.597000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 12, "describer_id": 16, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.597000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 29, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [262144], "is_leaf": true, "stride": [1], "storage": 12, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa30940fa0>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.597000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 29, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.598000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 13, "describer_id": 16, "size": 1024}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.598000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 30, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 13, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e830af0>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.598000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 30, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.599000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 14, "describer_id": 16, "size": 4096}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.599000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 31, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024], "is_leaf": true, "stride": [1], "storage": 14, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e833930>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.599000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 31, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.600000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 15, "describer_id": 16, "size": 2}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.600000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 32, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 15, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa2e8311d0>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.600000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 32, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.601000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 16, "describer_id": 16, "size": 64}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.601000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 33, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 16, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa30942f30>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.601000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 33, "source": "L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.614000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s8", "sources": ["L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1]"], "value": "2048", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 1680, "name": "CALL_FUNCTION_EX", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 1024, "name": "call_function", "filename": 54}, {"line": 774, "name": "call_method", "filename": 54}, {"line": 699, "name": "call_apply", "filename": 54}, {"line": 2015, "name": "call_function", "filename": 55}, {"line": 462, "name": "speculate_subgraph", "filename": 55}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 897, "name": "call_function", "filename": 48}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 271, "name": "_fn", "filename": 49}, {"line": 4364, "name": "matmul", "filename": 50}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 2013, "name": "_dispatch_impl", "filename": 32}, {"line": 716, "name": "__call__", "filename": 51}, {"line": 273, "name": "_fn", "filename": 49}, {"line": 2100, "name": "meta_mm", "filename": 52}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 496, "name": "forward", "filename": 39}, {"line": 110, "name": "inner_transpose_forward", "filename": 40}, {"line": 34, "name": "forward", "filename": 40}]}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.635000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 17, "describer_id": 16, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.635000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 18, "describer_id": 16, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.635000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 46, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 2048], "is_leaf": true, "stride": [2048, 1], "storage": 18, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ff9c22e7070>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.635000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 45, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 2048], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2048, 1], "storage": 17, "grad": 46, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7ffa2e834500>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.636000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 45, "source": "L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.639000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 19, "describer_id": 16, "size": 1048576}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.639000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 20, "describer_id": 16, "size": 1048576}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.639000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 51, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [8192, 32], "is_leaf": true, "stride": [32, 1], "storage": 20, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ff9c22e1090>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.640000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 50, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [8192, 32], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [32, 1], "storage": 19, "grad": 51, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7ffa2e834190>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.640000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 50, "source": "L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.644000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s9", "sources": ["L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0]"], "value": "8192", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2220, "name": "BINARY_OP", "filename": 27}, {"line": 301, "name": "impl", "filename": 27}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 943, "name": "_handle_insert_op_in_graph", "filename": 28}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 1924, "name": "_dispatch_impl", "filename": 32}, {"line": 831, "name": "fast_binary_impl", "filename": 33}, {"line": 785, "name": "infer_size", "filename": 33}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 536, "name": "forward", "filename": 39}]}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.656000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 21, "describer_id": 16, "size": 8388608}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.656000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 55, "ndim": 2, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [8388608, 1], "is_leaf": true, "stride": [1, 1], "storage": 21, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa34039680>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.657000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 55, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.663000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 22, "describer_id": 16, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.663000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 56, "ndim": 1, "dtype": "torch.uint8", "device": "device(type='cuda', index=0)", "size": [262144], "is_leaf": true, "stride": [1], "storage": 22, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa30949db0>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.663000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 56, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.664000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 23, "describer_id": 16, "size": 1024}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.664000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 57, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256], "is_leaf": true, "stride": [1], "storage": 23, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3094a1c0>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.664000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 57, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.665000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 24, "describer_id": 16, "size": 4096}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.665000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 58, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024], "is_leaf": true, "stride": [1], "storage": 24, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa30938c80>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.665000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 58, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.666000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 25, "describer_id": 16, "size": 2}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.666000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 59, "ndim": 0, "dtype": "torch.float16", "device": "device(type='cuda', index=0)", "size": [], "is_leaf": true, "stride": [], "storage": 25, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa30948eb0>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.666000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 59, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.667000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 26, "describer_id": 16, "size": 64}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.667000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 60, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [16], "is_leaf": true, "stride": [1], "storage": 26, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ffa3093dae0>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.667000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 60, "source": "L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.679000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s12", "sources": ["L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1]"], "value": "8192", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 1680, "name": "CALL_FUNCTION_EX", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 1024, "name": "call_function", "filename": 54}, {"line": 774, "name": "call_method", "filename": 54}, {"line": 699, "name": "call_apply", "filename": 54}, {"line": 2015, "name": "call_function", "filename": 55}, {"line": 462, "name": "speculate_subgraph", "filename": 55}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 897, "name": "call_function", "filename": 48}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 271, "name": "_fn", "filename": 49}, {"line": 4364, "name": "matmul", "filename": 50}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 2013, "name": "_dispatch_impl", "filename": 32}, {"line": 716, "name": "__call__", "filename": 51}, {"line": 273, "name": "_fn", "filename": 49}, {"line": 2100, "name": "meta_mm", "filename": 52}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 496, "name": "forward", "filename": 39}, {"line": 110, "name": "inner_transpose_forward", "filename": 40}, {"line": 34, "name": "forward", "filename": 40}]}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.700000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 27, "describer_id": 16, "size": 1048576}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.700000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 28, "describer_id": 16, "size": 1048576}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.700000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 73, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 8192], "is_leaf": true, "stride": [8192, 1], "storage": 28, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ff9c212dc70>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.701000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 72, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [32, 8192], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [8192, 1], "storage": 27, "grad": 73, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7ffa2e8359f0>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.701000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 72, "source": "L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.704000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 29, "describer_id": 16, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.704000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 30, "describer_id": 16, "size": 262144}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.705000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 78, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [2048, 32], "is_leaf": true, "stride": [32, 1], "storage": 30, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7ff9c212e3a0>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.705000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 77, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [2048, 32], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [32, 1], "storage": 29, "grad": 78, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7ffa2e834050>", "describer_id": 16}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.705000 331427 .venv/lib/python3.12/site-packages/torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 77, "source": "L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.709000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s13", "sources": ["L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0]"], "value": "2048", "reason": "range_refined_to_singleton", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 582, "name": "wrapper", "filename": 27}, {"line": 2279, "name": "CALL", "filename": 27}, {"line": 2273, "name": "_call", "filename": 27}, {"line": 830, "name": "call_function", "filename": 27}, {"line": 156, "name": "realize_and_forward", "filename": 45}, {"line": 899, "name": "call_function", "filename": 46}, {"line": 324, "name": "call_function", "filename": 47}, {"line": 111, "name": "call_function", "filename": 47}, {"line": 836, "name": "inline_user_function_return", "filename": 27}, {"line": 3011, "name": "inline_call", "filename": 27}, {"line": 3139, "name": "inline_call_", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2220, "name": "BINARY_OP", "filename": 27}, {"line": 301, "name": "impl", "filename": 27}, {"line": 967, "name": "call_function", "filename": 28}, {"line": 943, "name": "_handle_insert_op_in_graph", "filename": 28}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 29}, {"line": 2124, "name": "wrap_fx_proxy_cls", "filename": 29}, {"line": 2017, "name": "get_fake_value", "filename": 30}, {"line": 1574, "name": "wrap_fake_exception", "filename": 30}, {"line": 2018, "name": "<lambda>", "filename": 30}, {"line": 2132, "name": "run_node", "filename": 30}, {"line": 21, "name": "wrapper", "filename": 31}, {"line": 1238, "name": "__torch_dispatch__", "filename": 32}, {"line": 1692, "name": "dispatch", "filename": 32}, {"line": 1348, "name": "_cached_dispatch_impl", "filename": 32}, {"line": 1924, "name": "_dispatch_impl", "filename": 32}, {"line": 831, "name": "fast_binary_impl", "filename": 33}, {"line": 785, "name": "infer_size", "filename": 33}, {"line": 1564, "name": "_check", "filename": 34}, {"line": 1527, "name": "_check_with", "filename": 34}, {"line": 1010, "name": "expect_true", "filename": 35}, {"line": 465, "name": "expect_true", "filename": 36}, {"line": 449, "name": "guard_bool", "filename": 36}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 5122, "name": "evaluate_expr", "filename": 35}, {"line": 5282, "name": "_evaluate_expr", "filename": 35}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 35}, {"line": 5469, "name": "_refine_ranges", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": [{"line": 12, "name": "compiled_llama_mlp", "filename": 44}, {"line": 536, "name": "forward", "filename": 39}]}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.713000 331427 .venv/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s4", "sources": ["L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1]"], "value": "2048", "reason": "find", "stack": [{"line": 198, "name": "_run_module_as_main", "filename": 1}, {"line": 88, "name": "_run_code", "filename": 1}, {"line": 18, "name": "<module>", "filename": 2}, {"line": 1075, "name": "launch_instance", "filename": 3}, {"line": 739, "name": "start", "filename": 4}, {"line": 205, "name": "start", "filename": 5}, {"line": 641, "name": "run_forever", "filename": 6}, {"line": 1986, "name": "_run_once", "filename": 6}, {"line": 88, "name": "_run", "filename": 7}, {"line": 545, "name": "dispatch_queue", "filename": 8}, {"line": 534, "name": "process_one", "filename": 8}, {"line": 437, "name": "dispatch_shell", "filename": 8}, {"line": 362, "name": "execute_request", "filename": 9}, {"line": 778, "name": "execute_request", "filename": 8}, {"line": 449, "name": "do_execute", "filename": 9}, {"line": 549, "name": "run_cell", "filename": 10}, {"line": 3044, "name": "run_cell", "filename": 11}, {"line": 3099, "name": "_run_cell", "filename": 11}, {"line": 128, "name": "_pseudo_sync_runner", "filename": 12}, {"line": 3303, "name": "run_cell_async", "filename": 11}, {"line": 3486, "name": "run_ast_nodes", "filename": 11}, {"line": 3546, "name": "run_code", "filename": 11}, {"line": 20, "name": "<module>", "filename": 13}, {"line": 2241, "name": "train", "filename": 14}, {"line": 2548, "name": "_inner_training_loop", "filename": 14}, {"line": 3698, "name": "training_step", "filename": 14}, {"line": 474, "name": "compute_loss", "filename": 15}, {"line": 3759, "name": "compute_loss", "filename": 14}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 819, "name": "forward", "filename": 17}, {"line": 807, "name": "__call__", "filename": 17}, {"line": 44, "name": "decorate_autocast", "filename": 18}, {"line": 1719, "name": "forward", "filename": 19}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 197, "name": "forward", "filename": 20}, {"line": 172, "name": "wrapped_func", "filename": 21}, {"line": 842, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 594, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 352, "name": "forward", "filename": 22}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 16}, {"line": 1747, "name": "_call_impl", "filename": 16}, {"line": 465, "name": "_fn", "filename": 24}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 25}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 26}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 27}, {"line": 983, "name": "run", "filename": 27}, {"line": 895, "name": "step", "filename": 27}, {"line": 2987, "name": "RETURN_VALUE", "filename": 27}, {"line": 2972, "name": "_return", "filename": 27}, {"line": 1117, "name": "compile_subgraph", "filename": 56}, {"line": 1317, "name": "compile_and_call_fx_graph", "filename": 56}, {"line": 287, "name": "insert_deferred_runtime_asserts", "filename": 57}, {"line": 280, "name": "match_symbol", "filename": 57}, {"line": 169, "name": "expr", "filename": 36}, {"line": 1532, "name": "wrapper", "filename": 35}, {"line": 4572, "name": "replace", "filename": 35}, {"line": 1532, "name": "wrapper", "filename": 35}, {"line": 262, "name": "wrapper", "filename": 37}, {"line": 4885, "name": "_find", "filename": 35}, {"line": 4842, "name": "_set_replacement", "filename": 35}, {"line": 1122, "name": "trace_structured", "filename": 38}], "user_stack": null}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:06.723000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_x_": [1, "s0", "2048"], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data": [8388608, 1], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax": [262144], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code": [256], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax": [1024], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset": [], "l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code": [16], "l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_": [32, 2048], "l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_": [8192, 32], "l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data": [8388608, 1], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax": [262144], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code": [256], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax": [1024], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset": [], "l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code": [16], "l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_": [32, 2048], "l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_": [8192, 32], "l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data": [8388608, 1], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax": [262144], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code": [256], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax": [1024], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset": [], "l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code": [16], "l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_": [32, 8192], "l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_": [2048, 32], "x": [1, "s0", "2048"], "B": [1, 8388608], "autograd_function_apply": [1, "s0", "8192"], "result": [1, "s0", "8192"], "result_1": [1, "s0", "8192"], "linear": [1, "s0", 32], "linear_1": [1, "s0", 8192], "output": [1, "s0", 8192], "result_2": [1, "s0", 8192], "silu": [1, "s0", 8192], "x_1": [1, "s0", 2048], "B_1": [1, 8388608], "autograd_function_apply_1": [1, "s0", "8192"], "result_3": [1, "s0", "8192"], "result_4": [1, "s0", "8192"], "linear_2": [1, "s0", 32], "linear_3": [1, "s0", 8192], "output_1": [1, "s0", 8192], "result_5": [1, "s0", 8192], "mul_2": [1, "s0", 8192], "x_2": [1, "s0", 8192], "B_2": [1, 8388608], "autograd_function_apply_2": [1, "s0", "2048"], "result_6": [1, "s0", "2048"], "result_7": [1, "s0", "2048"], "linear_4": [1, "s0", 32], "linear_5": [1, "s0", 2048], "output_2": [1, "s0", 2048], "result_8": [1, "s0", 2048]}}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "9a36626f9a16bbb1ceafa7902a308e8e"}
	class GraphModule(torch.nn.Module):
	    def forward(self, s0: "Sym(s0)", L_x_: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data: "u8[8388608, 1][1, 1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s2)", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s3)", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)", L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", L_self_modules_gate_proj_modules_lora_A_modules_default_parameters_weight_: "f32[32, 2048][2048, 1]cuda:0", L_self_modules_gate_proj_modules_lora_B_modules_default_parameters_weight_: "f32[8192, 32][32, 1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_some_data: "u8[8388608, 1][1, 1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s6)", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s7)", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)", L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", L_self_modules_up_proj_modules_lora_A_modules_default_parameters_weight_: "f32[32, 2048][2048, 1]cuda:0", L_self_modules_up_proj_modules_lora_B_modules_default_parameters_weight_: "f32[8192, 32][32, 1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_some_data: "u8[8388608, 1][1, 1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s10)", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s11)", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(8192)", L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(2048)", L_self_modules_down_proj_modules_lora_A_modules_default_parameters_weight_: "f32[32, 8192][8192, 1]cuda:0", L_self_modules_down_proj_modules_lora_B_modules_default_parameters_weight_: "f32[2048, 32][32, 1]cuda:0"):
	        l_x_ = L_x_
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data = L_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_
	        l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = L_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_
	        l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_ = L_self_modules_gate_proj_modules_lora_A_modules_default_parameters_weight_
	        l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_ = L_self_modules_gate_proj_modules_lora_B_modules_default_parameters_weight_
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data = L_self_modules_up_proj_modules_base_layer_parameters_weight_some_data
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_
	        l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = L_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_
	        l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_ = L_self_modules_up_proj_modules_lora_A_modules_default_parameters_weight_
	        l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_ = L_self_modules_up_proj_modules_lora_B_modules_default_parameters_weight_
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data = L_self_modules_down_proj_modules_base_layer_parameters_weight_some_data
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_
	        l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = L_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_
	        l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_ = L_self_modules_down_proj_modules_lora_A_modules_default_parameters_weight_
	        l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_ = L_self_modules_down_proj_modules_lora_B_modules_default_parameters_weight_
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        x: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = l_x_.to(torch.bfloat16)
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        B: "u8[1, 8388608][1, 1]cuda:0" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data.t();  l_self_modules_gate_proj_modules_base_layer_parameters_weight_some_data = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        function_ctx = torch.autograd.function.FunctionCtx();  function_ctx = None
	        fwd_body_0 = self.fwd_body_0
	        bwd_body_0 = self.bwd_body_0
	        autograd_function_apply: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.higher_order.autograd_function_apply(fwd_body_0, bwd_body_0, x, B, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, args_tensor_mask = [True, True, False, False, False], non_differentiable_idx = []);  fwd_body_0 = bwd_body_0 = x = B = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	        result: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = autograd_function_apply.to(torch.float16);  autograd_function_apply = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        result_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result.clone();  result = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        linear: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_, None);  l_self_modules_gate_proj_modules_lora_a_modules_default_parameters_weight_ = None
	        linear_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(linear, l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_, None);  linear = l_self_modules_gate_proj_modules_lora_b_modules_default_parameters_weight_ = None
	        output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = linear_1 * 2.0;  linear_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        result_2: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result_1 + output;  result_1 = output = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        silu: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.nn.functional.silu(result_2, inplace = False);  result_2 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        x_1: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = l_x_.to(torch.bfloat16)
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        B_1: "u8[1, 8388608][1, 1]cuda:0" = l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data.t();  l_self_modules_up_proj_modules_base_layer_parameters_weight_some_data = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        function_ctx_1 = torch.autograd.function.FunctionCtx();  function_ctx_1 = None
	        fwd_body_1 = self.fwd_body_1
	        bwd_body_1 = self.bwd_body_1
	        autograd_function_apply_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.higher_order.autograd_function_apply(fwd_body_1, bwd_body_1, x_1, B_1, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, args_tensor_mask = [True, True, False, False, False], non_differentiable_idx = []);  fwd_body_1 = bwd_body_1 = x_1 = B_1 = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	        result_3: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = autograd_function_apply_1.to(torch.float16);  autograd_function_apply_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        result_4: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result_3.clone();  result_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        linear_2: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_, None);  l_x_ = l_self_modules_up_proj_modules_lora_a_modules_default_parameters_weight_ = None
	        linear_3: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(linear_2, l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_, None);  linear_2 = l_self_modules_up_proj_modules_lora_b_modules_default_parameters_weight_ = None
	        output_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = linear_3 * 2.0;  linear_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        result_5: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = result_4 + output_1;  result_4 = output_1 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_2: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = silu * result_5;  silu = result_5 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        x_2: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = mul_2.to(torch.bfloat16)
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        B_2: "u8[1, 8388608][1, 1]cuda:0" = l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data.t();  l_self_modules_down_proj_modules_base_layer_parameters_weight_some_data = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        function_ctx_2 = torch.autograd.function.FunctionCtx();  function_ctx_2 = None
	        fwd_body_2 = self.fwd_body_2
	        bwd_body_2 = self.bwd_body_2
	        autograd_function_apply_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.higher_order.autograd_function_apply(fwd_body_2, bwd_body_2, x_2, B_2, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, args_tensor_mask = [True, True, False, False, False], non_differentiable_idx = []);  fwd_body_2 = bwd_body_2 = x_2 = B_2 = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	        result_6: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = autograd_function_apply_2.to(torch.float16);  autograd_function_apply_2 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        result_7: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = result_6.clone();  result_6 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        linear_4: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch._C._nn.linear(mul_2, l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_, None);  mul_2 = l_self_modules_down_proj_modules_lora_a_modules_default_parameters_weight_ = None
	        linear_5: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch._C._nn.linear(linear_4, l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_, None);  linear_4 = l_self_modules_down_proj_modules_lora_b_modules_default_parameters_weight_ = None
	        output_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = linear_5 * 2.0;  linear_5 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        result_8: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = result_7 + output_2;  result_7 = output_2 = None
	        return (result_8,)
	        
	    class fwd_body_0(torch.nn.Module):
	        def forward(self, ctx, A: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", B: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s3)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s2)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            a = A
	            b = B
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:21 in forward, code: if prod(A.shape) == 0:
	            size = a.size()
	            getitem = size[0];  getitem = None
	            getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
	            getitem_2: "Sym(2048)" = size[2];  getitem_2 = None
	            prod: "Sym(2048*s0)" = math_prod(size);  size = None
	            eq: "Sym(False)" = prod == 0;  prod = eq = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s3/2))" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s2*s3)" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s2*s3/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  rshift = mul = rshift_1 = None
	            
	             # File: /tmp/ipykernel_331427/1346670402.py:18 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:34 in forward, code: output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias) # NOTE the transposition
	            to: "bf16[2048, 8192][1, 2048]cuda:0" = t.to(torch.bfloat16);  t = None
	            t_1: "bf16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(a, t_1, None);  a = t_1 = None
	            return (output, [l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_])
	            
	    class bwd_body_0(torch.nn.Module):
	        def forward(self, ctx, grad_output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s3)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s2)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", b: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            # No stacktrace found for following nodes
	            _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s3/2))" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s2*s3)" = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize;  l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_blocksize = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s2*s3/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_offset = b = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = rshift = mul = rshift_1 = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_gate_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	            
	             # File: /tmp/ipykernel_331427/1346670402.py:18 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:65 in backward, code: grad_A = torch.matmul(grad_output, F.dequantize_4bit(B, ctx.state).to(grad_output.dtype).t()) # NOTE the transposition
	            to: "f16[2048, 8192][1, 2048]cuda:0" = t.to(torch.float16);  t = None
	            t_1: "f16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            grad_A: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.matmul(grad_output, t_1);  grad_output = t_1 = None
	            
	            # No stacktrace found for following nodes
	            _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None
	            return (grad_A, None)
	            
	    class fwd_body_1(torch.nn.Module):
	        def forward(self, ctx, A: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", B: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s7)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s6)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            a = A
	            b = B
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:21 in forward, code: if prod(A.shape) == 0:
	            size = a.size()
	            getitem = size[0];  getitem = None
	            getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
	            getitem_2 = size[2];  getitem_2 = None
	            prod: "Sym(2048*s0)" = math_prod(size);  size = None
	            eq: "Sym(False)" = prod == 0;  prod = eq = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s7/2))" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s6*s7)" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s6*s7/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  rshift = mul = rshift_1 = None
	            
	             # File: /tmp/ipykernel_331427/1346670402.py:18 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:34 in forward, code: output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias) # NOTE the transposition
	            to: "bf16[2048, 8192][1, 2048]cuda:0" = t.to(torch.bfloat16);  t = None
	            t_1: "bf16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch._C._nn.linear(a, t_1, None);  a = t_1 = None
	            return (output, [l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_])
	            
	    class bwd_body_1(torch.nn.Module):
	        def forward(self, ctx, grad_output: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s7)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s6)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", b: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(8192)", l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(2048)"):
	            # No stacktrace found for following nodes
	            _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s7/2))" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s6*s7)" = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize;  l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_blocksize = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s6*s7/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_offset = b = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = rshift = mul = rshift_1 = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_up_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	            
	             # File: /tmp/ipykernel_331427/1346670402.py:18 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[2048, 8192][1, 2048]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:65 in backward, code: grad_A = torch.matmul(grad_output, F.dequantize_4bit(B, ctx.state).to(grad_output.dtype).t()) # NOTE the transposition
	            to: "f16[2048, 8192][1, 2048]cuda:0" = t.to(torch.float16);  t = None
	            t_1: "f16[8192, 2048][2048, 1]cuda:0" = to.t();  to = None
	            grad_A: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.matmul(grad_output, t_1);  grad_output = t_1 = None
	            
	            # No stacktrace found for following nodes
	            _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None
	            return (grad_A, None)
	            
	    class fwd_body_2(torch.nn.Module):
	        def forward(self, ctx, A: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", B: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s11)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s10)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(2048)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(8192)"):
	            a = A
	            b = B
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:21 in forward, code: if prod(A.shape) == 0:
	            size = a.size()
	            getitem = size[0];  getitem = None
	            getitem_1: "Sym(s0)" = size[1];  getitem_1 = None
	            getitem_2 = size[2];  getitem_2 = None
	            prod: "Sym(8192*s0)" = math_prod(size);  size = None
	            eq: "Sym(False)" = prod == 0;  prod = eq = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s11/2))" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s10*s11)" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s10*s11/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  rshift = mul = rshift_1 = None
	            
	             # File: /tmp/ipykernel_331427/1346670402.py:18 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[8192, 2048][1, 8192]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:34 in forward, code: output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias) # NOTE the transposition
	            to: "bf16[8192, 2048][1, 8192]cuda:0" = t.to(torch.bfloat16);  t = None
	            t_1: "bf16[2048, 8192][8192, 1]cuda:0" = to.t();  to = None
	            output: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch._C._nn.linear(a, t_1, None);  a = t_1 = None
	            return (output, [l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_])
	            
	    class bwd_body_2(torch.nn.Module):
	        def forward(self, ctx, grad_output: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize: "Sym(s11)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize: "Sym(s10)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax: "u8[262144][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code: "f32[256][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax: "f32[1024][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset: "f16[][]cuda:0", b: "u8[1, 8388608][1, 1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code: "f32[16][1]cuda:0", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_: "Sym(2048)", l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_: "Sym(8192)"):
	            # No stacktrace found for following nodes
	            _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:335 in fused_dequantize, code: half_values_block_size = values_block_size >> 1
	            rshift: "Sym(floor(s11/2))" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize >> 1
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:358 in fused_dequantize, code: TRITON_BLOCK_SIZE = absmax_block_size * values_block_size
	            mul: "Sym(s10*s11)" = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize * l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize;  l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_blocksize = None
	            
	             # File: /home/tom/unsloth-challenges/submit/Problem_C/kernels.py:359 in fused_dequantize, code: packed_block_size = TRITON_BLOCK_SIZE >> 1
	            rshift_1: "Sym(floor(s10*s11/2))" = mul >> 1
	            
	             # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)
	            output_ptr: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset, b, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code, 16777216, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize, rshift, mul, rshift_1, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_, l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_, torch.bfloat16);  l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_absmax = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_offset = b = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_code = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_state2_blocksize = rshift = mul = rshift_1 = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_0_ = l_self_modules_down_proj_modules_base_layer_parameters_weight_quant_state_shape_1_ = None
	            
	             # File: /tmp/ipykernel_331427/1346670402.py:18 in augmented_dequantize_4bit, code: return fused_dequantize(A, quant_state).t()
	            t: "bf16[8192, 2048][1, 8192]cuda:0" = output_ptr.t();  output_ptr = None
	            
	             # File: /tmp/ipykernel_331427/2489781049.py:65 in backward, code: grad_A = torch.matmul(grad_output, F.dequantize_4bit(B, ctx.state).to(grad_output.dtype).t()) # NOTE the transposition
	            to: "f16[8192, 2048][1, 8192]cuda:0" = t.to(torch.float16);  t = None
	            t_1: "f16[2048, 8192][8192, 1]cuda:0" = to.t();  to = None
	            grad_A: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.matmul(grad_output, t_1);  grad_output = t_1 = None
	            
	            # No stacktrace found for following nodes
	            _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None
	            return (grad_A, None)
	            
V0326 23:52:06.724000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "1fc6b24b58e15097d42008df820b889c"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993526724000.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.724000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "6100f38baf8c75aa39925771bce3fdc2"}
	{
	"name": "backend_compile",
	"ts": 1742993526724000.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.737000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "63c39dbf0283f9e30d9c0c0fd0a7a350"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993526737445.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:06.981000 331427 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "9d573e932b412c71350208d5afea101b"}
	class joint_helper(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "Sym(s0)"; primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"; primals_3: "u8[8388608, 1][1, 1]cuda:0"; primals_4: "Sym(s2)"; primals_5: "Sym(s3)"; primals_6: "u8[262144][1]cuda:0"; primals_7: "f32[256][1]cuda:0"; primals_8: "f32[1024][1]cuda:0"; primals_9: "f16[][]cuda:0"; primals_10: "f32[16][1]cuda:0"; primals_11: "Sym(2048)"; primals_12: "Sym(8192)"; primals_13: "f32[32, 2048][2048, 1]cuda:0"; primals_14: "f32[8192, 32][32, 1]cuda:0"; primals_15: "u8[8388608, 1][1, 1]cuda:0"; primals_16: "Sym(s6)"; primals_17: "Sym(s7)"; primals_18: "u8[262144][1]cuda:0"; primals_19: "f32[256][1]cuda:0"; primals_20: "f32[1024][1]cuda:0"; primals_21: "f16[][]cuda:0"; primals_22: "f32[16][1]cuda:0"; primals_23: "Sym(2048)"; primals_24: "Sym(8192)"; primals_25: "f32[32, 2048][2048, 1]cuda:0"; primals_26: "f32[8192, 32][32, 1]cuda:0"; primals_27: "u8[8388608, 1][1, 1]cuda:0"; primals_28: "Sym(s10)"; primals_29: "Sym(s11)"; primals_30: "u8[262144][1]cuda:0"; primals_31: "f32[256][1]cuda:0"; primals_32: "f32[1024][1]cuda:0"; primals_33: "f16[][]cuda:0"; primals_34: "f32[16][1]cuda:0"; primals_35: "Sym(8192)"; primals_36: "Sym(2048)"; primals_37: "f32[32, 8192][8192, 1]cuda:0"; primals_38: "f32[2048, 32][32, 1]cuda:0"; tangents_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.bfloat16)
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift: "Sym(floor(s3/2))" = primals_5 >> 1
	        mul_4: "Sym(s2*s3)" = primals_4 * primals_5;  primals_5 = None
	        rshift_1: "Sym(floor(s2*s3/2))" = mul_4 >> 1
	        fused_dequantize_op: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16)
	        permute_1: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None
	        permute_2: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0]);  permute_1 = None
	        convert_element_type_1: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None
	        convert_element_type_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type, torch.float16);  convert_element_type = None
	        permute_3: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None
	        view: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2, [primals_1, 2048]);  convert_element_type_2 = None
	        mm: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view, permute_3);  view = permute_3 = None
	        view_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        clone: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.clone.default(view_1);  view_1 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_5: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None
	        permute_4: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None
	        view_2: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(primals_2, [primals_1, 2048])
	        mm_1: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_2, permute_4)
	        view_3: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None
	        convert_element_type_8: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None
	        permute_5: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None
	        view_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None
	        mm_2: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_4, permute_5)
	        view_5: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None
	        mul_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(clone, mul_39);  clone = mul_39 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        convert_element_type_11: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_13: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.bfloat16)
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_6: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_2: "Sym(floor(s7/2))" = primals_17 >> 1
	        mul_54: "Sym(s6*s7)" = primals_16 * primals_17;  primals_17 = None
	        rshift_3: "Sym(floor(s6*s7/2))" = mul_54 >> 1
	        fused_dequantize_op_1: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16)
	        permute_7: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None
	        permute_8: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0]);  permute_7 = None
	        convert_element_type_14: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None
	        convert_element_type_15: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_13, torch.float16);  convert_element_type_13 = None
	        permute_9: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None
	        view_6: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_15, [primals_1, 2048]);  convert_element_type_15 = None
	        mm_3: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_6, permute_9);  view_6 = permute_9 = None
	        view_7: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        clone_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.clone.default(view_7);  view_7 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_18: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None
	        permute_10: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None
	        view_8: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None
	        mm_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_8, permute_10)
	        view_9: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None
	        convert_element_type_21: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None
	        permute_11: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None
	        view_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None
	        mm_5: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_10, permute_11)
	        view_11: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None
	        mul_87: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_85: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(clone_1, mul_87);  clone_1 = mul_87 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_94: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85)
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_24: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_94, torch.bfloat16)
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_12: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_4: "Sym(floor(s11/2))" = primals_29 >> 1
	        mul_102: "Sym(s10*s11)" = primals_28 * primals_29;  primals_29 = None
	        rshift_5: "Sym(floor(s10*s11/2))" = mul_102 >> 1
	        fused_dequantize_op_2: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16)
	        permute_13: "bf16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None
	        permute_14: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_13, [1, 0]);  permute_13 = None
	        convert_element_type_25: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None
	        convert_element_type_26: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_24, torch.float16);  convert_element_type_24 = None
	        permute_15: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None
	        view_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_26, [primals_1, 8192]);  convert_element_type_26 = None
	        mm_6: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None
	        view_13: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502 in forward, code: result = result.clone()
	        clone_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.clone.default(view_13);  view_13 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_29: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None
	        permute_16: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None
	        view_14: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None
	        mm_7: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_14, permute_16)
	        view_15: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None
	        convert_element_type_32: "f16[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None
	        permute_17: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None
	        view_16: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None
	        mm_8: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_16, permute_17)
	        view_17: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None
	        mul_137: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_131: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(clone_2, mul_137);  clone_2 = mul_137 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_144: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, 2.0)
	        view_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None
	        permute_18: "f16[2048, s0][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_18, [1, 0])
	        mm_9: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None
	        permute_19: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None
	        permute_20: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None
	        mm_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None
	        view_19: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None
	        permute_21: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None
	        convert_element_type_39: "f32[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None
	        view_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None
	        permute_22: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_20, [1, 0])
	        mm_11: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None
	        permute_23: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None
	        permute_24: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None
	        mm_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None
	        view_21: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None
	        permute_25: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None
	        convert_element_type_44: "f32[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        fused_dequantize_op_3: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None
	        permute_26: "bf16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_3, [1, 0]);  fused_dequantize_op_3 = None
	        convert_element_type_45: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.prims.convert_element_type.default(permute_26, torch.float16);  permute_26 = None
	        permute_27: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None
	        view_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None
	        mm_13: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None
	        view_23: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None
	        convert_element_type_48: "bf16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_23, torch.bfloat16);  view_23 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_49: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_48, torch.float16);  convert_element_type_48 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_135: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_21, convert_element_type_49);  view_21 = convert_element_type_49 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_146: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None
	        mul_147: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_148: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_146, 2.0)
	        view_24: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None
	        permute_28: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_24, [1, 0])
	        mm_14: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None
	        permute_29: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None
	        permute_30: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None
	        mm_15: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None
	        view_25: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None
	        permute_31: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None
	        convert_element_type_54: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None
	        view_26: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None
	        permute_32: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_26, [1, 0])
	        mm_16: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_32, view_8);  permute_32 = view_8 = None
	        permute_33: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None
	        permute_34: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None
	        mm_17: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None
	        view_27: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None
	        permute_35: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None
	        convert_element_type_59: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        fused_dequantize_op_4: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None
	        permute_36: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_4, [1, 0]);  fused_dequantize_op_4 = None
	        convert_element_type_60: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_36, torch.float16);  permute_36 = None
	        permute_37: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None
	        view_28: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None
	        mm_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None
	        view_29: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None
	        convert_element_type_63: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_29, torch.bfloat16);  view_29 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_64: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_63, torch.float16);  convert_element_type_63 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_136: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_27, convert_element_type_64);  view_27 = convert_element_type_64 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        sigmoid_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(add_39)
	        full: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_44: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(full, sigmoid_1);  full = None
	        mul_150: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None
	        add_137: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None
	        mul_151: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None
	        mul_152: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_153: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_152, 2.0)
	        view_30: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None
	        permute_39: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_30, [1, 0])
	        mm_19: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None
	        permute_40: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None
	        permute_41: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None
	        mm_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None
	        view_31: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None
	        permute_42: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None
	        convert_element_type_69: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None
	        view_32: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None
	        permute_43: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_32, [1, 0])
	        mm_21: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None
	        permute_44: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None
	        permute_45: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None
	        mm_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None
	        view_33: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        add_138: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_46: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None
	        convert_element_type_74: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        fused_dequantize_op_5: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None
	        permute_47: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_5, [1, 0]);  fused_dequantize_op_5 = None
	        convert_element_type_75: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_47, torch.float16);  permute_47 = None
	        permute_48: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None
	        view_34: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None
	        mm_23: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None
	        view_35: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None
	        convert_element_type_78: "bf16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_35, torch.bfloat16);  view_35 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_79: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_78, torch.float16);  convert_element_type_78 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_139: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_138, convert_element_type_79);  add_138 = convert_element_type_79 = None
	        return pytree.tree_unflatten([add_131, None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39], self._out_spec)
	        
V0326 23:52:07.045000 331427 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "502e69bfe3710ecee71d8da8593e0422"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "Sym(s0)", primals_2: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0", primals_3: "u8[8388608, 1][1, 1]cuda:0", primals_4: "Sym(s2)", primals_5: "Sym(s3)", primals_6: "u8[262144][1]cuda:0", primals_7: "f32[256][1]cuda:0", primals_8: "f32[1024][1]cuda:0", primals_9: "f16[][]cuda:0", primals_10: "f32[16][1]cuda:0", primals_11: "Sym(2048)", primals_12: "Sym(8192)", primals_13: "f32[32, 2048][2048, 1]cuda:0", primals_14: "f32[8192, 32][32, 1]cuda:0", primals_15: "u8[8388608, 1][1, 1]cuda:0", primals_16: "Sym(s6)", primals_17: "Sym(s7)", primals_18: "u8[262144][1]cuda:0", primals_19: "f32[256][1]cuda:0", primals_20: "f32[1024][1]cuda:0", primals_21: "f16[][]cuda:0", primals_22: "f32[16][1]cuda:0", primals_23: "Sym(2048)", primals_24: "Sym(8192)", primals_25: "f32[32, 2048][2048, 1]cuda:0", primals_26: "f32[8192, 32][32, 1]cuda:0", primals_27: "u8[8388608, 1][1, 1]cuda:0", primals_28: "Sym(s10)", primals_29: "Sym(s11)", primals_30: "u8[262144][1]cuda:0", primals_31: "f32[256][1]cuda:0", primals_32: "f32[1024][1]cuda:0", primals_33: "f16[][]cuda:0", primals_34: "f32[16][1]cuda:0", primals_35: "Sym(8192)", primals_36: "Sym(2048)", primals_37: "f32[32, 8192][8192, 1]cuda:0", primals_38: "f32[2048, 32][32, 1]cuda:0"):
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift: "Sym(floor(s3/2))" = primals_5 >> 1
	        mul_4: "Sym(s2*s3)" = primals_4 * primals_5;  primals_5 = None
	        rshift_1: "Sym(floor(s2*s3/2))" = mul_4 >> 1
	        fused_dequantize_op: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None
	        permute_1: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None
	        permute_2: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0])
	        convert_element_type_1: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None
	        convert_element_type_default_5: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.float16)
	        permute_3: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None
	        view: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_default_5, [primals_1, 2048]);  convert_element_type_default_5 = None
	        mm: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view, permute_3);  permute_3 = None
	        view_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_5: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None
	        permute_4: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None
	        view_2: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None
	        mm_1: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_2, permute_4)
	        view_3: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None
	        convert_element_type_8: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None
	        permute_5: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None
	        view_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None
	        mm_2: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_4, permute_5)
	        view_5: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None
	        mul_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1, mul_39);  view_1 = mul_39 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        convert_element_type_11: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_6: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_2: "Sym(floor(s7/2))" = primals_17 >> 1
	        mul_54: "Sym(s6*s7)" = primals_16 * primals_17;  primals_17 = None
	        rshift_3: "Sym(floor(s6*s7/2))" = mul_54 >> 1
	        fused_dequantize_op_1: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None
	        permute_7: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None
	        permute_8: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0])
	        convert_element_type_14: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None
	        permute_9: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None
	        mm_3: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view, permute_9);  view = permute_9 = None
	        view_7: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_18: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None
	        permute_10: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None
	        mm_4: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_2, permute_10)
	        view_9: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None
	        convert_element_type_21: "f16[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None
	        permute_11: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None
	        view_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None
	        mm_5: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_10, permute_11)
	        view_11: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None
	        mul_87: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_85: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_7, mul_87);  view_7 = mul_87 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        mul_94: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85);  convert_element_type_12 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:79 in get_data_transposed, code: return param.some_data.t()
	        permute_12: "u8[1, 8388608][1, 1]cuda:0" = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        rshift_4: "Sym(floor(s11/2))" = primals_29 >> 1
	        mul_102: "Sym(s10*s11)" = primals_28 * primals_29;  primals_29 = None
	        rshift_5: "Sym(floor(s10*s11/2))" = mul_102 >> 1
	        fused_dequantize_op_2: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None
	        permute_13: "bf16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None
	        permute_14: "bf16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_13, [1, 0])
	        convert_element_type_25: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None
	        convert_element_type_default_3: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_94, torch.float16)
	        permute_15: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None
	        view_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_default_3, [primals_1, 8192]);  convert_element_type_default_3 = None
	        mm_6: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None
	        view_13: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        convert_element_type_29: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None
	        permute_16: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None
	        view_14: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None
	        mm_7: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_14, permute_16)
	        view_15: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None
	        convert_element_type_32: "f16[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None
	        permute_17: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None
	        view_16: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None
	        mm_8: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_16, permute_17)
	        view_17: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None
	        mul_137: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:536 in forward, code: result = result + output
	        add_131: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_13, mul_137);  view_13 = mul_137 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_20: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None
	        permute_24: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_45: "f16[8192, 2048][1, 8192]cuda:0" = torch.ops.prims.convert_element_type.default(permute_13, torch.float16);  permute_13 = None
	        permute_27: "f16[2048, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_30: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None
	        permute_34: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_60: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_7, torch.float16);  permute_7 = None
	        permute_37: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_41: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None
	        permute_45: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        convert_element_type_75: "f16[2048, 8192][1, 2048]cuda:0" = torch.ops.prims.convert_element_type.default(permute_1, torch.float16);  permute_1 = None
	        permute_48: "f16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None
	        return (add_131, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, primals_1)
	        
V0326 23:52:07.048000 331427 .venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "e0aad3d7ff152d08541cef6a309ac5cb"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "Sym(s0)", view_2: "f16[s0, 2048][2048, 1]cuda:0", view_4: "f16[s0, 32][32, 1]cuda:0", add_39: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", view_10: "f16[s0, 32][32, 1]cuda:0", add_85: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0", view_14: "f16[s0, 8192][8192, 1]cuda:0", view_16: "f16[s0, 32][32, 1]cuda:0", permute_20: "f16[2048, 32][32, 1]cuda:0", permute_24: "f16[32, 8192][8192, 1]cuda:0", permute_27: "f16[2048, 8192][8192, 1]cuda:0", permute_30: "f16[8192, 32][32, 1]cuda:0", permute_34: "f16[32, 2048][2048, 1]cuda:0", permute_37: "f16[8192, 2048][2048, 1]cuda:0", permute_41: "f16[8192, 32][32, 1]cuda:0", permute_45: "f16[32, 2048][2048, 1]cuda:0", permute_48: "f16[8192, 2048][2048, 1]cuda:0", tangents_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0"):
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_144: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, 2.0)
	        view_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None
	        permute_18: "f16[2048, s0][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_18, [1, 0])
	        mm_9: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None
	        permute_19: "f16[32, 2048][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None
	        mm_10: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None
	        view_19: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None
	        permute_21: "f16[2048, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None
	        convert_element_type_39: "f32[2048, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None
	        view_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None
	        permute_22: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_20, [1, 0])
	        mm_11: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None
	        permute_23: "f16[8192, 32][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None
	        mm_12: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None
	        view_21: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None
	        permute_25: "f16[32, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None
	        convert_element_type_44: "f32[32, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None
	        mm_13: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None
	        view_23: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_default_2: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_23, torch.float16);  view_23 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_135: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_21, convert_element_type_default_2);  view_21 = convert_element_type_default_2 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        convert_element_type_11: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_39, torch.float32)
	        sigmoid: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_11)
	        mul_46: "f32[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None
	        convert_element_type_12: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
	        mul_146: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None
	        mul_147: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_148: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_146, 2.0)
	        view_24: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None
	        permute_28: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_24, [1, 0])
	        mm_14: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None
	        permute_29: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None
	        mm_15: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None
	        view_25: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None
	        permute_31: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None
	        convert_element_type_54: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None
	        view_26: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None
	        permute_32: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_26, [1, 0])
	        mm_16: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_32, view_2);  permute_32 = None
	        permute_33: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None
	        mm_17: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None
	        view_27: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None
	        permute_35: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None
	        convert_element_type_59: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_28: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None
	        mm_18: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None
	        view_29: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_default_1: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_29, torch.float16);  view_29 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_136: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_27, convert_element_type_default_1);  view_27 = convert_element_type_default_1 = None
	        
	         # File: /tmp/ipykernel_331427/318589162.py:12 in compiled_llama_mlp, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
	        sigmoid_1: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sigmoid.default(add_39)
	        full_default: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_44: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_default, sigmoid_1);  full_default = None
	        mul_150: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None
	        add_137: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None
	        mul_151: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None
	        mul_152: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        mul_153: "f16[1, s0, 8192][8192*s0, 8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_152, 2.0)
	        view_30: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None
	        permute_39: "f16[8192, s0][1, 8192]cuda:0" = torch.ops.aten.permute.default(view_30, [1, 0])
	        mm_19: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None
	        permute_40: "f16[32, 8192][1, 32]cuda:0" = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None
	        mm_20: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None
	        view_31: "f16[1, s0, 32][32*s0, 32, 1]cuda:0" = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None
	        permute_42: "f16[8192, 32][32, 1]cuda:0" = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None
	        convert_element_type_69: "f32[8192, 32][32, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None
	        view_32: "f16[s0, 32][32, 1]cuda:0" = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None
	        permute_43: "f16[32, s0][1, 32]cuda:0" = torch.ops.aten.permute.default(view_32, [1, 0])
	        mm_21: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None
	        permute_44: "f16[2048, 32][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None
	        mm_22: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None
	        view_33: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        add_138: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None
	        
	         # File: /home/tom/unsloth-challenges/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:518 in forward, code: output = lora_B(lora_A(dropout(x))) * scaling
	        permute_46: "f16[32, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None
	        convert_element_type_74: "f32[32, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:110 in inner_transpose_forward, code: return TransposeBMatMul4Bit.apply(A, B, out, bias, quant_state).to(inp_dtype)
	        view_34: "f16[s0, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None
	        mm_23: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None
	        view_35: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        convert_element_type_default: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_35, torch.float16);  view_35 = None
	        
	         # File: /tmp/ipykernel_331427/2489781049.py:94 in inner_transpose_forward, code: x = x.to(self.compute_dtype)
	        add_139: "f16[1, s0, 2048][2048*s0, 2048, 1]cuda:0" = torch.ops.aten.add.Tensor(add_138, convert_element_type_default);  add_138 = convert_element_type_default = None
	        return (None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39)
	        
V0326 23:52:07.049000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "8e02ab37d5470d71d3c39ac2c7d41212"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993527048929.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.049000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "9d9fda1a9f8bc6d48cc5c060ec246971"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993527049432.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.049000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "316f4f3327126362f8629ace0748ce24"}
	{
	"name": "inductor_compile",
	"ts": 1742993527049432.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.064000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/6t/c6ti5avby2m7pt3ut73u7cv6pskab7oqrj32flivbrhi5omzahyd.py"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "2cfff416bd40de843c972c9ab081a285"}
	# AOT ID: ['5_forward']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/tc/ctc3wd5ie7s2fjcr2f4u2qxws22nb2vss5um5266to6nceu4xt3o.py
	# Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [mylib.fused_dequantize_op]
	# Source node to ATen node mapping:
	#   autograd_function_apply => fused_dequantize_op
	# Graph fragment:
	#   %fused_dequantize_op : [num_users=1] = call_function[target=torch.ops.mylib.fused_dequantize_op.default](args = (%primals_6, %primals_7, %primals_8, %primals_9, %permute, %primals_10, 16777216, %primals_4, %rshift, %mul_4, %rshift_1, 8192, 2048, torch.bfloat16), kwargs = {})
	triton_poi_fused_fused_dequantize_op_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {2: 1}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=(2,))]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_fused_dequantize_op_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 1
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    tmp0 = tl.load(in_ptr0 + (0)).to(tl.float32)
	    tmp1 = tl.broadcast_to(tmp0, [XBLOCK])
	    tl.store(out_ptr0 + (tl.full([XBLOCK], 0, tl.int32)), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/4i/c4ighga7fod2ed36hdz67v2ofqmyk7mlh2kc24zdiinrkgsdjog7.py
	# Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	#   autograd_function_apply => convert_element_type_1
	# Graph fragment:
	#   %convert_element_type_1 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_2, torch.float16), kwargs = {})
	triton_poi_fused__to_copy_1 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[16777216], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*bf16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_1', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 16777216
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/dm/cdm5ebpcdjlaqnrcavlevwhnlhakhmbh235waumdxi2dlz4oqdc3.py
	# Topologically Sorted Source Nodes: [linear], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	#   linear => convert_element_type_5, permute_4
	# Graph fragment:
	#   %convert_element_type_5 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_13, torch.float16), kwargs = {})
	#   %permute_4 : [num_users=2] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_5, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_2 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[65536], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp32', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_2', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 65536
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/4v/c4vy7sd4hdenvrautochni2fe3ce3tkssjud74ei4t45g5jo4bxq.py
	# Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	#   linear_1 => convert_element_type_8, permute_5
	# Graph fragment:
	#   %convert_element_type_8 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_14, torch.float16), kwargs = {})
	#   %permute_5 : [num_users=2] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_8, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_3 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp32', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_3', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 262144
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/5b/c5bxmvhk7qwah3r5zprbeosehsg4smud74ljqbertg7zky4hs4tq.py
	# Topologically Sorted Source Nodes: [output, result_2, silu, output_1, result_5, mul_2], Original ATen: [aten.mul, aten.add, aten.silu]
	# Source node to ATen node mapping:
	#   mul_2 => mul_94
	#   output => mul_39
	#   output_1 => mul_87
	#   result_2 => add_39
	#   result_5 => add_85
	#   silu => convert_element_type_11, convert_element_type_12, mul_46, sigmoid
	# Graph fragment:
	#   %mul_39 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_5, 2.0), kwargs = {})
	#   %add_39 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_1, %mul_39), kwargs = {})
	#   %convert_element_type_11 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_39, torch.float32), kwargs = {})
	#   %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_11,), kwargs = {})
	#   %mul_46 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_11, %sigmoid), kwargs = {})
	#   %convert_element_type_12 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_46, torch.float16), kwargs = {})
	#   %mul_87 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_11, 2.0), kwargs = {})
	#   %add_85 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_7, %mul_87), kwargs = {})
	#   %mul_94 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_12, %add_85), kwargs = {})
	triton_poi_fused_add_mul_silu_4 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1048576], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: '*fp16', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mul_silu_4', 'mutated_arg_names': ['in_out_ptr0', 'in_out_ptr1'], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_out_ptr0, in_out_ptr1, in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_out_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp5 = tl.load(in_out_ptr1 + (x0), None).to(tl.float32)
	    tmp6 = tl.load(in_ptr1 + (x0), None).to(tl.float32)
	    tmp2 = 2.0
	    tmp3 = tmp1 * tmp2
	    tmp4 = tmp0 + tmp3
	    tmp7 = tmp6 * tmp2
	    tmp8 = tmp5 + tmp7
	    tmp9 = tmp4.to(tl.float32)
	    tmp10 = tl.sigmoid(tmp9)
	    tmp11 = tmp9 * tmp10
	    tmp12 = tmp11.to(tl.float32)
	    tmp13 = tmp12 * tmp8
	    tl.store(in_out_ptr0 + (x0), tmp4, None)
	    tl.store(in_out_ptr1 + (x0), tmp8, None)
	    tl.store(out_ptr0 + (x0), tmp13, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/3b/c3bktvkn3n6hu4v7tiwk7rpos2wp4lajxxzrdgoicggbyxk5d3cb.py
	# Topologically Sorted Source Nodes: [output_2, result_8], Original ATen: [aten.mul, aten.add]
	# Source node to ATen node mapping:
	#   output_2 => mul_137
	#   result_8 => add_131
	# Graph fragment:
	#   %mul_137 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%view_17, 2.0), kwargs = {})
	#   %add_131 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_13, %mul_137), kwargs = {})
	triton_poi_fused_add_mul_5 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mul_5', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_out_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp2 = 2.0
	    tmp3 = tmp1 * tmp2
	    tmp4 = tmp0 + tmp3
	    tl.store(in_out_ptr0 + (x0), tmp4, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/j7/cj776okodjzzoh7cfozhudvywfzftzc6cabab2mmcdjjege6gbpi.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_45 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_13, torch.float16), kwargs = {})
	triton_poi_fused__to_copy_6 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[8192, 2048], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*bf16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_6', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 8192
	    xnumel = 2048
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (8192*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (2048*y0)), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/mi/cmiz6hpfs7acuhgi57mdp7dpphsyd43zzboziig5wum6ghgma4e2.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_45 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_13, torch.float16), kwargs = {})
	#   %permute_27 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_45, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_7 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[2048, 8192], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_7', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 2048
	    xnumel = 8192
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (2048*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (8192*y0)), tmp0, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/vd/cvds2jaf5sirynr2qnt7rpujbamyslbkrjowetcdyb6ep4yhq4gq.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_60 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_7, torch.float16), kwargs = {})
	triton_poi_fused__to_copy_8 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[2048, 8192], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*bf16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_8', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 2048
	    xnumel = 8192
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (2048*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (8192*y0)), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/ai/caikh3jmq5xv3uwakclrkwfn3zmg36elfcn5skhch7uvaivem3uj.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_60 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_7, torch.float16), kwargs = {})
	#   %permute_37 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%convert_element_type_60, [1, 0]), kwargs = {})
	triton_poi_fused__to_copy_t_9 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[8192, 2048], tile_hint=TileHint.SQUARE,
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_t_9', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
	    ynumel = 8192
	    xnumel = 2048
	    yoffset = tl.program_id(1) * YBLOCK
	    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
	    ymask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
	    xmask = tl.full([XBLOCK, YBLOCK], True, tl.int1)
	    x1 = xindex
	    y0 = yindex
	    tmp0 = tl.load(in_ptr0 + (y0 + (8192*x1)), None, eviction_policy='evict_last').to(tl.float32)
	    tl.store(out_ptr0 + (x1 + (2048*y0)), tmp0, None)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38 = args
	    args.clear()
	    s0 = primals_1
	    s2 = primals_4
	    s3 = primals_5
	    s6 = primals_16
	    s7 = primals_17
	    s10 = primals_28
	    s11 = primals_29
	    assert_size_stride(primals_2, (1, s0, 2048), (2048*s0, 2048, 1))
	    assert_size_stride(primals_3, (8388608, 1), (1, 1))
	    assert_size_stride(primals_6, (262144, ), (1, ))
	    assert_size_stride(primals_7, (256, ), (1, ))
	    assert_size_stride(primals_8, (1024, ), (1, ))
	    assert_size_stride(primals_9, (), ())
	    assert_size_stride(primals_10, (16, ), (1, ))
	    assert_size_stride(primals_13, (32, 2048), (2048, 1))
	    assert_size_stride(primals_14, (8192, 32), (32, 1))
	    assert_size_stride(primals_15, (8388608, 1), (1, 1))
	    assert_size_stride(primals_18, (262144, ), (1, ))
	    assert_size_stride(primals_19, (256, ), (1, ))
	    assert_size_stride(primals_20, (1024, ), (1, ))
	    assert_size_stride(primals_21, (), ())
	    assert_size_stride(primals_22, (16, ), (1, ))
	    assert_size_stride(primals_25, (32, 2048), (2048, 1))
	    assert_size_stride(primals_26, (8192, 32), (32, 1))
	    assert_size_stride(primals_27, (8388608, 1), (1, 1))
	    assert_size_stride(primals_30, (262144, ), (1, ))
	    assert_size_stride(primals_31, (256, ), (1, ))
	    assert_size_stride(primals_32, (1024, ), (1, ))
	    assert_size_stride(primals_33, (), ())
	    assert_size_stride(primals_34, (16, ), (1, ))
	    assert_size_stride(primals_37, (32, 8192), (8192, 1))
	    assert_size_stride(primals_38, (2048, 32), (32, 1))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((), (), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [mylib.fused_dequantize_op]
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_fused_dequantize_op_0.run(primals_9, buf0, 1, grid=grid(1), stream=stream0)
	        del primals_9
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [mylib.fused_dequantize_op]
	        buf1 = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, buf0, reinterpret_tensor(primals_3, (1, 8388608), (1, 1), 0), primals_10, 16777216, s2, math.floor((1/2)*s3), s2*s3, math.floor((1/2)*s2*s3), 8192, 2048, torch.bfloat16)
	        del primals_10
	        del primals_3
	        del primals_6
	        del primals_7
	        del primals_8
	        buf2 = buf1
	        del buf1
	        buf3 = empty_strided_cuda((8192, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf2, buf3, 16777216, grid=grid(16777216), stream=stream0)
	        buf4 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), reinterpret_tensor(buf3, (2048, 8192), (1, 2048), 0), out=buf4)
	        buf5 = empty_strided_cuda((2048, 32), (1, 2048), torch.float16)
	        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_2.run(primals_13, buf5, 65536, grid=grid(65536), stream=stream0)
	        del primals_13
	        buf6 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), buf5, out=buf6)
	        buf7 = empty_strided_cuda((32, 8192), (1, 32), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_3.run(primals_14, buf7, 262144, grid=grid(262144), stream=stream0)
	        del primals_14
	        buf8 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten.mm]
	        extern_kernels.mm(buf6, buf7, out=buf8)
	        buf10 = buf0; del buf0  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [mylib.fused_dequantize_op]
	        triton_poi_fused_fused_dequantize_op_0.run(primals_21, buf10, 1, grid=grid(1), stream=stream0)
	        del primals_21
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [mylib.fused_dequantize_op]
	        buf11 = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, buf10, reinterpret_tensor(primals_15, (1, 8388608), (1, 1), 0), primals_22, 16777216, s6, math.floor((1/2)*s7), s6*s7, math.floor((1/2)*s6*s7), 8192, 2048, torch.bfloat16)
	        del primals_15
	        del primals_18
	        del primals_19
	        del primals_20
	        del primals_22
	        buf12 = buf11
	        del buf11
	        buf13 = buf3; del buf3  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf12, buf13, 16777216, grid=grid(16777216), stream=stream0)
	        buf14 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply_1], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), reinterpret_tensor(buf13, (2048, 8192), (1, 2048), 0), out=buf14)
	        buf15 = empty_strided_cuda((2048, 32), (1, 2048), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_2], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_2.run(primals_25, buf15, 65536, grid=grid(65536), stream=stream0)
	        del primals_25
	        buf16 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_2], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), buf15, out=buf16)
	        buf17 = empty_strided_cuda((32, 8192), (1, 32), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_3], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_3.run(primals_26, buf17, 262144, grid=grid(262144), stream=stream0)
	        del primals_26
	        buf18 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_3], Original ATen: [aten.mm]
	        extern_kernels.mm(buf16, buf17, out=buf18)
	        buf9 = reinterpret_tensor(buf4, (1, s0, 8192), (8192*s0, 8192, 1), 0); del buf4  # reuse
	        buf19 = reinterpret_tensor(buf14, (1, s0, 8192), (8192*s0, 8192, 1), 0); del buf14  # reuse
	        buf23 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [output, result_2, silu, output_1, result_5, mul_2], Original ATen: [aten.mul, aten.add, aten.silu]
	        triton_poi_fused_add_mul_silu_4_xnumel = 8192*s0
	        triton_poi_fused_add_mul_silu_4.run(buf9, buf19, buf8, buf18, buf23, triton_poi_fused_add_mul_silu_4_xnumel, grid=grid(triton_poi_fused_add_mul_silu_4_xnumel), stream=stream0)
	        del buf18
	        del buf8
	        buf20 = buf10; del buf10  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [mylib.fused_dequantize_op]
	        triton_poi_fused_fused_dequantize_op_0.run(primals_33, buf20, 1, grid=grid(1), stream=stream0)
	        del primals_33
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [mylib.fused_dequantize_op]
	        buf21 = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, buf20, reinterpret_tensor(primals_27, (1, 8388608), (1, 1), 0), primals_34, 16777216, s10, math.floor((1/2)*s11), s10*s11, math.floor((1/2)*s10*s11), 2048, 8192, torch.bfloat16)
	        del buf20
	        del primals_27
	        del primals_30
	        del primals_31
	        del primals_32
	        del primals_34
	        buf22 = buf21
	        del buf21
	        buf24 = reinterpret_tensor(buf13, (2048, 8192), (8192, 1), 0); del buf13  # reuse
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf22, buf24, 16777216, grid=grid(16777216), stream=stream0)
	        buf25 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [autograd_function_apply_2], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf23, (s0, 8192), (8192, 1), 0), reinterpret_tensor(buf24, (8192, 2048), (1, 8192), 0), out=buf25)
	        buf26 = empty_strided_cuda((8192, 32), (1, 8192), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_3.run(primals_37, buf26, 262144, grid=grid(262144), stream=stream0)
	        del primals_37
	        buf27 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf23, (s0, 8192), (8192, 1), 0), buf26, out=buf27)
	        buf28 = empty_strided_cuda((32, 2048), (1, 32), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_5], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_2.run(primals_38, buf28, 65536, grid=grid(65536), stream=stream0)
	        del primals_38
	        buf29 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [linear_5], Original ATen: [aten.mm]
	        extern_kernels.mm(buf27, buf28, out=buf29)
	        buf30 = reinterpret_tensor(buf25, (1, s0, 2048), (2048*s0, 2048, 1), 0); del buf25  # reuse
	        # Topologically Sorted Source Nodes: [output_2, result_8], Original ATen: [aten.mul, aten.add]
	        triton_poi_fused_add_mul_5_xnumel = 2048*s0
	        triton_poi_fused_add_mul_5.run(buf30, buf29, triton_poi_fused_add_mul_5_xnumel, grid=grid(triton_poi_fused_add_mul_5_xnumel), stream=stream0)
	        del buf29
	        buf31 = reinterpret_tensor(buf24, (8192, 2048), (2048, 1), 0); del buf24  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_6.run(buf22, buf31, 8192, 2048, grid=grid(8192, 2048), stream=stream0)
	        del buf22
	        buf32 = empty_strided_cuda((2048, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_7.run(buf31, buf32, 2048, 8192, grid=grid(2048, 8192), stream=stream0)
	        buf33 = reinterpret_tensor(buf31, (2048, 8192), (8192, 1), 0); del buf31  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_8.run(buf12, buf33, 2048, 8192, grid=grid(2048, 8192), stream=stream0)
	        del buf12
	        buf34 = empty_strided_cuda((8192, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_9.run(buf33, buf34, 8192, 2048, grid=grid(8192, 2048), stream=stream0)
	        buf35 = buf33; del buf33  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_8.run(buf2, buf35, 2048, 8192, grid=grid(2048, 8192), stream=stream0)
	        del buf2
	        buf36 = empty_strided_cuda((8192, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy, aten.t]
	        triton_poi_fused__to_copy_t_9.run(buf35, buf36, 8192, 2048, grid=grid(8192, 2048), stream=stream0)
	        del buf35
	    return (buf30, reinterpret_tensor(primals_2, (s0, 2048), (2048, 1), 0), buf6, buf9, buf16, buf19, reinterpret_tensor(buf23, (s0, 8192), (8192, 1), 0), buf27, reinterpret_tensor(buf28, (2048, 32), (32, 1), 0), reinterpret_tensor(buf26, (32, 8192), (8192, 1), 0), buf32, reinterpret_tensor(buf17, (8192, 32), (32, 1), 0), reinterpret_tensor(buf15, (32, 2048), (2048, 1), 0), buf34, reinterpret_tensor(buf7, (8192, 32), (32, 1), 0), reinterpret_tensor(buf5, (32, 2048), (2048, 1), 0), buf36, s0, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    primals_1 = 100
	    primals_2 = rand_strided((1, 100, 2048), (204800, 2048, 1), device='cuda:0', dtype=torch.float16)
	    primals_3 = rand_strided((8388608, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    primals_4 = 256
	    primals_5 = 64
	    primals_6 = rand_strided((262144, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    primals_7 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_8 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_9 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    primals_10 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_11 = 2048
	    primals_12 = 8192
	    primals_13 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)
	    primals_14 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float32)
	    primals_15 = rand_strided((8388608, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    primals_16 = 256
	    primals_17 = 64
	    primals_18 = rand_strided((262144, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    primals_19 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_20 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_21 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    primals_22 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_23 = 2048
	    primals_24 = 8192
	    primals_25 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)
	    primals_26 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float32)
	    primals_27 = rand_strided((8388608, 1), (1, 1), device='cuda:0', dtype=torch.uint8)
	    primals_28 = 256
	    primals_29 = 64
	    primals_30 = rand_strided((262144, ), (1, ), device='cuda:0', dtype=torch.uint8)
	    primals_31 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_32 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_33 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
	    primals_34 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)
	    primals_35 = 8192
	    primals_36 = 2048
	    primals_37 = rand_strided((32, 8192), (8192, 1), device='cuda:0', dtype=torch.float32)
	    primals_38 = rand_strided((2048, 32), (32, 1), device='cuda:0', dtype=torch.float32)
	    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:52:07.065000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "aeae71ecba094f7507243b1307116be1"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742993527065245.0,
	"args": {
	"key": "fibzwaj5j2pxd4mgs6jpctih67rusxg2lyckoxqaw3hr5wi42ks3",
	"components": [
	"[iu3kgzgo5x6udk6uhcxvnilg7lheh3rln2fhyjpg5qgapvm76o4] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38):\n    permute = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None\n    rshift = primals_5 >> 1\n    mul_4 = primals_4 * primals_5;  primals_5 = None\n    rshift_1 = mul_4 >> 1\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None\n    permute_1 = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None\n    permute_2 = torch.ops.aten.permute.default(permute_1, [1, 0])\n    convert_element_type_1 = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None\n    convert_element_type_default_5 = torch.ops.prims.convert_element_type.default(primals_2, torch.float16)\n    permute_3 = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None\n    view = torch.ops.aten.view.default(convert_element_type_default_5, [primals_1, 2048]);  convert_element_type_default_5 = None\n    mm = torch.ops.aten.mm.default(view, permute_3);  permute_3 = None\n    view_1 = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None\n    convert_element_type_5 = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None\n    permute_4 = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None\n    view_2 = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None\n    mm_1 = torch.ops.aten.mm.default(view_2, permute_4)\n    view_3 = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None\n    convert_element_type_8 = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None\n    permute_5 = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None\n    view_4 = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None\n    mm_2 = torch.ops.aten.mm.default(view_4, permute_5)\n    view_5 = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None\n    mul_39 = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None\n    add_39 = torch.ops.aten.add.Tensor(view_1, mul_39);  view_1 = mul_39 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    permute_6 = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None\n    rshift_2 = primals_17 >> 1\n    mul_54 = primals_16 * primals_17;  primals_17 = None\n    rshift_3 = mul_54 >> 1\n    fused_dequantize_op_1 = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None\n    permute_7 = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None\n    permute_8 = torch.ops.aten.permute.default(permute_7, [1, 0])\n    convert_element_type_14 = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None\n    permute_9 = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None\n    mm_3 = torch.ops.aten.mm.default(view, permute_9);  view = permute_9 = None\n    view_7 = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None\n    convert_element_type_18 = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None\n    permute_10 = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None\n    mm_4 = torch.ops.aten.mm.default(view_2, permute_10)\n    view_9 = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None\n    convert_element_type_21 = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None\n    permute_11 = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None\n    view_10 = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None\n    mm_5 = torch.ops.aten.mm.default(view_10, permute_11)\n    view_11 = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None\n    mul_87 = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None\n    add_85 = torch.ops.aten.add.Tensor(view_7, mul_87);  view_7 = mul_87 = None\n    mul_94 = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85);  convert_element_type_12 = None\n    permute_12 = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None\n    rshift_4 = primals_29 >> 1\n    mul_102 = primals_28 * primals_29;  primals_29 = None\n    rshift_5 = mul_102 >> 1\n    fused_dequantize_op_2 = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None\n    permute_13 = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None\n    permute_14 = torch.ops.aten.permute.default(permute_13, [1, 0])\n    convert_element_type_25 = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None\n    convert_element_type_default_3 = torch.ops.prims.convert_element_type.default(mul_94, torch.float16)\n    permute_15 = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None\n    view_12 = torch.ops.aten.view.default(convert_element_type_default_3, [primals_1, 8192]);  convert_element_type_default_3 = None\n    mm_6 = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None\n    view_13 = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None\n    convert_element_type_29 = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None\n    permute_16 = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None\n    view_14 = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None\n    mm_7 = torch.ops.aten.mm.default(view_14, permute_16)\n    view_15 = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None\n    convert_element_type_32 = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None\n    permute_17 = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None\n    view_16 = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None\n    mm_8 = torch.ops.aten.mm.default(view_16, permute_17)\n    view_17 = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None\n    mul_137 = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None\n    add_131 = torch.ops.aten.add.Tensor(view_13, mul_137);  view_13 = mul_137 = None\n    permute_20 = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None\n    permute_24 = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None\n    convert_element_type_45 = torch.ops.prims.convert_element_type.default(permute_13, torch.float16);  permute_13 = None\n    permute_27 = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None\n    permute_30 = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None\n    permute_34 = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None\n    convert_element_type_60 = torch.ops.prims.convert_element_type.default(permute_7, torch.float16);  permute_7 = None\n    permute_37 = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None\n    permute_41 = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None\n    permute_45 = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None\n    convert_element_type_75 = torch.ops.prims.convert_element_type.default(permute_1, torch.float16);  permute_1 = None\n    permute_48 = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None\n    return (add_131, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, primals_1)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[2]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[3]: ('s2',)",
	"[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[4]: ('s3',)",
	"[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[5]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[7]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[9]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[10]: ('2048',)",
	"[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[11]: ('8192',)",
	"[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[12]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[13]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[14]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[bp5m6l3n24hn6zbxwmg6wfbnn5g5tujtrpje3hczu5umrrf2qjx] example_inputs[15]: ('s6',)",
	"[i3xrsw5sisbji2ivhzqy3l7zuqzhmvjpsyt77nanlhezk7f6tur] example_inputs[16]: ('s7',)",
	"[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[17]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[18]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[19]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[20]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[21]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[22]: ('2048',)",
	"[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[23]: ('8192',)",
	"[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[24]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[25]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[26]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hj7nkho4e2pgg3rxjgt2xo2ofzqotfhatuhlt6bxcifwqbvipzw] example_inputs[27]: ('s10',)",
	"[kosoqok7aznz5iahzbmrdsrqfq3jkju4dvvd7cfij3xrvzxbmb4] example_inputs[28]: ('s11',)",
	"[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[29]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[30]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[31]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[32]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[33]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[34]: ('8192',)",
	"[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[35]: ('2048',)",
	"[zs5vjd7lduxixuzsgnizmtcmiocu75spefi5z3zyshqbfmva6ge] example_inputs[36]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[chru3fsd44dmnozqmsiynvahv26l3bk7j747je77m6edbjsd2kq] example_inputs[37]: TensorMetadata(dtype=torch.float32, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[34jl2fvzyx6odxtpy45nz66wulvmgs5bvsvvhx2zwehki6ym5kw] fx_kwargs[static_input_idxs]: [2, 5, 6, 7, 8, 9, 12, 13, 14, 17, 18, 19, 20, 21, 24, 25, 26, 29, 30, 31, 32, 33, 36, 37]",
	"[2ruvd6wcktb7xtp52gxtldu3ygrqbnwbiluwcx3yagimk7erlhx] fx_kwargs[user_visible_outputs]: {'add_131': None}",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 688660460,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:52:07.065000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "37a16f91963db61f5f565392fb5de95d"}
	{"key": "fibzwaj5j2pxd4mgs6jpctih67rusxg2lyckoxqaw3hr5wi42ks3", "components": ["[iu3kgzgo5x6udk6uhcxvnilg7lheh3rln2fhyjpg5qgapvm76o4] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38):\n    permute = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None\n    rshift = primals_5 >> 1\n    mul_4 = primals_4 * primals_5;  primals_5 = None\n    rshift_1 = mul_4 >> 1\n    fused_dequantize_op = torch.ops.mylib.fused_dequantize_op.default(primals_6, primals_7, primals_8, primals_9, permute, primals_10, 16777216, primals_4, rshift, mul_4, rshift_1, 8192, 2048, torch.bfloat16);  primals_6 = primals_7 = primals_8 = primals_9 = permute = primals_10 = primals_4 = rshift = mul_4 = rshift_1 = None\n    permute_1 = torch.ops.aten.permute.default(fused_dequantize_op, [1, 0]);  fused_dequantize_op = None\n    permute_2 = torch.ops.aten.permute.default(permute_1, [1, 0])\n    convert_element_type_1 = torch.ops.prims.convert_element_type.default(permute_2, torch.float16);  permute_2 = None\n    convert_element_type_default_5 = torch.ops.prims.convert_element_type.default(primals_2, torch.float16)\n    permute_3 = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None\n    view = torch.ops.aten.view.default(convert_element_type_default_5, [primals_1, 2048]);  convert_element_type_default_5 = None\n    mm = torch.ops.aten.mm.default(view, permute_3);  permute_3 = None\n    view_1 = torch.ops.aten.view.default(mm, [1, primals_1, 8192]);  mm = None\n    convert_element_type_5 = torch.ops.prims.convert_element_type.default(primals_13, torch.float16);  primals_13 = None\n    permute_4 = torch.ops.aten.permute.default(convert_element_type_5, [1, 0]);  convert_element_type_5 = None\n    view_2 = torch.ops.aten.view.default(primals_2, [primals_1, 2048]);  primals_2 = None\n    mm_1 = torch.ops.aten.mm.default(view_2, permute_4)\n    view_3 = torch.ops.aten.view.default(mm_1, [1, primals_1, 32]);  mm_1 = None\n    convert_element_type_8 = torch.ops.prims.convert_element_type.default(primals_14, torch.float16);  primals_14 = None\n    permute_5 = torch.ops.aten.permute.default(convert_element_type_8, [1, 0]);  convert_element_type_8 = None\n    view_4 = torch.ops.aten.view.default(view_3, [primals_1, 32]);  view_3 = None\n    mm_2 = torch.ops.aten.mm.default(view_4, permute_5)\n    view_5 = torch.ops.aten.view.default(mm_2, [1, primals_1, 8192]);  mm_2 = None\n    mul_39 = torch.ops.aten.mul.Tensor(view_5, 2.0);  view_5 = None\n    add_39 = torch.ops.aten.add.Tensor(view_1, mul_39);  view_1 = mul_39 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    permute_6 = torch.ops.aten.permute.default(primals_15, [1, 0]);  primals_15 = None\n    rshift_2 = primals_17 >> 1\n    mul_54 = primals_16 * primals_17;  primals_17 = None\n    rshift_3 = mul_54 >> 1\n    fused_dequantize_op_1 = torch.ops.mylib.fused_dequantize_op.default(primals_18, primals_19, primals_20, primals_21, permute_6, primals_22, 16777216, primals_16, rshift_2, mul_54, rshift_3, 8192, 2048, torch.bfloat16);  primals_18 = primals_19 = primals_20 = primals_21 = permute_6 = primals_22 = primals_16 = rshift_2 = mul_54 = rshift_3 = None\n    permute_7 = torch.ops.aten.permute.default(fused_dequantize_op_1, [1, 0]);  fused_dequantize_op_1 = None\n    permute_8 = torch.ops.aten.permute.default(permute_7, [1, 0])\n    convert_element_type_14 = torch.ops.prims.convert_element_type.default(permute_8, torch.float16);  permute_8 = None\n    permute_9 = torch.ops.aten.permute.default(convert_element_type_14, [1, 0]);  convert_element_type_14 = None\n    mm_3 = torch.ops.aten.mm.default(view, permute_9);  view = permute_9 = None\n    view_7 = torch.ops.aten.view.default(mm_3, [1, primals_1, 8192]);  mm_3 = None\n    convert_element_type_18 = torch.ops.prims.convert_element_type.default(primals_25, torch.float16);  primals_25 = None\n    permute_10 = torch.ops.aten.permute.default(convert_element_type_18, [1, 0]);  convert_element_type_18 = None\n    mm_4 = torch.ops.aten.mm.default(view_2, permute_10)\n    view_9 = torch.ops.aten.view.default(mm_4, [1, primals_1, 32]);  mm_4 = None\n    convert_element_type_21 = torch.ops.prims.convert_element_type.default(primals_26, torch.float16);  primals_26 = None\n    permute_11 = torch.ops.aten.permute.default(convert_element_type_21, [1, 0]);  convert_element_type_21 = None\n    view_10 = torch.ops.aten.view.default(view_9, [primals_1, 32]);  view_9 = None\n    mm_5 = torch.ops.aten.mm.default(view_10, permute_11)\n    view_11 = torch.ops.aten.view.default(mm_5, [1, primals_1, 8192]);  mm_5 = None\n    mul_87 = torch.ops.aten.mul.Tensor(view_11, 2.0);  view_11 = None\n    add_85 = torch.ops.aten.add.Tensor(view_7, mul_87);  view_7 = mul_87 = None\n    mul_94 = torch.ops.aten.mul.Tensor(convert_element_type_12, add_85);  convert_element_type_12 = None\n    permute_12 = torch.ops.aten.permute.default(primals_27, [1, 0]);  primals_27 = None\n    rshift_4 = primals_29 >> 1\n    mul_102 = primals_28 * primals_29;  primals_29 = None\n    rshift_5 = mul_102 >> 1\n    fused_dequantize_op_2 = torch.ops.mylib.fused_dequantize_op.default(primals_30, primals_31, primals_32, primals_33, permute_12, primals_34, 16777216, primals_28, rshift_4, mul_102, rshift_5, 2048, 8192, torch.bfloat16);  primals_30 = primals_31 = primals_32 = primals_33 = permute_12 = primals_34 = primals_28 = rshift_4 = mul_102 = rshift_5 = None\n    permute_13 = torch.ops.aten.permute.default(fused_dequantize_op_2, [1, 0]);  fused_dequantize_op_2 = None\n    permute_14 = torch.ops.aten.permute.default(permute_13, [1, 0])\n    convert_element_type_25 = torch.ops.prims.convert_element_type.default(permute_14, torch.float16);  permute_14 = None\n    convert_element_type_default_3 = torch.ops.prims.convert_element_type.default(mul_94, torch.float16)\n    permute_15 = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None\n    view_12 = torch.ops.aten.view.default(convert_element_type_default_3, [primals_1, 8192]);  convert_element_type_default_3 = None\n    mm_6 = torch.ops.aten.mm.default(view_12, permute_15);  view_12 = permute_15 = None\n    view_13 = torch.ops.aten.view.default(mm_6, [1, primals_1, 2048]);  mm_6 = None\n    convert_element_type_29 = torch.ops.prims.convert_element_type.default(primals_37, torch.float16);  primals_37 = None\n    permute_16 = torch.ops.aten.permute.default(convert_element_type_29, [1, 0]);  convert_element_type_29 = None\n    view_14 = torch.ops.aten.view.default(mul_94, [primals_1, 8192]);  mul_94 = None\n    mm_7 = torch.ops.aten.mm.default(view_14, permute_16)\n    view_15 = torch.ops.aten.view.default(mm_7, [1, primals_1, 32]);  mm_7 = None\n    convert_element_type_32 = torch.ops.prims.convert_element_type.default(primals_38, torch.float16);  primals_38 = None\n    permute_17 = torch.ops.aten.permute.default(convert_element_type_32, [1, 0]);  convert_element_type_32 = None\n    view_16 = torch.ops.aten.view.default(view_15, [primals_1, 32]);  view_15 = None\n    mm_8 = torch.ops.aten.mm.default(view_16, permute_17)\n    view_17 = torch.ops.aten.view.default(mm_8, [1, primals_1, 2048]);  mm_8 = None\n    mul_137 = torch.ops.aten.mul.Tensor(view_17, 2.0);  view_17 = None\n    add_131 = torch.ops.aten.add.Tensor(view_13, mul_137);  view_13 = mul_137 = None\n    permute_20 = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None\n    permute_24 = torch.ops.aten.permute.default(permute_16, [1, 0]);  permute_16 = None\n    convert_element_type_45 = torch.ops.prims.convert_element_type.default(permute_13, torch.float16);  permute_13 = None\n    permute_27 = torch.ops.aten.permute.default(convert_element_type_45, [1, 0]);  convert_element_type_45 = None\n    permute_30 = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None\n    permute_34 = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None\n    convert_element_type_60 = torch.ops.prims.convert_element_type.default(permute_7, torch.float16);  permute_7 = None\n    permute_37 = torch.ops.aten.permute.default(convert_element_type_60, [1, 0]);  convert_element_type_60 = None\n    permute_41 = torch.ops.aten.permute.default(permute_5, [1, 0]);  permute_5 = None\n    permute_45 = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None\n    convert_element_type_75 = torch.ops.prims.convert_element_type.default(permute_1, torch.float16);  permute_1 = None\n    permute_48 = torch.ops.aten.permute.default(convert_element_type_75, [1, 0]);  convert_element_type_75 = None\n    return (add_131, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, primals_1)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[2]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[3msdb5dbwakoaga7t5qshgspoaqmfwaxa57kkdfm4vgq7byh6us] example_inputs[3]: ('s2',)", "[7ko5rfbtvj356rgibg5o4fsjwbjg4ouhvmoyzs7vivhowssqbg5] example_inputs[4]: ('s3',)", "[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[5]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[6]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[7]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[9]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[10]: ('2048',)", "[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[11]: ('8192',)", "[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[12]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[13]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[14]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[bp5m6l3n24hn6zbxwmg6wfbnn5g5tujtrpje3hczu5umrrf2qjx] example_inputs[15]: ('s6',)", "[i3xrsw5sisbji2ivhzqy3l7zuqzhmvjpsyt77nanlhezk7f6tur] example_inputs[16]: ('s7',)", "[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[17]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[18]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[19]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[20]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[21]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[22]: ('2048',)", "[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[23]: ('8192',)", "[uyi3gjiu2xy4jsigfafocadwo325pf5czuttkwnmndhmcovbj4x] example_inputs[24]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[vo4fwgop7wopneupxrudhgrddezeyq3x3uietin75k5dhffb74k] example_inputs[25]: TensorMetadata(dtype=torch.float32, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[o6bvpbfll4j5fc756ldkzfs6tkw24z53ro6rgldcwhvw372xsds] example_inputs[26]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([8388608, 1]), stride=(1, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=8388608, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hj7nkho4e2pgg3rxjgt2xo2ofzqotfhatuhlt6bxcifwqbvipzw] example_inputs[27]: ('s10',)", "[kosoqok7aznz5iahzbmrdsrqfq3jkju4dvvd7cfij3xrvzxbmb4] example_inputs[28]: ('s11',)", "[bipxxxcjbwidee6bhaxi23lcytlibyjk4fkkuefzkiijstyfy6l] example_inputs[29]: TensorMetadata(dtype=torch.uint8, shape=torch.Size([262144]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ybu5ytahk4tirviinlte6l4uwd25rjwuhlprrcavkfcoebhaeui] example_inputs[30]: TensorMetadata(dtype=torch.float32, shape=torch.Size([256]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1024, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[lwylfzqnva66ey55zclvzdxucwqv72fkslsbznparlzjvy4wuaw] example_inputs[31]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1024]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=4096, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[yi5xtzrelmteznifoq5o2rtcjpji5z2umrdeigz4pyjmye2jssx] example_inputs[32]: TensorMetadata(dtype=torch.float16, shape=torch.Size([]), stride=(), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=2, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[mddyexj2qoxouaugfa7ewxlmax7lrnjibw5wstpsaxqwedquenp] example_inputs[33]: TensorMetadata(dtype=torch.float32, shape=torch.Size([16]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=64, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[uxepecsdjisq6uaawergrwnuemtsf25r76smflaqjv43if4wjkv] example_inputs[34]: ('8192',)", "[hwwmxalsjofewhqkqera3rlmvjxmijjubcaarbjhqohip24bahh] example_inputs[35]: ('2048',)", "[zs5vjd7lduxixuzsgnizmtcmiocu75spefi5z3zyshqbfmva6ge] example_inputs[36]: TensorMetadata(dtype=torch.float32, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=1048576, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[chru3fsd44dmnozqmsiynvahv26l3bk7j747je77m6edbjsd2kq] example_inputs[37]: TensorMetadata(dtype=torch.float32, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=262144, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[34jl2fvzyx6odxtpy45nz66wulvmgs5bvsvvhx2zwehki6ym5kw] fx_kwargs[static_input_idxs]: [2, 5, 6, 7, 8, 9, 12, 13, 14, 17, 18, 19, 20, 21, 24, 25, 26, 29, 30, 31, 32, 33, 36, 37]", "[2ruvd6wcktb7xtp52gxtldu3ygrqbnwbiluwcx3yagimk7erlhx] fx_kwargs[user_visible_outputs]: {'add_131': None}", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 688660460, "cache_state": "hit"}
V0326 23:52:07.067000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "ad7a933f4a5daec0e024f7c6af4d33e9"}
	{
	"name": "inductor_compile",
	"ts": 1742993527067917.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 10,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.068000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "1d11c6ccd24ce9859a1025d2060b6cc6"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993527068288.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 10,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.068000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "f26585b5e37a965bb0d522792b61a6ec"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1742993527068691.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 10,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.069000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "63e639e260d9cb164570f1739b92c7ac"}
	{
	"name": "compile_fx.<locals>.bw_compiler",
	"ts": 1742993527069501.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.069000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "244c0a43f0b0b0c55f273ea8e5ef6cc2"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993527069910.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.070000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "d1f4f7c7372e3789cca26cdbf0853fb6"}
	{
	"name": "inductor_compile",
	"ts": 1742993527069910.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.080000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_tom/hi/chi6hm2i2r5fg3p735q2myqwx4uplr5ji3czydmtzsbsjw6fmtl2.py"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "ccae98e057b275846abfbc378763924c"}
	# AOT ID: ['5_backward']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	import triton
	import triton.language as tl
	from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph
	from torch._C import _cuda_getCurrentRawStream as get_raw_stream
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	# kernel path: /tmp/torchinductor_tom/oz/cozopk2ehguq32bevie5q4mjctwrvknb2efn2zmifydb43kq7u2a.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten.mul, aten.view]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %mul_144 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%tangents_1, 2.0), kwargs = {})
	#   %view_18 : [num_users=2] = call_function[target=torch.ops.aten.reshape.default](args = (%mul_144, [%primals_1, 2048]), kwargs = {})
	triton_poi_fused_mul_view_0 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_view_0', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = 2.0
	    tmp2 = tmp0 * tmp1
	    tl.store(out_ptr0 + (x0), tmp2, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/6p/c6pwujwafd7jyea2qiskauahv6a3isypaxtecr7nxxt4nrphxtq6.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_39 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_21, torch.float32), kwargs = {})
	triton_poi_fused__to_copy_1 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[65536], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_1', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 65536
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/2o/c2odmo35jco5222u2qehkh6jj67bnvs7ccevylkmgiqf4prmli3t.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %convert_element_type_44 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%permute_25, torch.float32), kwargs = {})
	triton_poi_fused__to_copy_2 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_2', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
	    xnumel = 262144
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tmp0.to(tl.float32)
	    tl.store(out_ptr0 + (x0), tmp1, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/ut/cutu6nbumwt37rt3w5ivi5f2exxnf3nqjnpdd3meykytx4gvjuio.py
	# Topologically Sorted Source Nodes: [silu], Original ATen: [aten.add, aten.silu, aten.mul, aten.sigmoid, aten.fill, aten.sub]
	# Source node to ATen node mapping:
	#   silu => convert_element_type_11, convert_element_type_12, mul_46, sigmoid
	# Graph fragment:
	#   %add_135 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_21, %view_23), kwargs = {})
	#   %convert_element_type_11 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_39, torch.float32), kwargs = {})
	#   %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_11,), kwargs = {})
	#   %mul_46 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_11, %sigmoid), kwargs = {})
	#   %convert_element_type_12 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_46, torch.float16), kwargs = {})
	#   %mul_146 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_135, %convert_element_type_12), kwargs = {})
	#   %mul_147 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_135, %add_85), kwargs = {})
	#   %mul_148 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_146, 2.0), kwargs = {})
	#   %sigmoid_1 : [num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%add_39,), kwargs = {})
	#   %full_default : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([1, %primals_1, 8192], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
	#   %sub_44 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default, %sigmoid_1), kwargs = {})
	#   %mul_150 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_39, %sub_44), kwargs = {})
	#   %add_137 : [num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_150, 1), kwargs = {})
	#   %mul_151 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_1, %add_137), kwargs = {})
	#   %mul_152 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_147, %mul_151), kwargs = {})
	#   %mul_153 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_152, 2.0), kwargs = {})
	triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[1048576], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: '*fp16', 5: '*fp16', 6: '*fp16', 7: '*fp16', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, out_ptr2, out_ptr3, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr1 + (x0), None).to(tl.float32)
	    tmp3 = tl.load(in_ptr2 + (x0), None).to(tl.float32)
	    tmp11 = tl.load(in_ptr3 + (x0), None).to(tl.float32)
	    tmp2 = tmp0 + tmp1
	    tmp4 = tmp3.to(tl.float32)
	    tmp5 = tl.sigmoid(tmp4)
	    tmp6 = tmp4 * tmp5
	    tmp7 = tmp6.to(tl.float32)
	    tmp8 = tmp2 * tmp7
	    tmp9 = 2.0
	    tmp10 = tmp8 * tmp9
	    tmp12 = tmp2 * tmp11
	    tmp13 = tl.sigmoid(tmp3)
	    tmp14 = 1.0
	    tmp15 = tmp14 - tmp13
	    tmp16 = tmp3 * tmp15
	    tmp17 = tmp16 + tmp14
	    tmp18 = tmp13 * tmp17
	    tmp19 = tmp12 * tmp18
	    tmp20 = tmp19 * tmp9
	    tl.store(out_ptr0 + (x0), tmp10, None)
	    tl.store(out_ptr1 + (x0), tmp8, None)
	    tl.store(out_ptr2 + (x0), tmp20, None)
	    tl.store(out_ptr3 + (x0), tmp19, None)
	''', device_str='cuda')
	
	
	# kernel path: /tmp/torchinductor_tom/vd/cvdgoxiwnbzp7azzk6zja6cs44uw5kuroxt6qq5umvp5rqdwji7r.py
	# Topologically Sorted Source Nodes: [], Original ATen: [aten.add]
	# Source node to ATen node mapping:
	# Graph fragment:
	#   %add_136 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_27, %view_29), kwargs = {})
	#   %add_138 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_136, %view_33), kwargs = {})
	#   %add_139 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_138, %view_35), kwargs = {})
	triton_poi_fused_add_4 = async_compile.triton('triton_', '''
	import triton
	import triton.language as tl
	from triton.compiler.compiler import AttrsDescriptor
	
	from torch._inductor.runtime import triton_helpers, triton_heuristics
	from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
	from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
	
	@triton_heuristics.pointwise(
	    size_hints=[262144], 
	    filename=__file__,
	    triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, multi_processor_count=28), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
	    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_4', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': '61C517A620B536BFDF947A3BF37F0A6082A6B09AF732A916991DD30BE3883ED9', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
	    min_elem_per_thread=0
	)
	@triton.jit
	def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, XBLOCK : tl.constexpr):
	    xoffset = tl.program_id(0) * XBLOCK
	    xindex = xoffset + tl.arange(0, XBLOCK)[:]
	    xmask = tl.full([XBLOCK], True, tl.int1)
	    x0 = xindex
	    tmp0 = tl.load(in_out_ptr0 + (x0), None).to(tl.float32)
	    tmp1 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
	    tmp3 = tl.load(in_ptr1 + (x0), None).to(tl.float32)
	    tmp5 = tl.load(in_ptr2 + (x0), None).to(tl.float32)
	    tmp2 = tmp0 + tmp1
	    tmp4 = tmp2 + tmp3
	    tmp6 = tmp4 + tmp5
	    tl.store(in_out_ptr0 + (x0), tmp6, None)
	''', device_str='cuda')
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1 = args
	    args.clear()
	    s0 = primals_1
	    assert_size_stride(view_2, (s0, 2048), (2048, 1))
	    assert_size_stride(view_4, (s0, 32), (32, 1))
	    assert_size_stride(add_39, (1, s0, 8192), (8192*s0, 8192, 1))
	    assert_size_stride(view_10, (s0, 32), (32, 1))
	    assert_size_stride(add_85, (1, s0, 8192), (8192*s0, 8192, 1))
	    assert_size_stride(view_14, (s0, 8192), (8192, 1))
	    assert_size_stride(view_16, (s0, 32), (32, 1))
	    assert_size_stride(permute_20, (2048, 32), (32, 1))
	    assert_size_stride(permute_24, (32, 8192), (8192, 1))
	    assert_size_stride(permute_27, (2048, 8192), (8192, 1))
	    assert_size_stride(permute_30, (8192, 32), (32, 1))
	    assert_size_stride(permute_34, (32, 2048), (2048, 1))
	    assert_size_stride(permute_37, (8192, 2048), (2048, 1))
	    assert_size_stride(permute_41, (8192, 32), (32, 1))
	    assert_size_stride(permute_45, (32, 2048), (2048, 1))
	    assert_size_stride(permute_48, (8192, 2048), (2048, 1))
	    assert_size_stride(tangents_1, (1, s0, 2048), (2048*s0, 2048, 1))
	    with torch.cuda._DeviceGuard(0):
	        torch.cuda.set_device(0)
	        buf0 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mul, aten.view]
	        triton_poi_fused_mul_view_0_xnumel = 2048*s0
	        stream0 = get_raw_stream(0)
	        triton_poi_fused_mul_view_0.run(tangents_1, buf0, triton_poi_fused_mul_view_0_xnumel, grid=grid(triton_poi_fused_mul_view_0_xnumel), stream=stream0)
	        buf1 = empty_strided_cuda((2048, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf0, (2048, s0), (1, 2048), 0), view_16, out=buf1)
	        del view_16
	        buf2 = empty_strided_cuda((s0, 32), (32, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf0, permute_20, out=buf2)
	        del permute_20
	        buf3 = empty_strided_cuda((2048, 32), (32, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf1, buf3, 65536, grid=grid(65536), stream=stream0)
	        buf4 = empty_strided_cuda((32, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf2, (32, s0), (1, 32), 0), view_14, out=buf4)
	        del view_14
	        buf5 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf2, permute_24, out=buf5)
	        del permute_24
	        buf6 = empty_strided_cuda((32, 8192), (8192, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_2.run(buf4, buf6, 262144, grid=grid(262144), stream=stream0)
	        buf7 = empty_strided_cuda((s0, 8192), (8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(tangents_1, (s0, 2048), (2048, 1), 0), permute_27, out=buf7)
	        del permute_27
	        del tangents_1
	        buf8 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        buf15 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        buf17 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        buf24 = empty_strided_cuda((1, s0, 8192), (8192*s0, 8192, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [silu], Original ATen: [aten.add, aten.silu, aten.mul, aten.sigmoid, aten.fill, aten.sub]
	        triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3_xnumel = 8192*s0
	        triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3.run(buf5, buf7, add_39, add_85, buf8, buf15, buf17, buf24, triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3_xnumel, grid=grid(triton_poi_fused_add_fill_mul_sigmoid_silu_sub_3_xnumel), stream=stream0)
	        del add_39
	        del add_85
	        del buf5
	        del buf7
	        buf9 = reinterpret_tensor(buf4, (8192, 32), (32, 1), 0); del buf4  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf8, (8192, s0), (1, 8192), 0), view_10, out=buf9)
	        del view_10
	        buf10 = buf2; del buf2  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf8, (s0, 8192), (8192, 1), 0), permute_30, out=buf10)
	        del buf8
	        del permute_30
	        buf11 = empty_strided_cuda((8192, 32), (32, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_2.run(buf9, buf11, 262144, grid=grid(262144), stream=stream0)
	        buf12 = reinterpret_tensor(buf1, (32, 2048), (2048, 1), 0); del buf1  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf10, (32, s0), (1, 32), 0), view_2, out=buf12)
	        buf13 = buf0; del buf0  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf10, permute_34, out=buf13)
	        del permute_34
	        buf14 = empty_strided_cuda((32, 2048), (2048, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf12, buf14, 65536, grid=grid(65536), stream=stream0)
	        buf16 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf15, (s0, 8192), (8192, 1), 0), permute_37, out=buf16)
	        del buf15
	        del permute_37
	        buf18 = buf9; del buf9  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf17, (8192, s0), (1, 8192), 0), view_4, out=buf18)
	        del view_4
	        buf19 = buf10; del buf10  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf17, (s0, 8192), (8192, 1), 0), permute_41, out=buf19)
	        del buf17
	        del permute_41
	        buf20 = empty_strided_cuda((8192, 32), (32, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_2.run(buf18, buf20, 262144, grid=grid(262144), stream=stream0)
	        del buf18
	        buf21 = buf12; del buf12  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf19, (32, s0), (1, 32), 0), view_2, out=buf21)
	        del view_2
	        buf22 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(buf19, permute_45, out=buf22)
	        del buf19
	        del permute_45
	        buf23 = empty_strided_cuda((32, 2048), (2048, 1), torch.float32)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
	        triton_poi_fused__to_copy_1.run(buf21, buf23, 65536, grid=grid(65536), stream=stream0)
	        del buf21
	        buf25 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float16)
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
	        extern_kernels.mm(reinterpret_tensor(buf24, (s0, 8192), (8192, 1), 0), permute_48, out=buf25)
	        del buf24
	        del permute_48
	        buf26 = reinterpret_tensor(buf13, (1, s0, 2048), (2048*s0, 2048, 1), 0); del buf13  # reuse
	        # Topologically Sorted Source Nodes: [], Original ATen: [aten.add]
	        triton_poi_fused_add_4_xnumel = 2048*s0
	        triton_poi_fused_add_4.run(buf26, buf16, buf22, buf25, triton_poi_fused_add_4_xnumel, grid=grid(triton_poi_fused_add_4_xnumel), stream=stream0)
	        del buf16
	        del buf22
	        del buf25
	    return (None, buf26, None, None, None, None, None, None, None, None, None, None, buf23, buf20, None, None, None, None, None, None, None, None, None, None, buf14, buf11, None, None, None, None, None, None, None, None, None, None, buf6, buf3, )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    primals_1 = 100
	    view_2 = rand_strided((100, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    view_4 = rand_strided((100, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    add_39 = rand_strided((1, 100, 8192), (819200, 8192, 1), device='cuda:0', dtype=torch.float16)
	    view_10 = rand_strided((100, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    add_85 = rand_strided((1, 100, 8192), (819200, 8192, 1), device='cuda:0', dtype=torch.float16)
	    view_14 = rand_strided((100, 8192), (8192, 1), device='cuda:0', dtype=torch.float16)
	    view_16 = rand_strided((100, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_20 = rand_strided((2048, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_24 = rand_strided((32, 8192), (8192, 1), device='cuda:0', dtype=torch.float16)
	    permute_27 = rand_strided((2048, 8192), (8192, 1), device='cuda:0', dtype=torch.float16)
	    permute_30 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_34 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    permute_37 = rand_strided((8192, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    permute_41 = rand_strided((8192, 32), (32, 1), device='cuda:0', dtype=torch.float16)
	    permute_45 = rand_strided((32, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    permute_48 = rand_strided((8192, 2048), (2048, 1), device='cuda:0', dtype=torch.float16)
	    tangents_1 = rand_strided((1, 100, 2048), (204800, 2048, 1), device='cuda:0', dtype=torch.float16)
	    fn = lambda: call([primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0326 23:52:07.081000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "bfd36577ccbed597462490d53d7a9b52"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1742993527080869.8,
	"args": {
	"key": "fuwpd53kuljrtdqmdkesfbb743gwbdes3cogyfbosttj4mjzbydy",
	"components": [
	"[cjhczcrgi3pzc6cive3k3kqwasc5syujhgv54rquueioi4qb6w2] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1):\n    mul_144 = torch.ops.aten.mul.Tensor(tangents_1, 2.0)\n    view_18 = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None\n    permute_18 = torch.ops.aten.permute.default(view_18, [1, 0])\n    mm_9 = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None\n    permute_19 = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None\n    mm_10 = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None\n    view_19 = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None\n    permute_21 = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None\n    convert_element_type_39 = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None\n    view_20 = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None\n    permute_22 = torch.ops.aten.permute.default(view_20, [1, 0])\n    mm_11 = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None\n    permute_23 = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None\n    mm_12 = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None\n    view_21 = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None\n    permute_25 = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None\n    convert_element_type_44 = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None\n    view_22 = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None\n    mm_13 = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None\n    view_23 = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None\n    convert_element_type_default_2 = torch.ops.prims.convert_element_type.default(view_23, torch.float16);  view_23 = None\n    add_135 = torch.ops.aten.add.Tensor(view_21, convert_element_type_default_2);  view_21 = convert_element_type_default_2 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    mul_146 = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None\n    mul_147 = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None\n    mul_148 = torch.ops.aten.mul.Tensor(mul_146, 2.0)\n    view_24 = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None\n    permute_28 = torch.ops.aten.permute.default(view_24, [1, 0])\n    mm_14 = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None\n    permute_29 = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None\n    mm_15 = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None\n    view_25 = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None\n    permute_31 = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None\n    convert_element_type_54 = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None\n    view_26 = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None\n    permute_32 = torch.ops.aten.permute.default(view_26, [1, 0])\n    mm_16 = torch.ops.aten.mm.default(permute_32, view_2);  permute_32 = None\n    permute_33 = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None\n    mm_17 = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None\n    view_27 = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None\n    permute_35 = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None\n    convert_element_type_59 = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None\n    view_28 = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None\n    mm_18 = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None\n    view_29 = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None\n    convert_element_type_default_1 = torch.ops.prims.convert_element_type.default(view_29, torch.float16);  view_29 = None\n    add_136 = torch.ops.aten.add.Tensor(view_27, convert_element_type_default_1);  view_27 = convert_element_type_default_1 = None\n    sigmoid_1 = torch.ops.aten.sigmoid.default(add_39)\n    full_default = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)\n    sub_44 = torch.ops.aten.sub.Tensor(full_default, sigmoid_1);  full_default = None\n    mul_150 = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None\n    add_137 = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None\n    mul_151 = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None\n    mul_152 = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None\n    mul_153 = torch.ops.aten.mul.Tensor(mul_152, 2.0)\n    view_30 = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None\n    permute_39 = torch.ops.aten.permute.default(view_30, [1, 0])\n    mm_19 = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None\n    permute_40 = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None\n    mm_20 = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None\n    view_31 = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None\n    permute_42 = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None\n    convert_element_type_69 = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None\n    view_32 = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None\n    permute_43 = torch.ops.aten.permute.default(view_32, [1, 0])\n    mm_21 = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None\n    permute_44 = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None\n    mm_22 = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None\n    view_33 = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None\n    add_138 = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None\n    permute_46 = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None\n    convert_element_type_74 = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None\n    view_34 = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None\n    mm_23 = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None\n    view_35 = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None\n    convert_element_type_default = torch.ops.prims.convert_element_type.default(view_35, torch.float16);  view_35 = None\n    add_139 = torch.ops.aten.add.Tensor(add_138, convert_element_type_default);  add_138 = convert_element_type_default = None\n    return (None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[6pt52nx6wf2o3fnlscvbb6mpkzglnsxgtp26liujc4wx2tvtpm7] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4096*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[3]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[5]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[4ofuwkiqpjt5dr73pqxsydaoru4cj2w3m4sn674mztswxzkzyv5] example_inputs[6]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[kjiu5nyliqwsnl7icaeauzcjyk3boczmm4vcgqglpeyga4ezc26] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[buqdocmqej3phjqjaio73y24jqcnui6yzgcbitztcq67svbntcd] example_inputs[9]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[hxsqnh7shhh7eup67u4ogv42dqcuywp53tt5tp7sdfgeqxuk32j] example_inputs[10]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[11]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[12]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[13]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[14]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[15]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[16]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[17]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_backward]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[n2y367w3cvigrpqrkfggnd4ki5vge46t7f3bihnrmz2hx2qkg6o] fx_kwargs[static_input_idxs]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]",
	"[giofkincgu73hppmew27gpl3zgvnfyyigbbqjyi5t7e6a5ldenw] fx_kwargs[user_visible_outputs]: {'add_139': None, 'convert_element_type_74': None, 'convert_element_type_69': None, 'convert_element_type_59': None, 'convert_element_type_54': None, 'convert_element_type_44': None, 'convert_element_type_39': None}",
	"[vhi4lnshnjmzxgwsuiowu552sqjv64oariyhqqha4hh3x7qlxkw] inputs_to_check[0]: 17",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)",
	"[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>",
	"[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}",
	"[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 532666105,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0326 23:52:07.081000 331427 .venv/lib/python3.12/site-packages/torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "db354f4ac35fe5c12607d9e5788c27e9"}
	{"key": "fuwpd53kuljrtdqmdkesfbb743gwbdes3cogyfbosttj4mjzbydy", "components": ["[cjhczcrgi3pzc6cive3k3kqwasc5syujhgv54rquueioi4qb6w2] gm: GraphModule()\n\n\n\ndef forward(self, primals_1, view_2, view_4, add_39, view_10, add_85, view_14, view_16, permute_20, permute_24, permute_27, permute_30, permute_34, permute_37, permute_41, permute_45, permute_48, tangents_1):\n    mul_144 = torch.ops.aten.mul.Tensor(tangents_1, 2.0)\n    view_18 = torch.ops.aten.view.default(mul_144, [primals_1, 2048]);  mul_144 = None\n    permute_18 = torch.ops.aten.permute.default(view_18, [1, 0])\n    mm_9 = torch.ops.aten.mm.default(permute_18, view_16);  permute_18 = view_16 = None\n    permute_19 = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None\n    mm_10 = torch.ops.aten.mm.default(view_18, permute_20);  view_18 = permute_20 = None\n    view_19 = torch.ops.aten.view.default(mm_10, [1, primals_1, 32]);  mm_10 = None\n    permute_21 = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None\n    convert_element_type_39 = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None\n    view_20 = torch.ops.aten.view.default(view_19, [primals_1, 32]);  view_19 = None\n    permute_22 = torch.ops.aten.permute.default(view_20, [1, 0])\n    mm_11 = torch.ops.aten.mm.default(permute_22, view_14);  permute_22 = view_14 = None\n    permute_23 = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None\n    mm_12 = torch.ops.aten.mm.default(view_20, permute_24);  view_20 = permute_24 = None\n    view_21 = torch.ops.aten.view.default(mm_12, [1, primals_1, 8192]);  mm_12 = None\n    permute_25 = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None\n    convert_element_type_44 = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None\n    view_22 = torch.ops.aten.view.default(tangents_1, [primals_1, 2048]);  tangents_1 = None\n    mm_13 = torch.ops.aten.mm.default(view_22, permute_27);  view_22 = permute_27 = None\n    view_23 = torch.ops.aten.view.default(mm_13, [1, primals_1, 8192]);  mm_13 = None\n    convert_element_type_default_2 = torch.ops.prims.convert_element_type.default(view_23, torch.float16);  view_23 = None\n    add_135 = torch.ops.aten.add.Tensor(view_21, convert_element_type_default_2);  view_21 = convert_element_type_default_2 = None\n    convert_element_type_11 = torch.ops.prims.convert_element_type.default(add_39, torch.float32)\n    sigmoid = torch.ops.aten.sigmoid.default(convert_element_type_11)\n    mul_46 = torch.ops.aten.mul.Tensor(convert_element_type_11, sigmoid);  convert_element_type_11 = sigmoid = None\n    convert_element_type_12 = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None\n    mul_146 = torch.ops.aten.mul.Tensor(add_135, convert_element_type_12);  convert_element_type_12 = None\n    mul_147 = torch.ops.aten.mul.Tensor(add_135, add_85);  add_135 = add_85 = None\n    mul_148 = torch.ops.aten.mul.Tensor(mul_146, 2.0)\n    view_24 = torch.ops.aten.view.default(mul_148, [primals_1, 8192]);  mul_148 = None\n    permute_28 = torch.ops.aten.permute.default(view_24, [1, 0])\n    mm_14 = torch.ops.aten.mm.default(permute_28, view_10);  permute_28 = view_10 = None\n    permute_29 = torch.ops.aten.permute.default(mm_14, [1, 0]);  mm_14 = None\n    mm_15 = torch.ops.aten.mm.default(view_24, permute_30);  view_24 = permute_30 = None\n    view_25 = torch.ops.aten.view.default(mm_15, [1, primals_1, 32]);  mm_15 = None\n    permute_31 = torch.ops.aten.permute.default(permute_29, [1, 0]);  permute_29 = None\n    convert_element_type_54 = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None\n    view_26 = torch.ops.aten.view.default(view_25, [primals_1, 32]);  view_25 = None\n    permute_32 = torch.ops.aten.permute.default(view_26, [1, 0])\n    mm_16 = torch.ops.aten.mm.default(permute_32, view_2);  permute_32 = None\n    permute_33 = torch.ops.aten.permute.default(mm_16, [1, 0]);  mm_16 = None\n    mm_17 = torch.ops.aten.mm.default(view_26, permute_34);  view_26 = permute_34 = None\n    view_27 = torch.ops.aten.view.default(mm_17, [1, primals_1, 2048]);  mm_17 = None\n    permute_35 = torch.ops.aten.permute.default(permute_33, [1, 0]);  permute_33 = None\n    convert_element_type_59 = torch.ops.prims.convert_element_type.default(permute_35, torch.float32);  permute_35 = None\n    view_28 = torch.ops.aten.view.default(mul_146, [primals_1, 8192]);  mul_146 = None\n    mm_18 = torch.ops.aten.mm.default(view_28, permute_37);  view_28 = permute_37 = None\n    view_29 = torch.ops.aten.view.default(mm_18, [1, primals_1, 2048]);  mm_18 = None\n    convert_element_type_default_1 = torch.ops.prims.convert_element_type.default(view_29, torch.float16);  view_29 = None\n    add_136 = torch.ops.aten.add.Tensor(view_27, convert_element_type_default_1);  view_27 = convert_element_type_default_1 = None\n    sigmoid_1 = torch.ops.aten.sigmoid.default(add_39)\n    full_default = torch.ops.aten.full.default([1, primals_1, 8192], 1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)\n    sub_44 = torch.ops.aten.sub.Tensor(full_default, sigmoid_1);  full_default = None\n    mul_150 = torch.ops.aten.mul.Tensor(add_39, sub_44);  add_39 = sub_44 = None\n    add_137 = torch.ops.aten.add.Scalar(mul_150, 1);  mul_150 = None\n    mul_151 = torch.ops.aten.mul.Tensor(sigmoid_1, add_137);  sigmoid_1 = add_137 = None\n    mul_152 = torch.ops.aten.mul.Tensor(mul_147, mul_151);  mul_147 = mul_151 = None\n    mul_153 = torch.ops.aten.mul.Tensor(mul_152, 2.0)\n    view_30 = torch.ops.aten.view.default(mul_153, [primals_1, 8192]);  mul_153 = None\n    permute_39 = torch.ops.aten.permute.default(view_30, [1, 0])\n    mm_19 = torch.ops.aten.mm.default(permute_39, view_4);  permute_39 = view_4 = None\n    permute_40 = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None\n    mm_20 = torch.ops.aten.mm.default(view_30, permute_41);  view_30 = permute_41 = None\n    view_31 = torch.ops.aten.view.default(mm_20, [1, primals_1, 32]);  mm_20 = None\n    permute_42 = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None\n    convert_element_type_69 = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None\n    view_32 = torch.ops.aten.view.default(view_31, [primals_1, 32]);  view_31 = None\n    permute_43 = torch.ops.aten.permute.default(view_32, [1, 0])\n    mm_21 = torch.ops.aten.mm.default(permute_43, view_2);  permute_43 = view_2 = None\n    permute_44 = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None\n    mm_22 = torch.ops.aten.mm.default(view_32, permute_45);  view_32 = permute_45 = None\n    view_33 = torch.ops.aten.view.default(mm_22, [1, primals_1, 2048]);  mm_22 = None\n    add_138 = torch.ops.aten.add.Tensor(add_136, view_33);  add_136 = view_33 = None\n    permute_46 = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None\n    convert_element_type_74 = torch.ops.prims.convert_element_type.default(permute_46, torch.float32);  permute_46 = None\n    view_34 = torch.ops.aten.view.default(mul_152, [primals_1, 8192]);  mul_152 = None\n    mm_23 = torch.ops.aten.mm.default(view_34, permute_48);  view_34 = permute_48 = None\n    view_35 = torch.ops.aten.view.default(mm_23, [1, primals_1, 2048]);  mm_23 = primals_1 = None\n    convert_element_type_default = torch.ops.prims.convert_element_type.default(view_35, torch.float16);  view_35 = None\n    add_139 = torch.ops.aten.add.Tensor(add_138, convert_element_type_default);  add_138 = convert_element_type_default = None\n    return (None, add_139, None, None, None, None, None, None, None, None, None, None, convert_element_type_74, convert_element_type_69, None, None, None, None, None, None, None, None, None, None, convert_element_type_59, convert_element_type_54, None, None, None, None, None, None, None, None, None, None, convert_element_type_44, convert_element_type_39)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[6pt52nx6wf2o3fnlscvbb6mpkzglnsxgtp26liujc4wx2tvtpm7] example_inputs[1]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=4096*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[2]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[3]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[4]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[pfqthxkq4xxcoow74w72d53ytkfijvk3mlw5zzxt6wm7yzx2nz2] example_inputs[5]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 8192]), stride=(8192*s0, 8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[4ofuwkiqpjt5dr73pqxsydaoru4cj2w3m4sn674mztswxzkzyv5] example_inputs[6]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=16384*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[iqmd74gcdacyj47xo6ceso7gp6qs5mbjdmvq7wlmexjpgfl63gz] example_inputs[7]: TensorMetadata(dtype=torch.float16, shape=torch.Size([s0, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=64*s0, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[kjiu5nyliqwsnl7icaeauzcjyk3boczmm4vcgqglpeyga4ezc26] example_inputs[8]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[buqdocmqej3phjqjaio73y24jqcnui6yzgcbitztcq67svbntcd] example_inputs[9]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[hxsqnh7shhh7eup67u4ogv42dqcuywp53tt5tp7sdfgeqxuk32j] example_inputs[10]: TensorMetadata(dtype=torch.float16, shape=torch.Size([2048, 8192]), stride=(8192, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[11]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[12]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[13]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[d4rs76ndkz42ydr4yrfc4yeqkxl5zwc4sjjqbppanclsgntpewe] example_inputs[14]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 32]), stride=(32, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=524288, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[j3ilm7egdvame5wymphomc7cawul36bqgibo7qr4esv5xkh3li4] example_inputs[15]: TensorMetadata(dtype=torch.float16, shape=torch.Size([32, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=131072, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[s7ppdhoefn3msi7bvho46g46m2tdr5bzlmyqolm3phf5puumcpg] example_inputs[16]: TensorMetadata(dtype=torch.float16, shape=torch.Size([8192, 2048]), stride=(2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=33554432, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[ujjzoqcv6wmyzdhtwjrjvhe56njmp744dxs7e3k42ak7oa3l7px] example_inputs[17]: TensorMetadata(dtype=torch.float16, shape=torch.Size([1, s0, 2048]), stride=(2048*s0, 2048, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_backward]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[n2y367w3cvigrpqrkfggnd4ki5vge46t7f3bihnrmz2hx2qkg6o] fx_kwargs[static_input_idxs]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]", "[giofkincgu73hppmew27gpl3zgvnfyyigbbqjyi5t7e6a5ldenw] fx_kwargs[user_visible_outputs]: {'add_139': None, 'convert_element_type_74': None, 'convert_element_type_69': None, 'convert_element_type_59': None, 'convert_element_type_54': None, 'convert_element_type_44': None, 'convert_element_type_39': None}", "[vhi4lnshnjmzxgwsuiowu552sqjv64oariyhqqha4hh3x7qlxkw] inputs_to_check[0]: 17", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)", "[z3xdga3qzzzx5qyklwxiiffdem4sylvdag7kzy4ws5sezqqnuao] torch_version: <bytes>", "[3xaaxecnorvylnzhjva7ma7k7otckifedhnswp67wq2lu3rl554] system_info[device]: {'name': 'NVIDIA GeForce RTX 3060'}", "[toh4r32q7z3xyxmg7etasxhuxgmyrodce42itid2kbei3j5xxkl] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-c0b006abb5bd37ba48b048a92625ae26a3e2511d0ec2b593e17f7d831669e363-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[zlfn54ahigbinegd2654sjxthxt65oinsumfweaurpy74xbdwrj] system_info[hash]: 671b28410baff42074ceab234eb0529cb718d49a68bf08f8b0ba84acd582b51d", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[debug]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune]: True", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 532666105, "cache_state": "hit"}
V0326 23:52:07.082000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "830e11c3a72906512345dc6f6cc59892"}
	{
	"name": "inductor_compile",
	"ts": 1742993527082237.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 11,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.082000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "9c834866b185231b1d4e0afcfb21d765"}
	{
	"name": "compile_fx_inner",
	"ts": 1742993527082632.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 11,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.082000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"bwd_compilation_metrics": {"compile_id": "3/1", "inductor_compile_time_s": 0.012282371520996094, "code_gen_time_s": null, "fail_type": null, "fail_reason": null}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
V0326 23:52:07.083000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "522e7bbd90be232c09239f6e259037c4"}
	{
	"name": "compile_fx.<locals>.bw_compiler",
	"ts": 1742993527083224.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 11,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.084000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "357b3bc39df2519ed0203d889d4a264b"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1742993527084918.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 11,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.085000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "a8559c531b396ee99737e4fc0c470c06"}
	{
	"name": "backend_compile",
	"ts": 1742993527085362.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 11,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.085000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "52f73e1404c5e1f7485f38a4e0b859d1"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1742993527085610.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 11,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.126000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "f7883171d952f1805cb4ade689562946"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['x'], accessed_by=DictGetItemGuardAccessor(x)
	| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=True, size=[1, None, 2048], stride=[None, 2048, 1])
	| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False         
	| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['x'], L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight'], L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight'], L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight'], L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight'], L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight'], L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight'])
	| +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)
	| | +- TYPE_MATCH: ___check_type_id(L['self'], 102513794958288)                
	| | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | +- DICT_LENGTH: len(L['self']._modules) == 4                                
	| | | | +- GuardManager: source=L['self']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor(gate_proj)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj'], 102513828834816)
	| | | | | +- GuardManager: source=L['self']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].scaling, accessed_by=DictGetItemGuardAccessor(scaling)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj'].scaling) == 1           
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].scaling['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj'].scaling['default'] == 2.0   
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules) == 7          
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'], accessed_by=DictGetItemGuardAccessor(base_layer)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer'], 102513768941808)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['base_layer'].__dict__)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['base_layer']._parameters) == 2
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'], 102513768940032)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[8388608, 1], stride=[1, 1])
	| | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state, accessed_by=GetAttrGuardAccessor(quant_state)
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state, 102513767659184)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype == torch.bfloat16
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, 140717689517824)
	| | | | | | | | | | | | | +- LENGTH_CHECK: len(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], 140718482538560)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], 140718482538560)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[262144], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, 102513767659184)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[1024], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, 140718482538560)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, 140718482538560)
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['base_layer']._parameters['bias'], 140718482527040)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'].compute_dtype, accessed_by=DictGetItemGuardAccessor(compute_dtype)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj']._modules['base_layer'].compute_dtype == torch.bfloat16
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._modules['base_layer']._backward_hooks
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._modules['base_layer']._backward_pre_hooks
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['base_layer'].compute_type_is_set, accessed_by=DictGetItemGuardAccessor(compute_type_is_set)
	| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['base_layer'].compute_type_is_set, 140718482562208)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout'], accessed_by=DictGetItemGuardAccessor(lora_dropout)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_dropout'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_dropout']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'], 102513751199760)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['lora_dropout']._modules['default'].__dict__)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A'], accessed_by=DictGetItemGuardAccessor(lora_A)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_A'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DictGuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- KeyValueManager pair at index=0
	| | | | | | | | | | | +- KeyManager: GuardManager: source=list(L['self']._modules['gate_proj']._modules['lora_A']._modules.keys())[0]
	| | | | | | | | | | | | +- EQUALS_MATCH: list(L['self']._modules['gate_proj']._modules['lora_A']._modules.keys())[0] == 'default'
	| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_A']._modules['default'], 102513750850736)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['lora_A']._modules['default'].__dict__)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[32, 2048], stride=[2048, 1])
	| | | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['lora_A']._modules['default']._parameters['bias'], 140718482527040)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B'], accessed_by=DictGetItemGuardAccessor(lora_B)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_B'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_B']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._modules['lora_B']._modules['default'], 102513750850736)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['gate_proj']._modules['lora_B']._modules['default'].__dict__)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[8192, 32], stride=[32, 1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._modules['lora_B']._modules['default']._parameters['bias'], 140718482527040)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_embedding_A'], accessed_by=DictGetItemGuardAccessor(lora_embedding_A)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_embedding_B'], accessed_by=DictGetItemGuardAccessor(lora_embedding_B)
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._modules['lora_magnitude_vector'], accessed_by=DictGetItemGuardAccessor(lora_magnitude_vector)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].use_dora, accessed_by=DictGetItemGuardAccessor(use_dora)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['gate_proj'].use_dora) == 1          
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].use_dora['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj'].use_dora['default'], 140718482561760)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._parameters             
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._active_adapter, accessed_by=DictGetItemGuardAccessor(_active_adapter)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj']._active_adapter, 140718482539552)
	| | | | | | | +- LENGTH_CHECK: len(L['self']._modules['gate_proj']._active_adapter) == 1   
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['up_proj']._active_adapter
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['down_proj']._active_adapter
	| | | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._active_adapter[0], accessed_by=ListGetItemGuardAccessor(0)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['gate_proj']._active_adapter[0] == 'default'
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._backward_hooks         
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj'].merged_adapters, accessed_by=DictGetItemGuardAccessor(merged_adapters)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['gate_proj'].merged_adapters, 140718482539552)
	| | | | | | | +- LENGTH_CHECK: not L['self']._modules['gate_proj'].merged_adapters         
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._disable_adapters, accessed_by=DictGetItemGuardAccessor(_disable_adapters)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['gate_proj']._disable_adapters, 140718482561760)
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['gate_proj']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['gate_proj']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['gate_proj']._backward_pre_hooks     
	| | | | +- GuardManager: source=L['self']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor(up_proj)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj'], 102513828834816)
	| | | | | +- GuardManager: source=L['self']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj'].scaling, accessed_by=DictGetItemGuardAccessor(scaling)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj'].scaling) == 1             
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj'].scaling['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['up_proj'].scaling['default'] == 2.0     
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules) == 7            
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'], accessed_by=DictGetItemGuardAccessor(base_layer)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer'], 102513768941808)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['base_layer'].__dict__)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['base_layer']._parameters) == 2
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'], 102513768940032)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[8388608, 1], stride=[1, 1])
	| | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state, accessed_by=GetAttrGuardAccessor(quant_state)
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state, 102513767659184)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype == torch.bfloat16
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, 140717689517824)
	| | | | | | | | | | | | | +- LENGTH_CHECK: len(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], 140718482538560)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], 140718482538560)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[262144], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, 102513767659184)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[1024], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, 140718482538560)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, 140718482538560)
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['base_layer']._parameters['bias'], 140718482527040)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'].compute_dtype, accessed_by=DictGetItemGuardAccessor(compute_dtype)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['up_proj']._modules['base_layer'].compute_dtype == torch.bfloat16
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._modules['base_layer']._backward_hooks
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._modules['base_layer']._backward_pre_hooks
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['base_layer'].compute_type_is_set, accessed_by=DictGetItemGuardAccessor(compute_type_is_set)
	| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['base_layer'].compute_type_is_set, 140718482562208)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout'], accessed_by=DictGetItemGuardAccessor(lora_dropout)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_dropout'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_dropout']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'], 102513751199760)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['lora_dropout']._modules['default'].__dict__)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A'], accessed_by=DictGetItemGuardAccessor(lora_A)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_A'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DictGuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- KeyValueManager pair at index=0
	| | | | | | | | | | | +- KeyManager: GuardManager: source=list(L['self']._modules['up_proj']._modules['lora_A']._modules.keys())[0]
	| | | | | | | | | | | | +- EQUALS_MATCH: list(L['self']._modules['up_proj']._modules['lora_A']._modules.keys())[0] == 'default'
	| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_A']._modules['default'], 102513750850736)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['lora_A']._modules['default'].__dict__)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[32, 2048], stride=[2048, 1])
	| | | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['lora_A']._modules['default']._parameters['bias'], 140718482527040)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B'], accessed_by=DictGetItemGuardAccessor(lora_B)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_B'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_B']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj']._modules['lora_B']._modules['default'], 102513750850736)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['up_proj']._modules['lora_B']._modules['default'].__dict__)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[8192, 32], stride=[32, 1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._modules['lora_B']._modules['default']._parameters['bias'], 140718482527040)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_embedding_A'], accessed_by=DictGetItemGuardAccessor(lora_embedding_A)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_embedding_B'], accessed_by=DictGetItemGuardAccessor(lora_embedding_B)
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj']._modules['lora_magnitude_vector'], accessed_by=DictGetItemGuardAccessor(lora_magnitude_vector)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj'].use_dora, accessed_by=DictGetItemGuardAccessor(use_dora)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['up_proj'].use_dora) == 1            
	| | | | | | | +- GuardManager: source=L['self']._modules['up_proj'].use_dora['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj'].use_dora['default'], 140718482561760)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._parameters               
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._backward_hooks           
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj'].merged_adapters, accessed_by=DictGetItemGuardAccessor(merged_adapters)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['up_proj'].merged_adapters, 140718482539552)
	| | | | | | | +- LENGTH_CHECK: not L['self']._modules['up_proj'].merged_adapters           
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._disable_adapters, accessed_by=DictGetItemGuardAccessor(_disable_adapters)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['up_proj']._disable_adapters, 140718482561760)
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['up_proj']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['up_proj']._backward_pre_hooks       
	| | | | | | +- GuardManager: source=L['self']._modules['up_proj']._active_adapter, accessed_by=DictGetItemGuardAccessor(_active_adapter)
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['up_proj']._active_adapter
	| | | | +- GuardManager: source=L['self']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor(down_proj)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj'], 102513828834816)
	| | | | | +- GuardManager: source=L['self']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj'].scaling, accessed_by=DictGetItemGuardAccessor(scaling)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj'].scaling) == 1           
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj'].scaling['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- EQUALS_MATCH: L['self']._modules['down_proj'].scaling['default'] == 2.0   
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules) == 7          
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'], accessed_by=DictGetItemGuardAccessor(base_layer)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer'], 102513768941808)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['base_layer'].__dict__)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['base_layer']._parameters) == 2
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'], 102513768940032)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data, accessed_by=GetAttrGuardAccessor(_some_data)
	| | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight']._some_data, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[8388608, 1], stride=[1, 1])
	| | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state, accessed_by=GetAttrGuardAccessor(quant_state)
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state, 102513767659184)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[16], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype, accessed_by=GetAttrGuardAccessor(dtype)
	| | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.dtype == torch.bfloat16
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, accessed_by=GetAttrGuardAccessor(shape)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape, 140717689517824)
	| | | | | | | | | | | | | +- LENGTH_CHECK: len(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], accessed_by=TupleGetItemGuardAccessor(0)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0], 140718482538560)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], accessed_by=TupleGetItemGuardAccessor(1)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1], 140718482538560)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.uint8, device=0, requires_grad=False, size=[262144], stride=[1])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, accessed_by=GetAttrGuardAccessor(offset)
	| | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.offset, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float16, device=0, requires_grad=False, size=[], stride=[])
	| | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, accessed_by=GetAttrGuardAccessor(state2)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2, 102513767659184)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, accessed_by=GetAttrGuardAccessor(code)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.code, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[256], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, accessed_by=GetAttrGuardAccessor(absmax)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.absmax, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=False, size=[1024], stride=[1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.state2.blocksize, 140718482538560)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, accessed_by=GetAttrGuardAccessor(blocksize)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.blocksize, 140718482538560)
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['base_layer']._parameters['bias'], 140718482527040)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'].compute_dtype, accessed_by=DictGetItemGuardAccessor(compute_dtype)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['down_proj']._modules['base_layer'].compute_dtype == torch.bfloat16
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._modules['base_layer']._backward_hooks
	| | | | | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._modules['base_layer']._backward_pre_hooks
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['base_layer'].compute_type_is_set, accessed_by=DictGetItemGuardAccessor(compute_type_is_set)
	| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['base_layer'].compute_type_is_set, 140718482562208)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout'], accessed_by=DictGetItemGuardAccessor(lora_dropout)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_dropout'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_dropout']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'], 102513751199760)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['lora_dropout']._modules['default'].__dict__)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A'], accessed_by=DictGetItemGuardAccessor(lora_A)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_A'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- DictGuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- KeyValueManager pair at index=0
	| | | | | | | | | | | +- KeyManager: GuardManager: source=list(L['self']._modules['down_proj']._modules['lora_A']._modules.keys())[0]
	| | | | | | | | | | | | +- EQUALS_MATCH: list(L['self']._modules['down_proj']._modules['lora_A']._modules.keys())[0] == 'default'
	| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_A']._modules['default'], 102513750850736)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['lora_A']._modules['default'].__dict__)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[32, 8192], stride=[8192, 1])
	| | | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['lora_A']._modules['default']._parameters['bias'], 140718482527040)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B'], accessed_by=DictGetItemGuardAccessor(lora_B)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_B'], 102513750590560)
	| | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
	| | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_B']._modules) == 1
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj']._modules['lora_B']._modules['default'], 102513750850736)
	| | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['down_proj']._modules['lora_B']._modules['default'].__dict__)
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters) == 2
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight'], accessed_by=DictGetItemGuardAccessor(weight)
	| | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA, AutocastCUDA), torch.float32, device=0, requires_grad=True, size=[2048, 32], stride=[32, 1])
	| | | | | | | | | | | | | | +- NO_TENSOR_ALIASING
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['bias'], accessed_by=DictGetItemGuardAccessor(bias)
	| | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._modules['lora_B']._modules['default']._parameters['bias'], 140718482527040)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_embedding_A'], accessed_by=DictGetItemGuardAccessor(lora_embedding_A)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_embedding_B'], accessed_by=DictGetItemGuardAccessor(lora_embedding_B)
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj']._modules['lora_magnitude_vector'], accessed_by=DictGetItemGuardAccessor(lora_magnitude_vector)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj'].use_dora, accessed_by=DictGetItemGuardAccessor(use_dora)
	| | | | | | | +- DICT_LENGTH: len(L['self']._modules['down_proj'].use_dora) == 1          
	| | | | | | | +- GuardManager: source=L['self']._modules['down_proj'].use_dora['default'], accessed_by=DictGetItemGuardAccessor(default)
	| | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj'].use_dora['default'], 140718482561760)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._parameters             
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._backward_hooks         
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj'].merged_adapters, accessed_by=DictGetItemGuardAccessor(merged_adapters)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['down_proj'].merged_adapters, 140718482539552)
	| | | | | | | +- LENGTH_CHECK: not L['self']._modules['down_proj'].merged_adapters         
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._disable_adapters, accessed_by=DictGetItemGuardAccessor(_disable_adapters)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['down_proj']._disable_adapters, 140718482561760)
	| | | | | | +- DictSubclassGuardManager: source=L['self']._modules['down_proj']._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)
	| | | | | | | +- DICT_LENGTH: not L['self']._modules['down_proj']._backward_pre_hooks     
	| | | | | | +- GuardManager: source=L['self']._modules['down_proj']._active_adapter, accessed_by=DictGetItemGuardAccessor(_active_adapter)
	| | | | | | | +- OBJECT_ALIASING: L['self']._modules['gate_proj']._active_adapter is L['self']._modules['down_proj']._active_adapter
	| | | | +- GuardManager: source=L['self']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor(act_fn)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['act_fn'], 102513751241264)
	| | | | | +- GuardManager: source=L['self']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['act_fn'].__dict__)
	| | | | | | +- GuardManager: source=L['self']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor(inplace)
	| | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['act_fn'].inplace, 140718482561760)
	| | | +- GuardManager: source=L['self']._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)
	| | | | +- DICT_LENGTH: not L['self']._parameters                                   
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['F'], accessed_by=DictGetItemGuardAccessor(F)
	| | | +- ID_MATCH: ___check_obj_id(G['F'], 140713154810064)                    
	| | | +- GuardManager: source=G['F'].dequantize_4bit, accessed_by=GetAttrGuardAccessor(dequantize_4bit)
	| | | | +- GuardManager: source=G['F'].dequantize_4bit.__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | | +- ID_MATCH: ___check_obj_id(G['F'].dequantize_4bit.__code__, 140715525934704)
	| | | | +- GuardManager: source=G['F'].dequantize_4bit, accessed_by=FuncDefaultsGuardAccessor
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[1], accessed_by=GetItemGuardAccessor(1)
	| | | | | | +- ID_MATCH: ___check_obj_id(G['F'].dequantize_4bit.__defaults__[1], 140718482527040)
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[2], accessed_by=GetItemGuardAccessor(2)
	| | | | | | +- ID_MATCH: ___check_obj_id(G['F'].dequantize_4bit.__defaults__[2], 140718482527040)
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[3], accessed_by=GetItemGuardAccessor(3)
	| | | | | | +- EQUALS_MATCH: G['F'].dequantize_4bit.__defaults__[3] == 64                
	| | | | | +- GuardManager: source=G['F'].dequantize_4bit.__defaults__[4], accessed_by=GetItemGuardAccessor(4)
	| | | | | | +- EQUALS_MATCH: G['F'].dequantize_4bit.__defaults__[4] == 'fp4'             
	| | +- GuardManager: source=G['prod'], accessed_by=DictGetItemGuardAccessor(prod)
	| | | +- ID_MATCH: ___check_obj_id(G['prod'], 140718462147760)                 
	| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)
	| | | +- ID_MATCH: ___check_obj_id(G['torch'], 140718364431024)                
	| | | +- OBJECT_ALIASING: G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch is G['torch']
	| | | +- OBJECT_ALIASING: G['torch'] is G['__import_kernels'].torch                   
	| | | +- GuardManager: source=G['torch'].nn, accessed_by=GetAttrGuardAccessor(nn)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].nn, 140713181445024)             
	| | | | +- GuardManager: source=G['torch'].nn.functional, accessed_by=GetAttrGuardAccessor(functional)
	| | | | | +- ID_MATCH: ___check_obj_id(G['torch'].nn.functional, 140713178274832)  
	| | | | | +- GuardManager: source=G['torch'].nn.functional.linear, accessed_by=GetAttrGuardAccessor(linear)
	| | | | | | +- ID_MATCH: ___check_obj_id(G['torch'].nn.functional.linear, 140715851772960)
	| | | +- GuardManager: source=G['torch'].numel, accessed_by=GetAttrGuardAccessor(numel)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].numel, 140718363174816)          
	| | | +- GuardManager: source=G['torch'].matmul, accessed_by=GetAttrGuardAccessor(matmul)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].matmul, 140718363201264)         
	| | +- GuardManager: source=G['Params4bit'], accessed_by=DictGetItemGuardAccessor(Params4bit)
	| | | +- ID_MATCH: ___check_obj_id(G['Params4bit'], 102513768940032)           
	| | +- GuardManager: source=G['__import_kernels'], accessed_by=DictGetItemGuardAccessor(__import_kernels)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'], 140712752735120)     
	| | | +- GuardManager: source=G['__import_kernels'].DEBUG_FLAG, accessed_by=GetAttrGuardAccessor(DEBUG_FLAG)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'].DEBUG_FLAG, 140718482561760)
	| | | +- GuardManager: source=G['__import_kernels'].fused_dequantize_op, accessed_by=GetAttrGuardAccessor(fused_dequantize_op)
	| | | | +- TYPE_MATCH: ___check_type_id(G['__import_kernels'].fused_dequantize_op, 102513752736032)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'].fused_dequantize_op, 140712749840912)
	| | | | +- GuardManager: source=G['__import_kernels'].fused_dequantize_op._opoverload, accessed_by=GetAttrGuardAccessor(_opoverload)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_kernels'].fused_dequantize_op._opoverload, 140712749959024)
	| | | +- GuardManager: source=G['__import_kernels'].torch, accessed_by=GetAttrGuardAccessor(torch)
	| | | | +- OBJECT_ALIASING: G['torch'] is G['__import_kernels'].torch                   
	| | | | +- OBJECT_ALIASING: G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch is G['__import_kernels'].torch
	| | +- GuardManager: source=G['fused_dequantize'], accessed_by=DictGetItemGuardAccessor(fused_dequantize)
	| | | +- GuardManager: source=G['fused_dequantize'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | +- ID_MATCH: ___check_obj_id(G['fused_dequantize'].__code__, 102513784148304)
	| | +- GuardManager: source=G['ENABLE_ASSERTIONS'], accessed_by=DictGetItemGuardAccessor(ENABLE_ASSERTIONS)
	| | | +- ID_MATCH: ___check_obj_id(G['ENABLE_ASSERTIONS'], 140718482561760)    
	| | +- GuardManager: source=G['get_data_transposed'], accessed_by=DictGetItemGuardAccessor(get_data_transposed)
	| | | +- GuardManager: source=G['get_data_transposed'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | +- ID_MATCH: ___check_obj_id(G['get_data_transposed'].__code__, 102513784148304)
	| | +- GuardManager: source=G['TransposeBMatMul4Bit'], accessed_by=DictGetItemGuardAccessor(TransposeBMatMul4Bit)
	| | | +- ID_MATCH: ___check_obj_id(G['TransposeBMatMul4Bit'], 102513789618464) 
	| | | +- GuardManager: source=G['TransposeBMatMul4Bit'].forward, accessed_by=GetAttrGuardAccessor(forward)
	| | | | +- ID_MATCH: ___check_obj_id(G['TransposeBMatMul4Bit'].forward, 140712750173696)
	| | +- GuardManager: source=G['__builtins_dict___14'], accessed_by=DictGetItemGuardAccessor(__builtins_dict___14)
	| | | +- GuardManager: source=G['__builtins_dict___14']['any'], accessed_by=DictGetItemGuardAccessor(any)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___14']['any'], 140718464975872)
	| | | +- GuardManager: source=G['__builtins_dict___14']['str'], accessed_by=DictGetItemGuardAccessor(str)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___14']['str'], 140718482509248)
	| | | +- GuardManager: source=G['__builtins_dict___14']['bool'], accessed_by=DictGetItemGuardAccessor(bool)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___14']['bool'], 140718482561792)
	| | | +- GuardManager: source=G['__builtins_dict___14']['getattr'], accessed_by=DictGetItemGuardAccessor(getattr)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___14']['getattr'], 140718464976912)
	| | | +- GuardManager: source=G['__builtins_dict___14']['isinstance'], accessed_by=DictGetItemGuardAccessor(isinstance)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___14']['isinstance'], 140718464977472)
	| | +- GuardManager: source=G['fix_4bit_weight_quant_state_from_module'], accessed_by=DictGetItemGuardAccessor(fix_4bit_weight_quant_state_from_module)
	| | | +- GuardManager: source=G['fix_4bit_weight_quant_state_from_module'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
	| | | | +- ID_MATCH: ___check_obj_id(G['fix_4bit_weight_quant_state_from_module'].__code__, 102513768746192)
	| | +- GuardManager: source=G['__import_bitsandbytes_dot_nn_dot_modules'], accessed_by=DictGetItemGuardAccessor(__import_bitsandbytes_dot_nn_dot_modules)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_bitsandbytes_dot_nn_dot_modules'], 140713153122992)
	| | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'], accessed_by=DictGetItemGuardAccessor(__import_peft_dot_tuners_dot_lora_dot_bnb)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_lora_dot_bnb'], 140712533784624)
	| | | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch, accessed_by=GetAttrGuardAccessor(torch)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch, 140718364431024)
	| | | | +- OBJECT_ALIASING: G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch is G['torch']
	| | | | +- OBJECT_ALIASING: G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch is G['__import_kernels'].torch
	| | | | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.nn, accessed_by=GetAttrGuardAccessor(nn)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.nn, 140713181445024)
	| | | | | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.nn.functional, accessed_by=GetAttrGuardAccessor(functional)
	| | | | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.nn.functional
	| | | | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.numel, accessed_by=GetAttrGuardAccessor(numel)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.numel, 140718363174816)
	| | | | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.matmul, accessed_by=GetAttrGuardAccessor(matmul)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.matmul, 140718363201264)
	| | | | +- GuardManager: source=G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.is_autocast_enabled, accessed_by=GetAttrGuardAccessor(is_autocast_enabled)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.is_autocast_enabled, 140718363048544)
	| | +- GuardManager: source=G['__import_peft_dot_tuners_dot_tuners_utils'], accessed_by=DictGetItemGuardAccessor(__import_peft_dot_tuners_dot_tuners_utils)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_peft_dot_tuners_dot_tuners_utils'], 140712539645616)
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_linear)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'], 140713178274672)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F, accessed_by=GetAttrGuardAccessor(F)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F, 140713178274832)
	| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_torch_dot_nn_dot_modules_dot_activation'].F
	| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_peft_dot_tuners_dot_lora_dot_bnb'].torch.nn.functional
	| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F.silu, accessed_by=GetAttrGuardAccessor(silu)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F.silu, 140713176241856)
	| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F.linear, accessed_by=GetAttrGuardAccessor(linear)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F.linear, 140715851772960)
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_module)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'], 140713181532528)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_pre_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_pre_hooks)
	| | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_activation'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_activation)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_activation'], 140713176751280)
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_activation'].F, accessed_by=GetAttrGuardAccessor(F)
	| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_linear'].F is G['__import_torch_dot_nn_dot_modules_dot_activation'].F
	+- LAMBDA_GUARD: L['x'].stride()[0] == 2048*L['x'].size()[1]                   # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1] == 2048  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['gate_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0] == 8192  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1] == 2048  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['up_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0] == 8192  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[1] == 8192  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: L['self']._modules['down_proj']._modules['base_layer']._parameters['weight'].quant_state.shape[0] == 2048  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['x'].size()[1] <= 262143                               # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0326 23:52:07.127000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "d1b2e240c32c81b2eced770292f8045a"}
	{
	"name": "entire_frame_compile",
	"ts": 1742993527127485.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 11,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.127000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0, "has_payload": "5a9c22ed7bb6689cc4b657ebd5009df5"}
	{
	"name": "_compile.compile_inner",
	"ts": 1742993527127764.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 11,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0326 23:52:07.128000 331427 .venv/lib/python3.12/site-packages/torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "3/1", "frame_key": "8", "co_name": "compiled_llama_mlp", "co_filename": "/tmp/ipykernel_331427/318589162.py", "co_firstlineno": 10, "cache_size": 1, "accumulated_cache_size": 1, "guard_count": 342, "shape_env_guard_count": 29, "graph_op_count": 32, "graph_node_count": 77, "graph_input_count": 38, "start_time": 1742993526.50131, "entire_frame_compile_time_s": 0.6261110305786133, "backend_compile_time_s": 0.36132383346557617, "inductor_compile_time_s": 0.01839900016784668, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": ["mylib::fused_dequantize_op"], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 3, "frame_compile_id": 1, "attempt": 0}
